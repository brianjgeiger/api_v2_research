<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="25">
            <Title>Additional links</Title>
            <Text>From: Brandon Lorenz &lt;blorenz@gmail.com&gt;
Date: Mon, Aug 5, 2013 at 3:06 PM
Subject: Re: On now....
To: Jeffrey Spies &lt;jspies@gmail.com&gt;


Don't mean to inundate you with links and content, but here are some great resources.  Let me know how I can help! - Brandon

API Craft google group / Conference: https://groups.google.com/forum/#!forum/api-craft

These guys are serious  about API calls: http://apigee.com/about/
EXCELLENT MATERIAL here: https://blog.apigee.com/front

More reading:
https://restful-api-design.readthedocs.org/en/latest/
https://mathieu.fenniak.net/stop-designing-fragile-web-apis/

Very excellent post:
http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api

Good guidance:
http://bryanhelmig.com/your-api-consumers-arent-who-you-think-they-are/

The father of REST, Fielding's blog:
http://roy.gbiv.com/untangled/
Entry about HATEOAS:
http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven
</Text>
        </Document>
        <Document ID="18">
            <Title>Example Essay</Title>
            <Text>The Origins of the Meta-Essay: The Essay About the Essay
Arthur Caddemick
English Literature 101 Professor Jakespear 30 October 2010
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean laoreet posuere est, in consequat est imperdiet nec. Morbi vel orci mattis erat mattis vulputate vel nec eros. Sed at mauris tortor, nec ornare erat. Praesent felis velit, dapibus at pretium non, congue sed ligula. Nam ac gravida nunc. Nam sapien lectus, pellentesque pharetra semper eu, rutrum nec neque. Praesent purus orci, ornare et luctus quis, iaculis eu dolor. Proin ornare sodales semper. Sed aliquet, orci at volutpat tristique, justo mi condimentum velit, ac feugiat ipsum nulla eget leo. Integer purus sem, placerat nec lacinia vel, mollis vitae magna. In hac habitasse platea dictumst. Vivamus sed elit augue, a consequat dolor. Proin at elementum tellus. Donec sodales, turpis sit amet porta ornare, nisi diam fringilla erat, eget varius lorem lectus eget tortor. Etiam eros nisl, gravida eu eleifend eu, semper non nulla. Pellentesque semper laoreet volutpat. Nam ac malesuada metus. Nunc ut eros est, nec accumsan est.
This is a block quote. Block quotes should be separated from the main text by a blank line and should be indented by half an inch. Use the “Essay Block Quote (Preserved)” formatting preset in Scrivener to format block quotes so that they will be formatted correctly upon Compile. Vivamus porta auctor arcu eu scelerisque. Donec fringilla blandit ligula at laoreet. Cras lobortis est velit, in cursus erat. Donec convallis fermentum tortor, ut interdum ligula facilisis at. Nam posuere felis nec libero lobortis vitae porta est vulputate. Proin massa metus, condimentum vitae venenatis vel, sodales eu dolor. Suspendisse potenti. Cras nec nulla sit amet massa commodo aliquam. Quisque euismod dapibus ante quis lacinia. Praesent tempus dolor a libero sollicitudin quis consectetur ipsum bibendum. Donec pretium tincidunt odio, eu aliquet leo varius eu. Praesent euismod enim accumsan dui aliquet commodo. Nulla posuere, velit lacinia venenatis semper, sapien lacus aliquet lorem, in laoreet dolor lacus in arcu.1
Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec varius, sapien sit amet malesuada ultrices, dui sem tincidunt urna, at varius sapien libero sed ipsum. Suspendisse dolor mauris, auctor a volutpat feugiat, facilisis ac magna. Nullam metus nisl, ultricies sed laoreet nec, posuere vitae magna. Sed vitae mauris non dui scelerisque lobortis
Caddemick 2
ut sed risus. Aliquam erat volutpat. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Ut semper lacinia velit in ultricies. Mauris id quam enim, quis dapibus nisi. Fusce ipsum urna, lacinia sit amet euismod nec, mollis dapibus ligula. Fusce dolor nunc, auctor sit amet pharetra vel, porta et elit. Nam eget velit neque. Donec varius eros vitae quam viverra pulvinar.2
Duis est elit, dictum nec vestibulum non, ultrices ut massa. Suspendisse egestas vehicula justo, tempus mattis ipsum scelerisque sed. Vestibulum interdum porta risus, nec tristique sem feugiat eget. Vivamus a venenatis risus. Quisque ligula augue, euismod ac convallis vel, sagittis dapibus leo. Donec eleifend urna vitae eros mollis nec venenatis purus commodo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Proin eu sapien vitae arcu euismod pharetra et id arcu.3 Sed lectus urna, mattis a gravida ut, sollicitudin id tortor. Maecenas et mi justo, eget dictum arcu. Praesent felis nunc, tempus egestas bibendum sit amet, dictum id eros. Integer purus velit, aliquam placerat vehicula at, sodales in erat. Vestibulum at neque vitae orci convallis consectetur a at sapien. Nulla id consequat elit. Suspendisse potenti. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Ut lorem ipsum, blandit nec eleifend cursus, pharetra ut elit. Proin semper porta diam, id rutrum orci facilisis ac. Morbi blandit erat augue, et vulputate metus.4
Vestibulum non arcu felis, nec bibendum neque. Ut at odio velit. Aenean consectetur dapibus sollicitudin. Suspendisse potenti. Nam fermentum risus in odio sollicitudin interdum. Donec quam orci, commodo eget consectetur id, varius egestas magna. Morbi tincidunt fermentum orci ac imperdiet. Aliquam hendrerit lorem ac urna elementum fermentum. Aenean sollicitudin ipsum vitae arcu malesuada tempor. Phasellus feugiat dignissim erat non lobortis. Pellentesque at est at nisl fringilla posuere. Proin sagittis cursus nunc eget commodo. Nulla feugiat vehicula scelerisque. Nam quam neque, ornare sit amet tincidunt in, interdum sed nibh. Cras eget est sit amet diam adipiscing lacinia et at nisl. Nullam vulputate aliquam lorem at ultrices.
Caddemick 3
Notes
1 Depending on your requirements, footnotes can either be placed at the end of each page or on a separate “Notes” page at the end of the essay, before the “Works Cited” page. 2 In Scrivener, if you want your footnotes to be placed at the end of each page instead of on a separate “Notes” page, you should deselect “Include in Compile” for the “Notes” document and export to RTF format.
3 The “Works Cited” page is usually optional. 4 There are many variations of the Chicago essay format, so be sure to check your specific formatting requirements and amend this template as necessary.
Caddemick 4
Works Cited
Choi, Mihwa. “Contesting Imaginaires in Death Rituals during the Northern Song Dynasty.” PhD diss., University of Chicago, 2008.
Google. “Google Privacy Policy.” Last modified March 11, 2009. http://www.google.com/intl/ en/privacypolicy.html.
Kossinets, Gueorgi, and Duncan J. Watts. “Origins of Homophily in an Evolving Social Network.” American Journal of Sociology 115 (2009): 405–50. Accessed February 28, 2010. doi:10.1086/599247.
Kurland, Philip B., and Ralph Lerner, eds. The Founders’ Constitution. Chicago: University of Chicago Press, 1987. Accessed February 28, 2010. http://press-pubs.uchicago.edu/ founders/
Lattimore, Richmond, trans. The Iliad of Homer. Chicago: University of Chicago Press, 1951. McDonald’s Corporation. “McDonald’s Happy Meal Toy Safety Facts.” Accessed July 19, 2008.
http://www.mcdonalds.com/corp/about/factsheets.html.
Pollan, Michael. The Omnivore’s Dilemma: A Natural History of Four Meals. New York: Penguin, 2006.
Ward, Geoffrey C., and Ken Burns. The War: An Intimate History, 1941–1945. New York: Knopf, 2007.
Weinstein, Joshua I. “The Market in Plato’s Republic.” Classical Philology 104 (2009): 439–58.
Caddemick 5
</Text>
        </Document>
        <Document ID="10">
            <Title>About</Title>
            <Text>CHICAGO STYLE ESSAY TEMPLATE

About
This template is designed to produce an essay in the Chicago Style format. However, you should check the exact guidelines of the institution to which you are submitting your essay, because formatting expectations can vary (such as whether there should be a separate title page, whether footnotes should appear at the end of each page or as endnotes, and so on). See “Adjusting This Template” (below) for information on how to tweak your project to meet your requirements.

How To Use This Template
	•	Edit the “Title Page” document so that it contains the title of your essay, your instructor’s name and so on.
	•	Compose your essay in text documents inside the “Main Content” folder. Whether you use a single document or multiple text files to write your essay is entirely up to you - just make sure they are all inside the “Main Content” folder.
	•	Indent block quotes or other special passages using the ruler settings you require in the finished document and enclose them in a “Preserve Formatting” block. To do this, select the text of the block quote and go to Format &gt; Formatting &gt; Preserve Formatting. (The quickest way of formatting a block quote is to click in the paragraph and select “Essay Block Quote (Preserved)” from the formatting presets, either using the “Presets…” control in the format bar, or by going to the Format  menu and choosing Formatting &gt; Apply Preset - this will indent the text and apply the “Preserve Formatting” block for you.) All other text will have the default indentation assigned during the compile process.
	•	This template includes an “Endnotes” document which will be used to hold any footnotes you have created in the main body of the text. If you have not created any footnotes, you should either delete this document or deselect its “Include in Compile” setting in the inspector or Compile sheet.
	•	Edit the “Works Cited” file so that it contains your own bibliography, being careful to adhere to the Chicago style for the citations. Some notes on the style required for citations are provided in the Works Cited Format document. If you do not require a “Works Cited” page, either delete this document or deselect its “Include in Compile” setting in the inspector or Compile sheet.
	•	Use File &gt; Compile… to compile your essay for printing.

Adjusting This Template
	•	If you don’t require a separate title page but instead need the title and course information as the header of the first page, remove the “Title Page” document from the “Essay” folder and replace it with the “First Page Header” document, which you can find inside the “Miscellaneous” folder. Also be sure to deselect “Page Break Before” for the “Main Content” folder, either in the inspector or the “Contents” pane of the Compile sheet.
	•	This template uses Times New Roman as the font. If you require a different font, such as Arial or Courier, select the text in the “Title Page” and “Works Cited” documents and change the font using the font panel or format bar. To change the font for the main body of the text, select the “Formatting” tab in the Compile sheet, click on the third row in the table (“Level 1+” with the icon of a single text document next to it), and then click into the text area. Finally click on the “A” button in the format bar above the text area to choose a different font. To change the header font, select your preferred font in the “Page Settings” pane of the Compile sheet.
	•	If you require footnotes to appear at the end of each page instead of as endnotes, then you should compile to the RTF format. If you do this, be sure to deselect “Include in Compile” for the “Notes” document.

Sample Document
The “Example Essay” PDF file in the Research folder shows how the essay will look when compiled.

Final Note
Scrivener project templates are flexible and are not intended to restrict you to a particular workflow. You can change, delete or move the files and folders contained in the template and you can create your own templates by setting up a skeletal project with the files, folders and settings you would like to use for new projects and using File &gt; Save As Template.</Text>
        </Document>
        <Document ID="19">
            <Title>2002-REST-TOIT</Title>
            <Synopsis>The World Wide Web has succeeded in large part because its software architecture has been de- signed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, inde- pendent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the inter- action constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architec- ture in order to elicit mismatches between the existing protocols and the applications they are intended to support.</Synopsis>
            <Text>Principled Design of the Modern Web ArchitectureROY T. FIELDINGDay SoftwareandRICHARD N. TAYLOR University of California, IrvineThe World Wide Web has succeeded in large part because its software architecture has been de- signed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, inde- pendent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the inter- action constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architec- ture in order to elicit mismatches between the existing protocols and the applications they are intended to support.Categories and Subject Descriptors: D.2.11 [Software Engineering]: Software Architectures; H.5.4 [Hypertext/Hypermedia]: Architectures; H.3.5 [Information Storage and Retrieval]: On-line Information Services—Web-based servicesGeneral Terms: Design, Performance, StandardizationAdditional Key Words and Phrases: Network-based applications, REST, World Wide WebThis work was partially supported by the Defense Advanced Research Projects Agency and Air Force Research Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-97- 2-0021, and originated while the first author was at the University of California, Irvine.An earlier version of this paper appeared in the Proceedings of the 22nd International Conference on Software Engineering, Limerick, Ireland, June 2000 (ICSE 2000), 407–416.Authors’ addresses: R. T. Fielding, Day Software, 2 Corporate Plaza, Suite 150, Newport Beach, CA 92660; email: roy.fielding@day.com; R. N. Taylor, Information and Computer Science, University of California, Irvine CA 92697-3425; email: taylor@ics.uci.edu.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org.⃝C 2002 ACM 1533-5399/02/0500–0115 $5.00ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002, Pages 115–150.￼￼
116 • R. T. Fielding and R. N. Taylor 1. INTRODUCTIONAt the beginning of our efforts within the Internet Engineering Taskforce to define the existing Hypertext Transfer Protocol (HTTP/1.0) [Berners-Lee et al. 1996] and design the extensions for the new standards of HTTP/1.1 [Fielding et al. 1999] and Uniform Resource Identifiers (URI) [Berners-Lee et al. 1998], we recognized the need for a model of how the World Wide Web (WWW, or simply Web) should work. This idealized model of the interactions within an overall Web application—what we refer to as the Representational State Trans- fer (REST) architectural style—became the foundation for the modern Web architecture, providing the guiding principles by which flaws in the existing architecture could be identified and extensions validated prior to deployment.A software architecture is an abstraction of the runtime elements of a soft- ware system during some phase of its operation [Fielding 2000]. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture. An architecture determines how system ele- ments are identified and allocated, how the elements interact to form a system, the amount and granularity of communication needed for interaction, and the interface protocols used for communication. An architectural style is a coordi- nated set of architectural constraints that restricts the roles and features of architectural elements, and the allowed relationships among those elements, within any architecture that conforms to the style. Thus, a style provides a name by which we can refer to a packaged set of architectural design decisions and the set of architectural properties that are induced by applying the style.REST is a coordinated set of architectural constraints that attempts to min- imize latency and network communication, while at the same time maximiz- ing the independence and scalability of component implementations. This is achieved by placing constraints on connector semantics, where other styles have focused on component semantics. REST enables the caching and reuse of interactions, dynamic substitutability of components, and processing of actions by intermediaries, in order to meet the needs of an Internet-scale distributed hypermedia system.The first edition of REST was developed between October 1994 and August 1995, primarily as a means for communicating Web concepts while developing the HTTP/1.0 specification and the initial HTTP/1.1 proposal. It was iteratively improved over the next five years and applied to various revisions and exten- sions of the Web protocol standards. REST was originally referred to as the “HTTP object model,” but that name often led to its misinterpretation as the implementation model of an HTTP server. The name “Representational State Transfer” is intended to evoke an image of how a well-designed Web application behaves: a network of Web pages forms a virtual state machine, allowing a user to progress through the application by selecting a link or submitting a short data-entry form, with each action resulting in a transition to the next state of the application by transferring a representation of that state to the user.The modern Web is one instance of a REST-style architecture. Although Web- based applications can include access to other styles of interaction, the central focus of its protocol and performance concerns is distributed hypermedia. RESTACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 117elaborates only those portions of the architecture that are considered essential for Internet-scale distributed hypermedia interaction. Areas for improvement of the Web architecture can be seen where existing protocols fail to express all of the potential semantics for component interaction, and where the details of syntax can be replaced with more efficient forms without changing the archi- tecture capabilities. Likewise, proposed extensions can be compared to REST to see if they fit within the architecture; if not, it is usually more efficient to redi- rect that functionality to a system running in parallel with a more applicable architectural style.This article presents REST after many years of work on architectural stan- dards for the modern (post-1993) Web. It does not present the details of the Web architecture, since those are found within the standards. Instead, we focus on the rationale behind the modern Web’s architectural design and the software engineering principles upon which it is based. In the process, we identify areas where the Web protocols have failed to match the style, the extent to which these failures can be fixed within the immediate future via protocol enhance- ments, and the lessons learned from using an interaction style to guide the design of a distributed architecture.2. WWW REQUIREMENTSArchitecting the Web requires an understanding of its requirements. Berners- Lee [1996] writes that the “Web’s major goal was to be a shared information space through which people and machines could communicate.” What was needed was a way for people to store and structure their own information, whether permanent or ephemeral in nature, such that it could be usable by themselves and others, and to be able to reference and structure the informa- tion stored by others so that it would not be necessary for everyone to keep and maintain local copies.The intended end-users of this system were located around the world, at var- ious university and government high-energy physics research labs connected via the Internet. Their machines were a heterogeneous collection of terminals, workstations, servers, and supercomputers, requiring a hodge podge of oper- ating system software and file formats. The information ranged from personal research notes to organizational phone listings. The challenge was to build a system that would provide a universally consistent interface to this structured information, available on as many platforms as possible, and incrementally deployable as new people and organizations joined the project.2.1 Low Entry-BarrierSince participation in the creation and structuring of information was volun- tary, a low entry barrier was necessary to enable sufficient adoption. This ap- plied to all users of the Web architecture: readers, authors, and application developers.Hypermedia was chosen as the user interface due to its simplicity and gener- ality: the same interface can be used regardless of the information source, theACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
118 • R. T. Fielding and R. N. Taylorflexibility of hypermedia relationships (links) allows for unlimited structuring, and the direct manipulation of links allows the complex relationships within the information to guide the reader through an application. Since information within large databases is often much easier to access via a search interface rather than browsing, the Web also incorporated the ability to perform simple queries by providing user-entered data to a service and rendering the result as hypermedia.For authors, the primary requirement was that partial availability of the overall system must not prevent the authoring of content. The hypertext au- thoring language had to be simple and capable of being created using existing editing tools. Authors were expected to keep such things as personal research notes in this format, whether directly connected to the Internet or not, so the fact that some referenced information was unavailable, either temporarily or permanently, could not be allowed to prevent the reading and authoring of available information. For similar reasons, it was necessary to be able to cre- ate references to information before the target of that reference was available. Since authors were encouraged to collaborate in the development of informa- tion sources, references needed to be easy to communicate, whether in the form of email directions or written on the back of a napkin at a conference.Simplicity was also a goal for the sake of application developers. Since all of the protocols were defined as text, communication could be viewed and inter- actively tested using existing network tools. This enabled early adoption of the protocols to take place in spite of the lack of standards.2.2 ExtensibilityWhile simplicity makes it possible to deploy an initial implementation of a distributed system, extensibility allows us to avoid getting stuck forever with the limitations of what was deployed. Even if it were possible to build a software system that perfectly matches the requirements of its users, those requirements will change over time, just as society changes over time. A system intending to be as long-lived as the Web must be prepared for change.2.3 Distributed HypermediaHypermedia is defined by the presence of application control information em- bedded within, or as a layer above, the presentation of information. Distributed hypermedia allows the presentation and control information to be stored at re- mote locations. By its nature, user actions within a distributed hypermedia system require the transfer of large amounts of data from where the data is stored to where it is used. Thus, the Web architecture must be designed for large-grain data transfer.The usability of hypermedia interaction is highly sensitive to user-perceived latency: the time between selecting a link and the rendering of a usable result. Since the Web’s information sources are distributed across the global Internet, the architecture needs to minimize network interactions (round-trips within the data transfer protocols).ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
2.4 Internet-ScaleThe Web is intended to be an Internet-scale distributed hypermedia system, which means considerably more than just geographical dispersion. The Internet is about interconnecting information networks across multiple organizational boundaries. Suppliers of information services must be able to cope with the demands of anarchic scalability and the independent deployment of software components.2.4.1 Anarchic Scalability. Most software systems are created with the implicit assumption that the entire system is under the control of one entity, or at least that all entities participating within a system are acting towards a common goal and not at cross-purposes. Such an assumption cannot be safely made when the system runs openly on the Internet. Anarchic scalability refers to the need for architectural elements to continue operating when subjected to an unanticipated load, or when given malformed or maliciously constructed data, since they may be communicating with elements outside their organiza- tional control. The architecture must be amenable to mechanisms that enhance visibility and scalability.The anarchic scalability requirement applies to all architectural elements. Clients cannot be expected to maintain knowledge of all servers. Servers cannot be expected to retain knowledge of state across requests. Hypermedia data elements cannot retain “back-pointers,” an identifier for each data element that references them, since the number of references to a resource is proportional to the number of people interested in that information. Particularly newsworthy information can also lead to “flash crowds”: sudden spikes in access attempts as news of its availability spreads across the world.Security of the architectural elements, and the platforms on which they op- erate, also becomes a significant concern. Multiple organizational boundaries imply that multiple trust boundaries could be present in any communication. Intermediary applications, such as firewalls, should be able to inspect the ap- plication interactions and prevent those outside the security policy of the orga- nization from being acted upon. The participants in an application interaction should either assume that any information received is untrusted, or require some additional authentication before trust can be given. This requires that the architecture be capable of communicating authentication data and autho- rization controls. However, since authentication degrades scalability, the archi- tecture’s default operation should be limited to actions that do not need trusted data: a safe set of operations with well-defined semantics.2.4.2 Independent Deployment. Multiple organizational boundaries also mean that the system must be prepared for gradual and fragmented change, where old and new implementations co-exist without preventing the new imple- mentations from making use of their extended capabilities. Existing architec- tural elements need to be designed with the expectation that later architectural features will be added. Likewise, older implementations need to be easily iden- tified, so that legacy behavior can be encapsulated without adversely impacting newer architectural elements. The architecture as a whole must be designed toACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.Modern Web Architecture • 119
120 • R. T. Fielding and R. N. TaylorFig. 1. REST derivation by style constraints.ease the deployment of architectural elements in a partial, iterative fashion, since it is not possible to force deployment in an orderly manner.2.5 Evolving RequirementsEach of these project goals and information system characteristics fed into the design of the Web’s architecture. As the Web has matured, additional goals have been added to support greater collaboration and distributed authoring [Fielding et al. 1998]. The introduction of each new goal presents us with a challenge: how do we introduce a new set of functionality to an architecture that is already widely deployed, and how do we ensure that its introduction does not adversely impact, or even destroy, the architectural properties that have enabled the Web to succeed? These questions motivated our development of the REST architec- tural style.3. DERIVING REST AS A HYBRID ARCHITECTURAL STYLEThe REST architectural style consists of a set of architectural constraints cho- sen for the properties they induce on candidate architectures. Although each of these constraints can be considered in isolation, describing them in terms of their derivation from common architectural styles makes it easier to un- derstand the rationale behind their selection. Figure 1 depicts the derivation of REST’s constraints graphically in terms of the network-based architectural styles examined in Fielding [2000]. The relevant base styles from which REST was derived include replicated repository (RR), cache ($), client-server (CS), lay- ered system (LS), stateless (S), virtual machine (VM), code on demand (COD), and uniform interface (U).The null style is simply an empty set of constraints. From an architectural perspective, the null style describes a system in which there are no distin- guished boundaries between components. It is the starting point for our de- scription of REST.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.￼
Modern Web Architecture • 121The first constraints added to our hybrid style are those of the client-server architectural style (CS). Separation of concerns is the principle behind the client-server constraints. By separating the user interface concerns from the data storage concerns, we improve the portability of the user interface across multiple platforms and improve scalability by simplifying the server compo- nents. Perhaps most significant to the Web, however, is that the separation allows the components to evolve independently, thus supporting the Internet- scale requirement of multiple organizational domains.We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style, such that each request from client to server must contain all of the information nec- essary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. This constraint induces the properties of visibility, reliability, and scalability. Visi- bility is improved because a monitoring system does not have to look beyond a single request datum in order to determine the full nature of the request. Reli- ability is improved because it eases the task of recovering from partial failures [Waldo et al. 1994]. Scalability is improved because not having to store state between requests allows the server component to quickly free resources, and further simplifies implementation because the server doesn’t have to manage resource usage across requests.Like most architectural choices, the stateless constraint reflects a design tradeoff. The disadvantage is that it may decrease network performance by increasing the repetitive data (per-interaction overhead) sent in a series of re- quests, since that data cannot be left on the server in a shared context. In addi- tion, placing the application state on the client-side reduces the server’s control over consistent application behavior, since the application becomes dependent on the correct implementation of semantics across multiple client versions.In order to improve network efficiency, we add cache constraints to form the client-cache-stateless-server style (C$SS). Cache constraints require that the data within a response to a request be implicitly or explicitly labeled as cacheable or noncacheable. If a response is cacheable, then a client cache is given the right to reuse that response data for later, equivalent, requests.The advantage of adding cache constraints is that they have the potential to partially or completely eliminate some interactions, improving efficiency, scalability, and user-perceived performance by reducing the average latency of a series of interactions. The tradeoff, however, is that a cache can decrease reliability if stale data within the cache differs significantly from the data that would have been obtained had the request been sent directly to the server.The early Web architecture was defined by the client-cache-stateless-server set of constraints. That is, the design rationale presented for the Web architec- ture prior to 1994 focused on stateless client-server interaction for the exchange of static documents over the Internet. The protocols for communicating inter- actions had rudimentary support for nonshared caches, but did not constrain the interface to a consistent set of semantics for all resources. Instead, the Web relied on the use of a common client-server implementation library (CERN libwww) to maintain consistency across Web applications.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
122 • R. T. Fielding and R. N. TaylorDevelopers of Web implementations had already exceeded the early design. In addition to static documents, requests could identify services that dynam- ically generated responses, such as image maps and server-side scripts. Work had also begun on intermediary components, in the form of proxies [Luotonen and Altis 1994] and shared caches [Glassman 1994], but extensions to the pro- tocols were needed in order for them to communicate reliably. The remaining constraints were added to the Web’s architectural style in order to guide the extensions that form the modern Web architecture.The central feature that distinguishes the REST architectural style from other network-based styles is its emphasis on a uniform interface between components. By applying the software engineering principle of generality to the component interface, the overall system architecture is simplified and the visibility of interactions is improved. Implementations are decoupled from the services they provide, which encourages independent evolvability. The trade- off, though, is that a uniform interface degrades efficiency, since information is transferred in a standardized form rather than one which is specific to an application’s needs. The REST interface is designed to be efficient for large- grain hypermedia data transfer, optimizing for the common case of the Web, but resulting in an interface that is not optimal for other forms of architectural interaction.In order to obtain a uniform interface, multiple architectural constraints are needed to guide the behavior of components. REST is defined by four inter- face constraints: identification of resources; manipulation of resources through representations; self-descriptive messages; and, hypermedia as the engine of application state. These constraints are discussed in Section 4.In order to further improve behavior for Internet-scale requirements, we add layered system constraints. The layered system style (LS) allows an architec- ture to be composed of hierarchical layers by constraining component behavior such that each component cannot “see” beyond the immediate layer with which they are interacting. By restricting knowledge of the system to a single layer, we place a bound on the overall system complexity and promote substrate inde- pendence. Layers can be used to encapsulate legacy services and to protect new services from legacy clients, simplifying components by moving infrequently used functionality to a shared intermediary. Intermediaries can also be used to improve system scalability by enabling load balancing of services across mul- tiple networks and processors.The primary disadvantage of layered systems is that they add overhead and latency to the processing of data, reducing user-perceived performance [Clark and Tennenhouse 1990]. For a network-based system that supports cache con- straints, this can be offset by the benefits of shared caching at intermediaries. Placing shared caches at the boundaries of an organizational domain can result in significant performance benefits [Wolman et al. 1999]. Such layers also allow security policies to be enforced on data crossing the organizational boundary, as is required by firewalls [Luotonen and Altis 1994].The combination of layered system and uniform interface constraints induces architectural properties similar to those of the uniform pipe-and-filter style. Although REST interaction is two-way, the large-grain dataflows of hypermediaACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 123interaction can each be processed like a dataflow network, with filter compo- nents selectively applied to the data stream in order to transform the con- tent as it passes [Brooks et al. 1995]. Within REST, intermediary components can actively transform the content of messages because the messages are self- descriptive and their semantics are visible to intermediaries.The final addition to our constraint set for REST comes from the code-on- demand style (COD). REST allows client functionality to be extended by down- loading and executing code in the form of applets or scripts. This simplifies clients by reducing the number of features required to be preimplemented. Al- lowing features to be downloaded after deployment improves system extensi- bility. However, it also reduces visibility, and thus is only an optional constraint within REST.The notion of an optional constraint may seem like an oxymoron. However, it does have a purpose in the architectural design of a system that encompasses multiple organizational boundaries. It means that the architecture only gains the benefit (and suffers the disadvantages) of the optional constraints when they are known to be in effect for some realm of the overall system. For ex- ample, if all of the client software within an organization is known to support JavaTM applets [Flanagan 1999], then services within that organization can be constructed such that they gain the benefit of enhanced functionality via downloadable JavaTM classes. At the same time, however, the organization’s firewall may prevent the transfer of JavaTM applets from external sources, and thus to the rest of the Web it will appear as if those clients do not support code- on-demand. An optional constraint allows us to design an architecture that supports the desired behavior in the general case, but with the understanding that it may be disabled within some contexts.4. REST ARCHITECTURAL ELEMENTSThe Representational State Transfer (REST) style is an abstraction of the ar- chitectural elements within a distributed hypermedia system. Perry and Wolf [1992] distinguish three classes of architectural elements: processing elements (a.k.a., components), data elements, and connecting elements (a.k.a., connec- tors). REST ignores the details of component implementation and protocol syn- tax in order to focus on the roles of components, the constraints upon their interaction with other components, and their interpretation of significant data elements. It establishes the fundamental constraints upon components, con- nectors, and data that define the basis of the Web architecture, and thus the essence of its behavior as a network-based application.Using the software architecture framework of Perry and Wolf [1992], we first define the architectural elements of REST and then examine sample process, connector, and data views of prototypical architectures to gain a better under- standing of REST’s design principles.4.1 Data ElementsUnlike the distributed object style [Chin and Chanson 1991], where all data is encapsulated within and hidden by the processing components, the nature andACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
124• R. T. Fielding and R. N. TaylorTable I. REST Data Elements￼Data Element resourceresource identifier representation representation metadata resource metadata control dataModern Web Examplesthe intended conceptual target of a hypertext referenceURL, URNHTML document, JPEG image media type, last-modified time source link, alternates, vary if-modified-since, cache-control￼￼state of an architecture’s data elements is a key aspect of REST. The rationale for this design can be seen in the nature of distributed hypermedia.When a link is selected, information needs to be moved from the location where it is stored to the location where it will be used by, in most cases, a human reader. This is in distinct contrast to most distributed processing paradigms [Andrews 1991; Fuggetta et al. 1998], where it is often more efficient to move the “processing entity” to the data rather than move the data to the proces- sor. A distributed hypermedia architect has only three fundamental options: (1) render the data where it is located and send a fixed-format image to the recipient; (2) encapsulate the data with a rendering engine and send both to the recipient; or (3) send the raw data to the recipient along with metadata that describes the data type, so that the recipient can choose their own rendering engine.Each option has its advantages and disadvantages. Option 1, the traditional client/server style [Sinha 1992], allows all information about the true nature of the data to remain hidden within the sender, preventing assumptions from being made about the data structure and making client implementation eas- ier. However, it also severely restricts the functionality of the recipient and places most of the processing load on the sender, leading to scalability problems. Option 2, the mobile object style [Fuggetta et al. 1998], provides information hiding while enabling specialized processing of the data via its unique render- ing engine, but limits the functionality of the recipient to what is anticipated within that engine, and may vastly increase the amount of data transferred. Option 3 allows the sender to remain simple and scalable while minimizing the bytes transferred, but loses the advantages of information hiding and requires that both sender and recipient understand the same data types.REST provides a hybrid of all three options by focusing on a shared un- derstanding of data types with metadata, but limiting the scope of what is revealed to a standardized interface. REST components communicate by trans- ferring a representation of the data in a format matching one of an evolving set of standard data types, selected dynamically based on the capabilities or desires of the recipient and the nature of the data. Whether the repre- sentation is in the same format as the raw source, or is derived from the source, remains hidden behind the interface. The benefits of the mobile object style are approximated by sending a representation that consists of instruc- tions in the standard data format of an encapsulated rendering engine (e.g., JavaTM). REST therefore gains the separation of concerns of the client/server style without the server scalability problem, allows information hiding throughACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 125a generic interface to enable encapsulation and evolution of services, and provides for a diverse set of functionality through downloadable feature- engines.4.1.1 Resources and Resource Identifiers. The key abstraction of informa- tion in REST is a resource. Any information that can be named can be a resource: a document or image, a temporal service (e.g., “today’s weather in Los Angeles”), a collection of other resources, a nonvirtual object (e.g., a person), and so on. In other words, any concept that might be the target of an author’s hypertext reference must fit within the definition of a resource. A resource is a conceptual mapping to a set of entities, not the entity that corresponds to the mapping at any particular point in time.More precisely, a resource R is a temporally varying membership function M R (t ), which for time t maps to a set of entities, or values, which are equivalent. The values in the set may be resource representations and/or resource identifiers. A resource can map to the empty set, which allows references to be made to a concept before any realization of that concept exists—a notion that was foreign to most hypertext systems prior to the Web [Grønbaek and Trigg 1994]. Some resources are static in the sense that, when examined at any time after their creation, they always correspond to the same value set. Others have a high degree of variance in their value over time. The only thing that is required to be static for a resource is the semantics of the mapping, since the semantics is what distinguishes one resource from another.For example, the “authors’ preferred version” of this paper is a mapping that has changed over time, whereas a mapping to “the paper published in the proceedings of conference X ” is static. These are two distinct resources, even if they map to the same value at some point in time. The distinction is necessary so that both resources can be identified and referenced independently. A similar example from software engineering is the separate identification of a version-controlled source code file when referring to the “latest revision,” “revision number 1.2.7,” or “revision included with the Orange release.”This abstract definition of a resource enables key features of the Web ar- chitecture. First, it provides generality by encompassing many sources of in- formation without artificially distinguishing them by type or implementation. Second, it allows late binding of the reference to a representation, enabling con- tent negotiation to take place based on characteristics of the request. Finally, it allows an author to reference the concept rather than some singular repre- sentation of that concept, thus removing the need to change all existing links whenever the representation changes.REST uses a resource identifier to identify the particular resource involved in an interaction between components. REST connectors provide a generic in- terface for accessing and manipulating the value set of a resource, regardless of how the membership function is defined or the type of software that is han- dling the request. The naming authority that assigned the resource identifier, making it possible to reference the resource, is responsible for maintaining the semantic validity of the mapping over time (i.e., ensuring that the membership function does not change when its values change).ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
126 • R. T. Fielding and R. N. TaylorTraditional hypertext systems [Grønbaek and Trigg 1994], which typically operate in a closed or local environment, use unique node or document identi- fiers that change every time the information changes, relying on link servers to maintain references separately from the content. Since centralized link servers are anathema to its immense scale and multiorganizational domain require- ments, REST relies instead on the author choosing a resource identifier that best fits the nature of the concept being identified. Naturally, the quality of an identifier is often proportional to the amount of money spent to retain its valid- ity, which leads to broken links as ephemeral (or poorly supported) information moves or disappears over time.4.1.2 Representations. REST components perform actions on a resource by using a representation to capture the current or intended state of that resource and transferring that representation between components. A representation is a sequence of bytes, plus representation metadata to describe those bytes. Other commonly used but less precise names for a representation include document, file, and HTTP message entity, instance, or variant.A representation consists of data, metadata describing the data, and, on occasion, metadata to describe the metadata (usually for verifying message integrity). Metadata is in the form of name-value pairs, where the name corre- sponds to a standard that defines the value’s structure and semantics. Response messages may include both representation metadata and resource metadata: in- formation about the resource that is not specific to the supplied representation.Control data defines the purpose of a message between components, such as the action being requested or the meaning of a response. It is also used to parameterize requests and override the default behavior of some connecting elements. For example, cache behavior can be modified by control data included in the request or response message.Depending on the message control data, a given representation may indicate the current state of the requested resource, the desired state for the requested resource, or the value of some other resource, such as a representation of the input data within a client’s query form, or a representation of some error condi- tion for a response. For example, remote authoring of a resource requires that the author send a representation to the server, thus establishing a value for that resource which can be retrieved by later requests. If the value set of a re- source at a given time consists of multiple representations, content negotiation may be used to select the best representation for inclusion in a given message.The data format of a representation is known as a media type [Postel 1996]. A representation can be included in a message and processed by the recipient according to the control data of the message and the nature of the media type. Some media types are intended for automated processing, some to be rendered for viewing by a user, and a few are appropriate for both. Composite media types can be used to enclose multiple representations in a single message.The design of a media type can directly impact the user-perceived perfor- mance of a distributed hypermedia system. Any data that must be received before the recipient can begin rendering the representation adds to the latency of an interaction. A data format that places the most important renderingACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 127information up front, such that the initial information can be incrementally rendered while the rest of the information is being received, results in much better user-perceived performance than a data format that must be received entirely before rendering can begin.For example, a Web browser that can incrementally render a large HTML document while it is being received provides significantly better user-perceived performance than one that waits until the entire document is received prior to rendering, even though the network performance is the same. Note that the rendering ability of a representation can also be impacted by the choice of content. If the dimensions of dynamically-sized tables and embedded objects must be determined before they can be rendered, their occurrence within the viewing area of a hypermedia page will increase its latency.4.2 ConnectorsREST uses various connector types to encapsulate the activities of accessing re- sources and transferring resource representations. The connectors present an abstract interface for component communication, enhancing simplicity by pro- viding a clean separation of concerns and hiding the underlying implementation of resources and communication mechanisms. The generality of the interface also enables substitutability: if the users’ only access to the system is via an abstract interface, the implementation can be replaced without impacting the users. Since a connector manages network communication for a component, information can be shared across multiple interactions in order to improve ef- ficiency and responsiveness.All REST interactions are stateless. That is, each request contains all of the information necessary for a connector to understand the request, independent of any requests that may have preceded it. This restriction accomplishes four functions: (1) it removes any need for the connectors to retain application state between requests, thus reducing consumption of physical resources and im- proving scalability; (2) it allows interactions to be processed in parallel without requiring that the processing mechanism understand the interaction seman- tics; (3) it allows an intermediary to view and understand a request in isolation, which may be necessary when services are dynamically rearranged; and (4) it forces all of the information that might factor into the reusability of a cached response to be present in each request.The connector interface is similar to procedural invocation, but with impor- tant differences in the passing of parameters and results. The in-parameters consist of request control data, a resource identifier indicating the target of the request, and an optional representation. The out-parameters consist of re- sponse control data, optional resource metadata, and an optional representa- tion. From an abstract viewpoint the invocation is synchronous, but both in- and out-parameters can be passed as data streams. In other words, processing can be invoked before the value of the parameters is completely known, thus avoiding the latency of batch processing large data transfers.The primary connector types are client and server. The essential difference between the two is that a client initiates communication by making a request,ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
128 •R. T. Fielding and R. N. TaylorTable II. REST Connector Types￼Connectorclient libwww, libwww-perlserver libwww, Apache API, NSAPIcache browser cache, Akamai cache network resolver bind (DNS lookup library)tunnel SOCKS, SSL after HTTP CONNECTModern Web Examples￼￼whereas a server listens for connections and responds to requests in order to supply access to its services. A component may include both client and server connectors.A third connector type, the cache connector, can be located on the interface to a client or server connector in order to save cacheable responses to current interactions so that they can be reused for later requested interactions. A cache may be used by a client to avoid repetition of network communication, or by a server to avoid repeating the process of generating a response, with both cases serving to reduce interaction latency. A cache is typically implemented within the address space of the connector that uses it.Some cache connectors are shared, meaning that its cached responses may be used in answer to a client other than the one for which the response was originally obtained. Shared caching can be effective at reducing the impact of “flash crowds” on the load of a popular server, particularly when the caching is arranged hierarchically to cover large groups of users, such as those within a company’s intranet, the customers of an Internet service provider, or univer- sities sharing a national network backbone. However, shared caching can also lead to errors if the cached response does not match what would have been ob- tained by a new request. REST attempts to balance the desire for transparency in cache behavior with the desire for efficient use of the network, rather than assuming that absolute transparency is always required.A cache is able to determine the cacheability of a response because the inter- face is generic rather than specific to each resource. By default, the response to a retrieval request is cacheable and the responses to other requests are noncacheable. If some form of user authentication is part of the request, or if the response indicates that it should not be shared, then the response is only cacheable by a nonshared cache. A component can override these defaults by including control data that marks the interaction as cacheable, noncacheable, or cacheable for only a limited time.A resolver translates partial or complete resource identifiers into the net- work address information needed to establish an intercomponent connection. For example, most URI include a DNS hostname as the mechanism for iden- tifying the naming authority for the resource. In order to initiate a request, a Web browser will extract the hostname from the URI and make use of a DNS resolver to obtain the Internet protocol address for that authority. An- other example is that some identification schemes (e.g., URN [Sollins and Masinter 1994]) require an intermediary to translate a permanent identi- fier to a more transient address in order to access the identified resource. Use of one or more intermediate resolvers can improve the longevity ofACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web ArchitectureTable III. REST Component Types• 129￼Component origin server gateway proxyuser agentModern Web Examples Apache httpd, Microsoft IISSquid, CGI, Reverse ProxyCERN Proxy, Netscape Proxy, Gauntlet Netscape Navigator, Lynx, MOMspider￼￼resource references through indirection, though doing so adds to the request latency.The final form of connector type is a tunnel, which simply relays communi- cation across a connection boundary, such as a firewall or lower-level network gateway. The only reason it is modeled as part of REST and not abstracted away as part of the network infrastructure is that some REST components may dynamically switch from active component behavior to that of a tunnel. The primary example is an HTTP proxy that switches to a tunnel in response to a CONNECT method request, thus allowing its client to directly commu- nicate with a remote server using a different protocol, such as TLS, which doesn’t allow proxies. The tunnel disappears when both ends terminate their communication.4.3 ComponentsREST components (processing elements) are typed by their roles in an overall application action.A user agent uses a client connector to initiate a request and becomes the ultimate recipient of the response. The most common example is a Web browser, which provides access to information services and renders service responses according to the application needs.An origin server uses a server connector to govern the namespace for a re- quested resource. It is the definitive source for representations of its resources and must be the ultimate recipient of any request that intends to modify the value of its resources. Each origin server provides a generic interface to its ser- vices as a resource hierarchy. The resource implementation details are hidden behind the interface.Intermediary components act as both a client and a server in order to forward, with possible translation, requests and responses. A proxy component is an intermediary selected by a client to provide interface encapsulation of other services, data translation, performance enhancement, or security protection. A gateway (a.k.a., reverse proxy) component is an intermediary imposed by the network or origin server to provide an interface encapsulation of other services, for data translation, performance enhancement, or security enforcement. Note that the difference between a proxy and a gateway is that a client determines when it will use a proxy.5. REST ARCHITECTURAL VIEWSNow that we have an understanding of the REST architectural elements in isolation, we can use architectural views [Perry and Wolf 1992] to describe howACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
130 • R. T. Fielding and R. N. TaylorFig. 2. Process view of a REST-based architecture at one instance in time. A user agent is portrayed in the midst of three parallel interactions: a, b, and c. The interactions were not satisfied by the user agent’s client connector cache, so each request has been routed to the resource origin according to the properties of each resource identifier and the configuration of the client connector. Request (a) has been sent to a local proxy, which in turn accesses a caching gateway found by DNS lookup, which forwards the request on to be satisfied by an origin server whose internal resources are defined by an encapsulated object request broker architecture. Request (b) is sent directly to an origin server, which is able to satisfy the request from its own cache. Request (c) is sent to a proxy that is capable of directly accessing WAIS, an information service that is separate from the Web architecture, and translating the WAIS response into a format recognized by the generic connector interface. Each component is only aware of the interaction with their own client or server connectors; the overall process topology is an artifact of our view.the elements work together to form an architecture. All three types of view— process, connector, and data—are useful for illuminating the design principles of REST.5.1 Process ViewA process view of an architecture is primarily effective at eliciting the interac- tion relationships among components by revealing the path of data as it flows through the system. Unfortunately, the interaction of a real system usually in- volves an extensive number of components, resulting in an overall view that is obscured by the details. Figure 2 provides a sample of the process view from a REST-based architecture at a particular instance during the processing of three parallel requests.The client/server [Andrews 1991] separation of concerns simplifies compo- nent implementation, reduces the complexity of connector semantics, improves the effectiveness of performance tuning, and increases the scalability of pure server components.Since the components are connected dynamically, their arrangement and function for a particular application action has characteristics similar to a pipe-and-filter style. Although REST components communicate via bidirec- tional streams, the processing of each direction is independent and thereforeACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.￼
Modern Web Architecture • 131susceptible to stream transducers (filters). The generic connector interface al- lows components to be placed on the stream based on the properties of each request or response.Services may be implemented using a complex hierarchy of intermediaries and multiple distributed origin servers. The stateless nature of REST allows each interaction to be independent of the others, removing the need for an awareness of the overall component topology, an impossible task for an Internet- scale architecture, and allowing components to act as either destinations or intermediaries, determined dynamically by the target of each request. Con- nectors need only be aware of each other’s existence during the scope of their communication. A connector may cache the existence and capabilities of other components for performance reasons.5.2 Connector ViewA connector view of an architecture concentrates on the mechanics of the com- munication between components. For a REST-based architecture, we are par- ticularly interested in the constraints that define the generic resource interface.Client connectors examine the resource identifier in order to select an ap- propriate communication mechanism for each request. For example, a client may be configured to connect to a specific proxy component, perhaps one act- ing as an annotation filter, when the identifier indicates that it is a local re- source. Likewise, a client can be configured to reject requests for some subset of identifiers.Although the Web’s primary transfer protocol is HTTP, the architecture in- cludes seamless access to resources that originate on many pre-existing network servers, including FTP [Postel and Reynolds 1985], Gopher [Anklesaria et al. 1993], and WAIS [Davis et al. 1990]. However, interaction with these services is restricted to the semantics of a REST connector. This constraint sacrifices some of the advantages of other architectures, such as the stateful interaction of a relevance feedback protocol like WAIS, in order to retain the advantages of a single, generic interface for connector semantics. This generic interface makes it possible to access a multitude of services through a single proxy connection. If an application needs the additional capabilities of another architecture, it can implement and invoke those capabilities as a separate system running in par- allel, similar to how the Web architecture interfaces with “telnet” and “mailto” resources.5.3 Data ViewA data view of an architecture reveals the application state as information flows through the components. Since REST is specifically targeted at distributed in- formation systems, it views an application as a cohesive structure of information and control alternatives through which a user can perform a desired task. For example, an online dictionary is one application, as is a museum tour or a set of class notes.Component interactions occur in the form of dynamically-sized messages. Small- or medium-grain messages are used for control semantics, but the bulkACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
132 • R. T. Fielding and R. N. Taylorof application work is accomplished via large-grain messages containing a com- plete resource representation. The most frequent form of request semantics is retrieving a representation of a resource (e.g., the “GET” method in HTTP), which can often be cached for later reuse.REST concentrates all of the control state into the representations received in response to interactions. The goal is to improve server scalability by elim- inating any need for the server to maintain an awareness of the client state beyond the current request. An application’s state is therefore defined by its pending requests, the topology of connected components (some of which may be filtering buffered data), the active requests on those connectors, the dataflow of representations in response to those requests, and the processing of those representations as they are received by the user agent.An application reaches a steady-state whenever it has no outstanding re- quests; i.e., it has no pending requests and all of the responses to its current set of requests have been completely received or received to the point where they can be treated as a representation data stream. For a browser application, this state corresponds to a “web page,” including the primary representation and ancillary representations, such as in-line images, embedded applets, and style sheets. The significance of application steady-states is seen in their im- pact on both user-perceived performance and the burstiness of network request traffic.The user-perceived performance of a browser application is determined by the latency between steady states: the period of time between the selection of a hypermedia link or submit button on one Web page and the point when us- able information has been rendered for the next Web page. The optimization of browser performance is therefore centered around reducing this latency, which leads to the following observations:—The most efficient network request is one that doesn’t use the network. In other words, reusing a cached response results in the best performance. Al- though use of a cache adds some latency to each individual request due to lookup overhead, the average request latency is significantly reduced when even a small percentage of requests result in usable cache hits.—The next control state of the application resides in the representation of the first requested resource, so obtaining that first representation is a priority.—Incremental rendering of the first nonredirect response representation can considerably reduce latency, since then the representation can be rendered as it is being received rather than after the response has been completed. Incremental rendering is impacted by the design of the media type and the early availability of layout information (visual dimensions of in-line objects).The application state is controlled and stored by the user agent and can be composed of representations from multiple servers. In addition to freeing the server from the scalability problems of storing state, this allows the user to directly manipulate the state (e.g., a Web browser’s history), anticipate changes to that state (e.g., link maps and prefetching of representations), and jump from one application to another (e.g., bookmarks and URI-entry dialogs).ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 133The model application is therefore an engine that moves from one state to the next by examining and choosing from among the alternative state transitions in the current set of representations. Not surprisingly, this exactly matches the user interface of a hypermedia browser. However, the style does not assume that all applications are browsers. In fact, the application details are hidden from the server by the generic connector interface, and thus a user agent could equally be an automated robot performing information retrieval for an indexing service, a personal agent looking for data that matches certain criteria, or a maintenance spider busy patrolling the information for broken references or modified content [Fielding 1994].6. RELATED WORKGarlan and Shaw [1993] provide an introduction to software architecture re- search and describe several “pure” styles. Their work differs significantly from the framework of Perry and Wolf [1992] used in this article due to a lack of consideration for data elements. As observed above, the characteristics of data elements are fundamental to understanding the modern Web architecture—it simply cannot be adequately described without them. The same conclusion can be seen in the comparison of mobile code paradigms by Fuggetta et al. [1998], where the analysis of when to go mobile depends on active comparison of the size of the code that would be transferred versus the preprocessed information that would otherwise be transferred.Bass et al. [1998] devote a chapter on architecture for the World Wide Web, but their description only encompasses the implementation architecture within the CERN/W3C-developed libwww (client and server libraries) and Jigsaw soft- ware. Although those implementations reflect some of the design constraints of REST, having been developed by people familiar with the intended architectural style, the real WWW architecture is independent of any single implementation. The Web is defined by its standard interfaces and protocols, not how those in- terfaces and protocols are implemented in a given piece of software.The REST style draws from many preexisting distributed process paradigms [Andrews 1991; Fuggetta et al. 1998], communication protocols, and software fields. REST component interactions are structured in a layered client-server style, but the added constraints of the generic resource interface create the opportunity for substitutability and inspection by intermediaries. Requests and responses have the appearance of a remote invocation style, but REST messages are targeted at a conceptual resource rather than an implementation identifier.Several attempts have been made to model the Web architecture as a form of distributed file system (e.g., WebNFS) or as a distributed object system [Manola 1999]. However, they exclude various Web resource types or implementation strategies as being “not interesting,” when in fact their presence invalidates the assumptions that underlie such models. REST works well because it does not limit the implementation of resources to certain predefined models, allow- ing each application to choose an implementation that best matches its own needs and enabling the replacement of implementations without impacting the user.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
134 • R. T. Fielding and R. N. TaylorThe interaction method of sending representations of resources to consum- ing components has some parallels with event-based integration (EBI) styles [Barrett et al. 1996; Rosenblum and Wolf 1997; Sullivan and Notkin 1992]. The key difference is that EBI styles are push-based. The component containing the state (equivalent to an origin server in REST) issues an event whenever the state changes, whether or not any component is actually interested in or listening for such an event. In the REST style, consuming components usually pull representations. Although this is less efficient when viewed as a single client wishing to monitor a single resource, the scale of the Web makes an unregulated push model infeasible.The principled use of the REST style in the Web, with its clear notion of com- ponents, connectors, and representations, relates closely to the C2 architectural style [Taylor et al. 1996]. The C2 style supports the development of distributed, dynamic applications by focusing on structured use of connectors to obtain sub- strate independence. C2 applications rely on asynchronous notification of state changes and request messages. As with other event-based schemes, C2 is nom- inally push-based, though a C2 architecture could operate in REST’s pull style by only emitting a notification upon receipt of a request. However, the C2 style lacks the intermediary-friendly constraints of REST, such as the generic re- source interface, guaranteed stateless interactions, and intrinsic support for caching.7. EXPERIENCE AND EVALUATIONIn an ideal world, the implementation of a software system would exactly match its design. Some features of the modern Web architecture do correspond exactly to their design criteria in REST, such as the use of URI [Berners-Lee et al. 1998] as resource identifiers and the use of Internet media types [Postel 1996] to identify representation data formats. However, there are also some aspects of the modern Web protocols that exist in spite of the architectural design, due to legacy experiments that failed (but must be retained for backwards compatibil- ity) and extensions deployed by developers unaware of the architectural style. REST provides a model not only for the development and evaluation of new features, but also for the identification and understanding of broken features.REST is not intended to capture all possible uses of the Web protocol stan- dards. There are applications of HTTP and URI that do not match the applica- tion model of a distributed hypermedia system. The important point, however, is that REST does capture all of those aspects of a distributed hypermedia system that are considered central to the behavioral and performance require- ments of the Web, such that optimizing behavior within the model will result in optimum behavior within the deployed Web architecture. In other words, REST is optimized for the common case, so that the constraints it applies to the Web architecture will also be optimized for the common case.7.1 Rest Applied to URIUniform Resource Identifiers (URI) are both the simplest element of the Web architecture and the most important. URI have been known by manyACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 135names: WWW addresses, Universal Document Identifiers, Universal Resource Identifiers [Berners-Lee 1994], and finally the combination of Uniform Resource Locators (URL) [Berners-Lee et al. 1994] and Names (URN) [Sollins and Masinter 1994] that are collectively referred to as URI. Aside from its name, the URI syntax has remained relatively unchanged since 1992. However, the specification of Web addresses also defines the scope and semantics of what we mean by resource, which has changed since the early Web architecture. REST was used to define the term resource for the URI standard [Berners-Lee et al. 1998], as well as the overall semantics of the generic interface for manipulating resources via their representations.7.1.1 Redefinition of Resource. The early Web architecture defined URI as document identifiers. Authors were instructed to define identifiers in terms of a document’s location on the network. Web protocols could then be used to retrieve that document. However, this definition proved to be unsatisfactory for a number of reasons. First, it suggests that the author is identifying the content transferred, which would imply that the identifier should change whenever the content changes. Second, there exist many addresses that corresponded to a service rather than a document—authors may be intending to direct readers to that service, rather than to any specific result from a prior access of that service. Finally, there exist addresses that do not correspond to a document at some periods of time, as when the document does not yet exist or when the address is being used solely for naming, rather than locating, information.The definition of resource in REST is based on a simple premise: identifiers should change as infrequently as possible. Because the Web uses embedded identifiers rather than link servers, authors need an identifier that closely matches the semantics they intend by a hypermedia reference, allowing the reference to remain static even though the result of accessing that reference may change over time. REST accomplishes this by defining a resource to be the semantics of what the author intends to identify, rather than the value cor- responding to those semantics at the time the reference is created. It is then left to the author to ensure that the identifier chosen for a reference does indeed identify the intended semantics.7.1.2 Manipulating Shadows. Defining resource such that a URI identifies a concept rather than a document leaves us with another question: how does a user access, manipulate, or transfer a concept such that they can get some- thing useful when a hypertext link is selected? REST answers that question by defining the things that are manipulated to be representations of the identified resource, rather than the resource itself. An origin server maintains a mapping from resource identifiers to the set of representations corresponding to each resource. A resource is therefore manipulated by transferring representations through the generic interface defined by the resource identifier.REST’s definition of resource derives from the central requirement of the Web: independent authoring of interconnected hypertext across multiple trust domains. Forcing the interface definitions to match the interface requirements causes the protocols to seem vague, but that is just because the interface beingACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
136 • R. T. Fielding and R. N. Taylormanipulated is only an interface and not an implementation. The protocols are specific about the intent of an application action, but the mechanism behind the interface must decide how that intention affects the underlying implementation of the resource mapping to representations.Information hiding is one of the key software engineering principles that motivates the uniform interface of REST. Because a client is restricted to the manipulation of representations rather than directly accessing the implemen- tation of a resource, the implementation can be constructed in whatever form is desired by the naming authority without impacting the clients that may use its representations. In addition, if multiple representations of the resource exist at the time it is accessed, a content selection algorithm can be used to dynamically select a representation that best fits the capabilities of that client. The disadvantage, of course, is that remote authoring of a resource is not as straightforward as remote authoring of a file.7.1.3 Remote Authoring. The challenge of remote authoring via the Web’s uniform interface is due to the separation between the representation that can be retrieved by a client and the mechanism that might be used on the server to store, generate, or retrieve the content of that representation. An individual server may map some part of its namespace to a filesystem, which in turn maps to the equivalent of an inode that can be mapped into a disk location, but those underlying mechanisms provide a means of associating a resource to a set of representations rather than identifying the resource itself. Many different resources could map to the same representation, while other resources may have no representation mapped at all.In order to author an existing resource, the author must first obtain the specific source resource URI: the set of URI that bind to the handler’s under- lying representation for the target resource. A resource does not always map to a singular file, but all resources that are not static are derived from some other resources, and by following the derivation tree an author can eventu- ally find all of the source resources that must be edited in order to modify the representation of a resource. These same principles apply to any form of de- rived representation, whether it be from content negotiation, scripts, servlets, managed configurations, versioning, etc.7.1.4 Binding Semantics to URI. Semantics are a byproduct of the act of assigning resource identifiers and populating those resources with represen- tations. At no time whatsoever do the server or client software need to know or understand the meaning of a URI—they merely act as a conduit through which the creator of a resource (a human naming authority) can associate rep- resentations with the semantics identified by the URI. In other words, there are no resources on the server; just mechanisms that supply answers across an abstract interface defined by resources. It may seem odd, but this is the essence of what makes the Web work across so many different implementations.It is the nature of every engineer to define things in terms of the charac- teristics of the components that will be used to compose the finished product. The Web doesn’t work that way. The Web architecture consists of constraintsACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 137on the communication model between components, based on the role of each component during an application action. This prevents the components from assuming anything beyond the resource abstraction, thus hiding the actual mechanisms on either side of the abstract interface.7.1.5 REST Mismatches in URI. Like most real-world systems, not all components of the deployed Web architecture obey every constraint in its archi- tectural design. REST has been used both as a means to define architectural im- provements and to identify architectural mismatches. Mismatches occur when, due to ignorance or oversight, a software implementation is deployed that vi- olates the architectural constraints. While mismatches cannot be avoided in general, it is possible to identify them before they become standardized.Although the URI design matches REST’s architectural notion of identifiers, syntax alone is insufficient to force naming authorities to define their own URI according to the resource model. One form of abuse is to include infor- mation that identifies the current user within all of the URI referenced by a hypermedia response representation. Such embedded user ids can be used to maintain session state on the server, to track user behavior by logging their actions, or carry user preferences across multiple actions (e.g., Hyper-G’s gate- ways [Maurer 1996]). However, by violating REST’s constraints, these systems also cause shared caching to become ineffective, reduce server scalability, and result in undesirable effects when a user shares those references with others.Another conflict with the resource interface of REST occurs when software attempts to treat the Web as a distributed file system. Since file systems expose the implementation of their information, tools exist to “mirror” that information across to multiple sites as a means of load balancing and redistributing the content closer to users. However, they can do so only because files have a fixed set of semantics (a named sequence of bytes) that can be duplicated easily. In contrast, attempts to mirror the content of a Web server as files will fail because the resource interface does not always match the semantics of a file system, and because both data and metadata are included within, and significant to, the semantics of a representation. Web server content can be replicated at remote sites, but only by replicating the entire server mechanism and configuration, or by selectively replicating only those resources with representations known to be static (e.g., content distribution networks contract with Web sites to replicate specific resource representations to the “edges” of the overall Internet in order to reduce latency and distribute load away from the origin server).7.2 REST Applied to HTTPThe Hypertext Transfer Protocol (HTTP) has a special role in the Web architec- ture as both the primary application-level protocol for communication between Web components and the only protocol designed specifically for the transfer of resource representations. Unlike URI, there were a large number of changes needed in order for HTTP to support the modern Web architecture. The de- velopers of HTTP implementations have been conservative in their adoption of proposed enhancements, and thus extensions needed to be proven and subjectedACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
138 • R. T. Fielding and R. N. Taylorto standards review before they could be deployed. REST was used to identify problems with the existing HTTP implementations, specify an interoperable subset of that protocol as HTTP/1.0 [Berners-Lee et al. 1996], analyze proposed extensions for HTTP/1.1 [Fielding et al. 1999], and provide motivating rationale for deploying HTTP/1.1.The key problem areas in HTTP that were identified by REST include plan- ning for the deployment of new protocol versions, separating message parsing from HTTP semantics and the underlying transport layer (TCP), distinguish- ing between authoritative and nonauthoritative responses, fine-grained control of caching, and various aspects of the protocol that failed to be self-descriptive. REST has also been used to model the performance of Web applications based on HTTP and anticipate the impact of such extensions as persistent connec- tions and content negotiation. Finally, REST has been used to limit the scope of standardized HTTP extensions to those that fit within the architectural model, rather than allowing the applications that misuse HTTP to influence the stan- dard equally.7.2.1 Extensibility. One of the major goals of REST is to support the grad- ual and fragmented deployment of changes within an already deployed archi- tecture. HTTP was modified to support that goal through the introduction of versioning requirements and rules for extending each of the protocol’s syntax elements.Protocol versioning. HTTP is a family of protocols, distinguished by major and minor version numbers, which share the name primarily because they correspond to the protocol expected when communicating directly with a service based on the “http” URL namespace. A connector must obey the constraints placed on the HTTP-version protocol element included in each message [Mogul et al. 1997].The HTTP version of a message represents the protocol capabilities of the sender and the gross-compatibility (major version number) of the message being sent. This allows a client to use a reduced (HTTP/1.0) subset of features in making a normal HTTP/1.1 request, while at the same time indicating to the recipient that it is capable of supporting full HTTP/1.1 communication. In other words, it provides a tentative form of protocol negotiation on the HTTP scale. Each connection on a request/response chain can operate at its best protocol level in spite of the limitations of some clients or servers that are parts of the chain.The intention of the protocol is that the server should always respond with the highest minor version of the protocol it understands within the same ma- jor version of the client’s request message. The restriction is that the server cannot use those optional features of the higher-level protocol that are forbid- den to send to such an older-version client. There are no required features of a protocol that cannot be used with all other minor versions within that major version, since that would be an incompatible change and thus require a change in the major version. The only features of HTTP that can depend on a minor version number change are those interpreted by immediate neighbors in theACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 139communication, since HTTP does not require that the entire request/response chain of intermediary components speak the same version.These rules exist to assist in deploying multiple protocol revisions and pre- venting the HTTP architects from forgetting that deployment of the protocol is an important aspect of its design. They do so by making it easy to differentiate between compatible changes to the protocol and incompatible changes. Com- patible changes are easy to deploy and communication of the differences can be achieved within the protocol stream. Incompatible changes are difficult to deploy because they require some determination of acceptance of the protocol before the protocol stream can commence.Extensible protocol elements. HTTP includes a number of separate names- paces, each of which has differing constraints, but all of which share the re- quirement of being extensible without bound. Some of the namespaces are governed by separate Internet standards and shared by multiple protocols (e.g., URI schemes [Berners-Lee et al. 1998], media types [Freed et al. 1996], MIME header field names [Freed and Borenstein 1996], charset values, lan- guage tags), while others are governed by HTTP, including the namespaces for method names, response status codes, nonMIME header field names, and values within standard HTTP header fields. Since early HTTP did not define a consistent set of rules for how changes within these namespaces could be deployed, this was one of the first problems tackled by the specification effort.HTTP request semantics are signified by the request method name. Method extension is allowed whenever a standardizable set of semantics can be shared among client, server, and any intermediaries that may be between them. Un- fortunately, early HTTP extensions, specifically the HEAD method, made the parsing of an HTTP response message dependent on knowing the semantics of the request method. This led to a deployment contradiction: if a recipient needs to know the semantics of a method before it can be safely forwarded by an intermediary, then all intermediaries must be updated before a new method can be deployed.This deployment problem was fixed by separating the rules for parsing and forwarding HTTP messages from the semantics associated with new HTTP pro- tocol elements. For example, HEAD is the only method for which the Content- Length header field has a meaning other than signifying the message body length, and no new method can change the message length calculation. GET and HEAD are also the only methods for which conditional request header fields have the semantics of a cache refresh, whereas for all other methods they have the meaning of a precondition.Likewise, HTTP needed a general rule for interpreting new response status codes, such that new responses could be deployed without significantly harm- ing older clients. We therefore expanded upon the rule that each status code belonged to a class signified by the first digit of its three-digit decimal num- ber: 100–199, indicating that the message contains a provisional information response; 200–299, indicating that the request succeeded; 300–399, indicating that the request needs to be redirected to another resource; 400–499, indi- cating that the client made an error that should not be repeated; and 500–599,ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
140 • R. T. Fielding and R. N. Taylorindicating that the server encountered an error, but that the client may get a bet- ter response later (or via some other server). If a recipient does not understand the specific semantics of the status code in a given message, then they must treat it in the same way as the x00 code within its class. Like the rule for method names, this extensibility rule places a requirement on the current architecture such that it anticipates future change. Changes can therefore be deployed onto an existing architecture with less fear of adverse component reactions.Upgrade. The addition of the Upgrade header field in HTTP/1.1 reduces the difficulty of deploying incompatible changes by allowing the client to advertise its willingness for a better protocol while communicating in an older protocol stream. Upgrade was specifically added to support the selective replacement of HTTP/1.x with other, future protocols that might be more efficient for some tasks. Thus, HTTP not only supports internal extensibility, but also complete replacement of itself during an active connection. If the server supports the improved protocol and desires to switch, it simply responds with a 101 status and continues on as if the request were received in that upgraded protocol.7.2.2 Self-Descriptive Messages. REST constrains messages between com- ponents to be self-descriptive in order to support intermediate processing of interactions. However, there were aspects of early HTTP that failed to be self- descriptive, including the lack of host identification within requests; failure to syntactically distinguish between message control data and representation metadata; failure to differentiate between control data intended only for the immediate connection peer versus metadata intended for all recipients; lack of support for mandatory extensions; and the need for metadata to describe representations with layered encodings.Host. One of the worst mistakes in the early HTTP design was the decision not to send the complete URI that is the target of a request message, but rather send only those portions that were not used in setting up the connection. The assumption was that a server would know its own naming authority based on the IP address and TCP port of the connection. However, this failed to anticipate that multiple naming authorities might exist on a single server, which became a critical problem as the Web grew at an exponential rate and new domain names (the basis for naming authority within the http URL namespace) far exceeded the availability of new IP addresses.The solution defined and deployed for both HTTP/1.0 and HTTP/1.1 was to include the target URL’s host information within a Host header field of the request message. Deployment of this feature was considered so important that the HTTP/1.1 specification requires servers to reject any HTTP/1.1 request that doesn’t include a Host field. As a result, there now exist many large ISP servers that run tens of thousands of name-based virtual host Websites on a single IP address.Layered encodings. HTTP inherited its syntax for describing representation metadata from the Multipurpose Internet Mail Extensions (MIME) [Freed and Borenstein 1996]. MIME does not define layered media types, preferring insteadACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 141to only include the label of the outermost media type within the Content-Type field value. However, this prevents a recipient from determining the nature of an encoded message without decoding the layers. An early HTTP extension worked around this failing by listing the outer encodings separately within the Content-Encoding field and placing the label for the innermost media type in the Content-Type. That was a poor design decision, since it changed the seman- tics of Content-Type without changing its field name, resulting in confusion whenever older user agents encountered the extension.A better solution would have been to continue treating Content-Type as the outermost media type, and use a new field to describe the nested types within that type. Unfortunately, the first extension was deployed before its faults were identified.REST did identify the need for another layer of encodings: those placed on a message by a connector in order to improve its transferability over the network. This new layer, called a transfer-encoding—in reference to a similar concept in MIME—allows messages to be encoded for transfer without implying that the representation is encoded by nature. Transfer encodings can be added or re- moved by transfer agents, for whatever reason, without changing the semantics of the representation.Semantic independence. As described above, HTTP message parsing has been separated from its semantics. Message parsing, including finding and cob- bling the header fields, occurs separately from the process of parsing the header field contents. In this way, intermediaries can quickly process and forward HTTP messages, and extensions can be deployed without breaking existing parsers.Transport independence. Early HTTP, including most implementations of HTTP/1.0, used the underlying transport protocol as the means for signaling the end of a response message. A server would indicate the end of a response message body by closing the TCP connection. Unfortunately, this created a significant failure condition in the protocol: a client had no means for distin- guishing between a completed response and one that was truncated by network failure. To solve this, the Content-Length header field was redefined within HTTP/1.0 to indicate the message body length in bytes, whenever the length was known in advance, and the “chunked” transfer encoding was introduced to HTTP/1.1.The chunked encoding allows a representation whose size is unknown at the beginning of its generation (when the header fields are sent) to have its boundaries delineated by a series of chunks that can be individually sized before being sent. It also allows metadata to be sent at the end of the message as trailers, enabling the creation of optional metadata at the origin while the message is being generated, without adding to response latency.Size limits. A frequent barrier to the flexibility of application-layer proto- cols is the tendency to over-specify size limits on protocol parameters. Although some practical limits within implementations of the protocol always exist (e.g., available memory), specifying those limits within the protocol restricts allACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
142 • R. T. Fielding and R. N. Taylorapplications to the same limits, regardless of their implementation. The re- sult is often a lowest-common-denominator protocol that cannot be extended much beyond the vision of its original creator.In the HTTP protocol there is no limit on the length of the URI, the length of header fields, the length of a representation, or the length of any field value that consists of a list of items. Although older Web clients have a well-known problem with URI that consist of more than 255 characters, it is sufficient to note that problem in the HTTP specification rather than require that all servers be so limited. The reason that this does not make for a protocol maximum is that applications within a controlled context (such as an intranet) can avoid those limits by replacing the older components.Although we did not need to invent artificial limitations, HTTP/1.1 did need to define an appropriate set of response status codes for indicating when a given protocol element is too long for a server to process. Such response codes were added for the following conditions: Request-URI too long, header field too long, and body too long. Unfortunately, there is no way for a client to indicate to a server that it may have resource limits, which leads to problems when resource-constrained devices, such as PDAs, attempt to use HTTP without a device-specific intermediary adjusting the communication.Cache control. Because REST tries to balance the need for efficient, low- latency behavior against the desire for semantically transparent cache behav- ior, it is critical that HTTP allow the application to determine the caching re- quirements rather than hard-code it into the protocol itself. The most important thing for the protocol to do is to fully and accurately describe the data being transferred, so that no application is fooled into thinking it has one thing when it actually has something else. HTTP/1.1 does this through the addition of the Cache-Control, Age, Etag, and Vary header fields.Content negotiation. All resources map a request (consisting of method, identifier, request-header fields, and sometimes a representation) to a response (consisting of a status code, response-header fields, and sometimes a represen- tation). When an HTTP request maps to multiple representations on the server, the server may engage in content negotiation with the client in order to deter- mine which one best meets the client’s needs. This is really more of a “content selection” process than negotiation.Although there were several implementations of content negotiation de- ployed, they were not included in the specification of HTTP/1.0 because there was no interoperable subset of implementations at the time it was published. This was partly due to a poor implementation within NCSA Mosaic, which would send 1 KB of preference information in the header fields on every re- quest, regardless of the negotiability of the resource [Spero 1994]. Since far less than 0.01% of all URI are negotiable in content, the result was substan- tially increased request latency for very little gain, which led to later browsers disregarding the negotiation features of HTTP/1.0.Preemptive (server-driven) negotiation occurs when the server varies the response representation for a particular request method*identifier*status-codeACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 143combination according to the value of the request header fields, or something external to the normal request parameters above. The client needs to be notified when this occurs, so that a cache can know when it is semantically transparent to use a particular cached response for a future request, and also so that a user agent can supply more detailed preferences than it might normally send once it knows they are having an effect on the received response. HTTP/1.1 introduced the Vary header field for this purpose. Vary simply lists those request header field dimensions under which the response may vary.In preemptive negotiation, the user agent tells the server what it can accept. The server is then supposed to select the representation that best matches what the user agent claims to be its capabilities. However, this is a nontractable problem because it requires not only information on what the UA will accept, but also how well it accepts each feature and to what purpose users intend to put the representation. For example, users who want to view an image on screen might prefer a simple bitmap representation, but the same users with the same browsers may prefer a PostScript representation if they intend to send it to a printer instead. It also depends on the users correctly configuring their browsers according to their own personal content preferences. In short, a server is rarely able to make effective use of preemptive negotiation, but it was the only form of automated content selection defined by early HTTP.HTTP/1.1 added the notion of reactive (agent-driven) negotiation. In this case, when a user agent requests a negotiated resource, the server responds with a list of the available representations. The user agent can then choose which one is best according to its own capabilities and purposes. The infor- mation about the available representations may be supplied via a separate representation (e.g., a 300 response), inside the response data (e.g., condi- tional HTML), or as a supplement to the “most likely” response. The latter works best for the Web because an additional interaction only becomes neces- sary if the user agent decides one of the other variants would be better. Re- active negotiation is simply an automated reflection of the normal browser model, which means it can take full advantage of all the performance benefits of REST.Both preemptive and reactive negotiation suffer from the difficulty of com- municating the actual characteristics of the representation dimensions (e.g., how to say that a browser supports HTML tables but not the INSERT ele- ment). However, reactive negotiation has the distinct advantages of not having to send preferences on every request, having more context information with which to make a decision when faced with alternatives, and not interfering with caches.A third form of negotiation, transparent negotiation [Holtman and Mutz 1998], is a license for an intermediary cache to act as an agent, on behalf of other agents, for selecting a better representation, and initiating requests to retrieve that representation. The request may be resolved internally by another cache hit, and thus it is possible that no additional network request will be made. In so doing, however, they are performing server-driven negotiation, and must therefore add the appropriate Vary information so that other outbound caches won’t be confused.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
144 • R. T. Fielding and R. N. Taylor7.2.3 Performance. HTTP/1.1 was focused on improving the semantics of communication between components, but there were also some improvements to user-perceived performance, albeit limited by the requirement of syntax com- patibility with HTTP/1.0.Persistent connections. Although early HTTP’s single request/response per connection behavior made for simple implementations, it resulted in inefficient use of the underlying TCP transport, due to the overhead of per-interaction set-up costs and the nature of TCP’s slow-start congestion control algorithm [Heidemann et al. 1997; Spero 1994]. As a result, several extensions were pro- posed to combine multiple requests and responses within a single connection.The first proposal was to define a new set of methods for encapsulating mul- tiple requests within a single message (MGET, MHEAD, etc.) and returning the response as a MIME multipart. This was rejected because it violated sev- eral REST constraints. First, the client would need to know all of the requests it wanted to package before the first request could be written to the network, since a request body must be length-delimited by a content-length field set in the initial request header fields. Second, intermediaries would have to extract each of the messages to determine which ones it could satisfy locally. Finally, it effectively doubles the number of request methods and complicates mecha- nisms for selectively denying access to certain methods.Instead, we adopted a form of persistent connections, which uses length- delimited messages in order to send multiple HTTP messages on a single con- nection [Padmanabhan and Mogul 1995]. For HTTP/1.0, this was done using the “keep-alive” directive within the Connection header field. Unfortunately, this did not work in general because the header field could be forwarded by interme- diaries to other intermediaries that do not understand keep-alive, resulting in a dead-lock condition. HTTP/1.1 eventually settled on making persistent con- nections the default, thus signaling their presence via the HTTP-version value, and only using the connection-directive “close” to reverse the default.It is important to note that persistent connections became possible only after HTTP messages were redefined to be self-descriptive and independent of the underlying transport protocol.Write-through caching. HTTP does not support write-back caching. An HTTP cache cannot assume that what gets written through it is the same as what would be retrievable from a subsequent request for that resource, and thus it cannot cache a PUT request body and reuse it for a later GET response. There are two reasons for this rule: (1) metadata might be generated behind- the-scenes; and (2) access control on later GET requests cannot be determined from the PUT request. However, since write actions using the Web are ex- tremely rare, the lack of write-back caching does not have a significant impact on performance.7.2.4 REST Mismatches in HTTP Extensions. There are several architec- tural mismatches present within HTTP, some due to 3rd-party extensions that were deployed external to the standards process and others due to the necessity of remaining compatible with deployed HTTP/1.0 components.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 145Differentiating nonauthoritative responses. One weakness that still exists in HTTP is that there is no consistent mechanism for differentiating between authoritative responses generated by the origin server in response to the cur- rent request, and nonauthoritative responses obtained from an intermediary or cache without accessing the origin server. The distinction can be important for applications that require authoritative responses, such as the safety-critical information appliances used in the health industry, and for those times when an error response is returned and the client is left wondering whether the error was due to the origin or to some intermediary. Attempts to solve this using ad- ditional status codes did not succeed, since the authoritative nature is usually orthogonal to the response status.HTTP/1.1 did add a mechanism to control cache behavior so that the desire for an authoritative response can be indicated. The “no-cache” directive on a request message requires any cache to forward the request toward the origin server, even if it has a cached copy of what is being requested. This allows a client to refresh a cached copy known to be corrupted or stale. However, using this field on a regular basis interferes with the performance benefits of caching. A more general solution is to require that responses be marked as nonauthoritative whenever an action does not result in contacting the origin server. A Warning response header field was defined in HTTP/1.1 for this purpose (and others), but it has not been widely implemented in practice.Cookies. The introduction of site-wide state information in the form of HTTP cookies [Kristol and Montulli 1997] is an example of an inappropriate extension to the protocol. Cookie interaction fails to match REST’s application state model, often resulting in confusion for the typical browser application.An HTTP cookie is opaque data that can be assigned by the origin server to a user agent by including it within a Set-Cookie response header field, with the intention that the user agent include the same cookie in all future requests to that server until it is replaced or expires. Such cookies typically contain an array of user-specific configuration choices, or a token to be matched against the server’s database in future requests. The problem is that a cookie is defined as being attached to any future requests for a given set of resource identi- fiers, usually encompassing an entire site, rather than being associated with the particular application state (the set of currently rendered representations) on the browser. When the browser’s history function (the “Back” button) is subsequently used to back-up to a view prior to that reflected by the cookie, the browser’s application state no longer matches the stored state within the cookie. Therefore, the next request sent to the same server will contain a cookie that misrepresents the current application context, leading to confusion on both sides.Cookies also violate REST because they allow data to be passed without suffi- ciently identifying its semantics, thus becoming a concern for both security and privacy. The combination of cookies with the Referer [sic] header field makes it possible to track users as they browse between sites.As a result, cookie-based applications on the Web will never be reliable. The same functionality should have been achieved via anonymous authenticationACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
146 • R. T. Fielding and R. N. Taylorand true client-side state. A state mechanism that involves preferences can be more efficiently implemented with judicious use of context-setting URI rather than cookies, where judicious means one URI per state rather than an un- bounded number of URIs due to the embedding of a user-id. Likewise, the use of cookies to identify a user-specific “shopping basket” within a server-side database could be more efficiently implemented by defining the semantics of shopping items within the hypermedia data formats, allowing the user agent to select and store those items within his or her own client-side shopping bas- ket, complete with a URI to be used for check-out when the client is ready to purchase.Mandatory extensions. HTTP header field names can be extended at will, but only when the information they contain is not required for properly un- derstanding the message. Mandatory header field extensions require a major protocol revision or a substantial change to method semantics, such as that proposed in [Nielsen et al. 2000]. This is an aspect of modern Web architecture that does not yet match the self-descriptive messaging constraints of the REST architectural style, primarily because the cost of implementing a mandatory ex- tension framework within the existing HTTP syntax exceeds any clear benefits that might be gained from mandatory extensions. However, it is reasonable to expect that mandatory field name extensions will be supported in the next ma- jor revision of HTTP, when the existing constraints on backwards-compatibility of syntax no longer apply.Mixing metadata. HTTP is designed to extend the generic connector inter- face across a network connection. As such, it is intended to match the character- istics of that interface, including the delineation of parameters as control data, metadata, and representation. However, two of the most significant limitations of the HTTP/1.x protocol family are that it fails to syntactically distinguish be- tween representation metadata and message control information (both trans- mitted as header fields) and does not allow metadata to be effectively layered for message integrity checks.REST identified these as limitations in the protocol early in the standardiza- tion process, anticipating that they would lead to problems in the deployment of other features, such as persistent connections and digest authentication. Workarounds were developed, including adding the Connection header field to identify per-connection control data that is unsafe for forwarding by intermedi- aries, as well as an algorithm for the canonical treatment of header field digests [Franks et al. 1999].MIME syntax. HTTP inherited its message syntax from MIME [Freed and Borenstein 1996] in order to retain commonality with other Internet proto- cols and reuse many of the standardized fields for describing media types in messages. Unfortunately, MIME and HTTP have very different goals, and the syntax is only designed for MIME’s goals.In MIME, user agents send a bunch of information, which is to be treated as a coherent whole, to unknown recipients with whom they never directly inter- act. MIME assumes that the agents would want to send all that informationACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 147in one message, since sending multiple messages across Internet mail is less efficient. Thus, MIME syntax is constructed to package messages within a part or multipart in much the way postal carriers wrap packages in extra paper.In HTTP, packaging different objects within a single message doesn’t make any sense other than for secure encapsulation or packaged archives, since it is more efficient to make separate requests for those documents not already cached. Thus, HTTP applications use media types like HTML as containers for references to the “package”—user agents can then choose what parts of the package to retrieve as separate requests. Although it is possible that HTTP could use a multipart package in which only the non-URI resources were in- cluded after the first part, there hasn’t been much demand for it.The problem with MIME syntax is that it assumes that the transport is lossy, deliberately corrupting things like line breaks and content lengths. Hence the syntax is verbose and inefficient for any system not based on a lossy transport, which makes it inappropriate for HTTP. Since HTTP/1.1 can sup- port deployment of incompatible protocols, retaining the MIME syntax won’t be necessary for the next major version of HTTP, even though it will likely continue to use the many standardized protocol elements for representation metadata.8. CONCLUSIONS AND FUTURE WORKThe World Wide Web is arguably the world’s largest distributed application. Understanding the key architectural principles underlying the Web can help explain its technical success and may lead to improvements in other distributed applications, particularly those amenable to the same or similar methods of interaction.For network-based applications, system performance is dominated by net- work communication. For a distributed hypermedia system, component inter- actions consist of large-grain data transfers rather than computation-intensive tasks. The REST style was developed in response to those needs. Its focus upon the generic connector interface of resources and representations has enabled intermediate processing, caching, and substitutability of components, which in turn has allowed Web-based applications to scale from 100,000 requests/day in 1994 to 600,000,000 requests/day in 1999.The REST architectural style has succeeded in guiding the design and de- ployment of modern Web architecture. It has been validated through develop- ment of the HTTP/1.0 [Berners-Lee et al. 1996] and HTTP/1.1 [Fielding et al. 1999] standards, elaboration of the URI [Berners-Lee et al. 1998] and relative URL [Fielding 1995] standards, and successful deployment of several dozen independently developed, commercial-grade software systems within the mod- ern Web architecture. REST has served as both a model for design guidance and as an acid test for architectural extensions to the Web protocols. To date, there have been no significant problems caused by the introduction of the new standards, even though they have been subject to gradual and fragmented deployment alongside legacy Web applications. Furthermore, the new stan- dards have had a positive effect on the robustness of the Web and enabled newACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
148 • R. T. Fielding and R. N. Taylormethods for improving user-perceived performance through caching hierar- chies and content distribution networks.Future work will focus on extending the architectural guidance toward the development of a replacement for the HTTP/1.x protocol, using a more effi- cient tokenized syntax, but without losing the desirable properties identified by REST. There has also been some interest in extending REST to consider vari- able request priorities, differentiated quality-of-service, and representations consisting of continuous data streams, such as those generated by broadcast audio and video sources. Independent of the original authors, a public discus- sion list and collaboratively authored Web forum [Baker et al. 2002] have been created to further explore the REST architectural style and examine how its constraints might be selectively applied to other software systems.ACKNOWLEDGMENTSThis article is a subset of the first author’s dissertation work [Fielding 2000]. He would like to acknowledge Mark Ackerman for introducing him to the Web developer community and David Rosenblum whose work on Internet-scale soft- ware architectures inspired this research in terms of architecture, rather than simply hypermedia or application-layer protocol design. Kari A. Nies assisted in writing this article by extracting the relevant parts of the dissertation that had been updated since its original publication and helping to make sense of them with less context.The Web’s architectural style was developed iteratively over a six-year pe- riod, but primarily during the first six months of 1995. It was influenced by countless discussions with researchers at UCI, staff at the World Wide Web Consortium (W3C), and engineers within the HTTP and URI working groups of the Internet Engineering Taskforce (IETF). We thank Tim Berners-Lee, Henrik Frystyk Nielsen, Dan Connolly, Dave Raggett, Rohit Khare, Jim Whitehead, Larry Masinter, and Dan LaLiberte for many thoughtful conversations regard- ing the nature and goals of the WWW architecture.REFERENCESANDREWS, G. 1991. Paradigms for process interaction in distributed programs. ACM Computing Surv. 23, 1 (March 1991), 49–90.ANKLESARIA, F., ET AL. 1993. The Internet Gopher protocol (a distributed document search and retrieval protocol). Internet RFC 1436, March 1993.BAKER, M., ET AL. 2002. RESTwiki. &lt;http://conveyor.com/RESTwiki/moin.cgi&gt;, April 2002. BARRETT, D. J., CLARKE, L. A., TARR, P. L., AND WISE, A. E. 1996. A framework for event-basedsoftware integration. ACM Trans. Soft. Eng. Methodol. 5, 4 (Oct. 1996), 378–421.BASS, L., CLEMENTS, P., AND KAZMAN, R. 1996. Software Architecture in Practice. Addison Wesley,Reading, MA.BERNERS-LEE, T. 1994. Universal resource identifiers in WWW. Internet RFC 1630, June 1994. BERNERS-LEE, T., MASINTER, L., AND MCCAHILL, M. 1994. Uniform resource locators (URL). InternetRFC 1738, Dec. 1994.BERNERS-LEE, T. AND CONNOLLY, D. 1995. Hypertext markup language—2.0. Internet RFC 1866,Nov. 1995.BERNERS-LEE, T. 1996. WWW: Past, present, and future. Computer 29, 10 (Oct. 1996), 69–77. BERNERS-LEE, T., FIELDING, R. T., AND NIELSEN, H. F. 1996. Hypertext transfer protocol—HTTP/1.0.Internet RFC 1945, May 1996.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
Modern Web Architecture • 149BERNERS-LEE, T., FIELDING, R. T., AND MASINTER, L. 1998. Uniform resource identifiers (URI): Generic syntax. Internet RFC 2396, Aug. 1998.BROOKS, C., MAZER, M. S., MEEKS, S., AND MILLER, J. 1995. Application-specific proxy servers as HTTP stream transducers. In Proceedings of the Fourth International World Wide Web Conference (Boston, MA, Dec. 1995), 539–548.CHIN, R. S. AND CHANSON, S. T. 1991. Distributed object-based programming systems. ACM Comput. Surv. 23, 1 (March 1991), 91–124.CLARK, D. D. AND TENNENHOUSE, D. L. 1990. Architectural considerations for a new generation of protocols. In Proceedings ACM SIGCOMM‘90 Symposium (Philadelphia, PA, Sept. 1990), 200– 208.DAVIS, F., ET. AL. 1990. WAIS interface protocol prototype functional specification (v.1.5). Thinking Machines Corp., Apr. 1990.FIELDING, R. T. 1995. Relative uniform resource locators. Internet RFC 1808, June 1995. FIELDING, R. T. 1994. Maintaining distributed hypertext infostructures: Welcome to MOMspider’sweb. Comput. Net. ISDN Syst. 27, 2 (Nov. 1994), 193–204.FIELDING, R. T. 2000. Architectural styles and the design of network-based software architec-tures. PhD Dissertation. Dept. of Information and Computer Science, University of California,Irvine.FIELDING, R. T., GETTYS, J., MOGUL, J. C., NIELSEN, H. F., MASINTER, L., LEACH, P., AND BERNERS-LEE, T.1999. Hypertext transfer protocol—HTTP/1.1. Internet RFC 2616, June 1999.FIELDING, R. T., WHITEHEAD, E. J., JR., ANDERSON, K. M., BOLCER, G., OREIZY, P., AND TAYLOR, R. N. 1998. Web-based development of complex information products. Commun. ACM 41, 8 (Aug.1998), 84–92.FLANAGAN, D. 1999. JavaTM in a Nutshell, 3rd ed. O’Reilly &amp; Associates, Sebastopol.FRANKS, J., HALLAM-BAKER, P., HOSTETLER, J., LAWRENCE, S., LEACH, P., LUOTONEN, A., SINK, E., ANDSTEWART, L. 1999. HTTP authentication: Basic and digest access authentication. Internet RFC2617, June 1999.FREED, N. AND BORENSTEIN, N. 1996. Multipurpose internet mail extensions (MIME) Part One:Format of internet message bodies. Internet RFC 2045, Nov. 1996.FREED, N., KLENSIN, J., AND J. POSTEL, J. 1996. Multipurpose internet mail extensions (MIME)Part Four: Registration procedures. Internet RFC 2048, Nov. 1996.FUGGETTA, A., PICCO, G. P., AND VIGNA, G. 1998. Understanding code mobility. IEEE Trans. Soft.Eng. 24, 5 (May 1998), 342–361.GARLAN, D. AND SHAW, M. 1993. An introduction to software architecture. Ambriola and Tortola,eds. In Advances in Software Engineering &amp; Knowledge Engineering, vol. II, World Scientific1993, 1–39.GLASSMAN, S. 1994. A caching relay for the World Wide Web. Comput. Net. ISDN Syst. 27, 2(Nov. 1994), 165–173.GRøNBAEK, K. AND TRIGG, R. H. 1994. Design issues for a Dexter-based hypermedia system.Commun. ACM 37, 2 (Feb. 1994), 41–49.HEIDEMANN, J., OBRACZKA, K., AND TOUCH, J. 1997. Modeling the performance of HTTP over severaltransport protocols. IEEE/ACM Trans. Net. 5, 5 (Oct. 1997), 616–630.HOLTMAN, K. AND MUTZ, A. 1998. Transparent content negotiation in HTTP. Internet RFC 2295,March 1998.KRISTOL, D. AND MONTULLI, L. 1997. HTTP state management mechanism. Internet RFC 2109,Feb. 1997.LUOTONEN, A. AND ALTIS, K. 1994. World-Wide Web proxies. Comput. Net. ISDN Syst. 27, 2(Nov. 1994), 147–154.MANOLA, F. 1999. Technologies for a Web object model. IEEE Internet Comput. 3, 1 (Jan.-Feb.1999), 38–47.MAURER, H. 1996. HyperWave: The Next-Generation Web Solution. Addison-Wesley, Harlow,England, 1996.MOGUL, J., FIELDING, R. T., GETTYS, J., AND NIELSEN, H. F. 1997. Use and interpretation of HTTPversion numbers. Internet RFC 2145, May 1997.NIELSEN, H. F., LEACH, P., AND LAWRENCE, S. 2000. HTTP extension framework, Internet RFC 2774,Feb. 2000.ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.
150 • R. T. Fielding and R. N. TaylorPADMANABHAN, V. N. AND MOGUL, J. C. 1995. Improving HTTP latency. Comput. Net. ISDN Syst. 28 (Dec. 1995), 25–35.PERRY, D. E. AND WOLF, A. 1992. Foundations for the study of software architecture. ACM SIGSOFT Soft. Eng. Notes 17, 4 (Oct. 1992), 40–52.POSTEL, J. 1996. Media type registration procedure. Internet RFC 1590, Nov. 1996.POSTEL, J. AND REYNOLDS, J. 1985. File transfer protocol. Internet STD 9, RFC 959, Oct. 1985. ROSENBLUM, D. S. AND WOLF, A. L. 1997. A design framework for Internet-scale event observationand notification. In Proceedings of the 6th European Software Engineering Conference and 5th ACM SIGSOFT Symposium on the Foundations of Software Engineering (Zurich, Switzerland, Sept. 1997), 344–360.SINHA, A. 1992. Client-server computing. Commun. ACM 35, 7 (July 1992), 77–98.SOLLINS, K. AND MASINTER, L. 1994. Functional requirements for Uniform Resource Names.Internet RFC 1737, Dec. 1994.SPERO, S. E. 1994. Analysis of HTTP performance problems. Published on the Web,&lt;http://metalab.unc.edu/mdma-release/http-prob.html&gt;.SULLIVAN, K. J. AND NOTKIN, D. 1992. Reconciling environment integration and software evolution.ACM Trans. Soft. Eng. Methodol. 1, 3 (July 1992), 229–268.TAYLOR, R. N., MEDVIDOVIC, N., ANDERSON, K. M., WHITEHEAD JR., E. J., ROBBINS, J. E., NIES, K. A.,OREIZY, P., AND DUBROW, D. L. 1996. A component- and message-based architectural style forGUI software. IEEE Trans. Soft. Eng. 22, 6 (June 1996), 390–406.WALDO, J., WYANT, G., WOLLRATH, A., AND KENDALL, S. 1994. A note on distributed computing. Tech.Rep. SMLI TR-94-29, Sun Microsystems Laboratories, Inc., Nov. 1994.WOLMAN, A., VOELKER, G., SHARMA, N., CARDWELL, N., BROWN, M., LANDRAY, T., PINNEL, D., KARLIN, A., ANDLEVY, H. 1999. Organization-based analysis of Web-object sharing and caching. In Proceedings of the 2nd USENIX Conference on Internet Technologies and Systems (Oct. 1999).Received December 2001; revised February 2002; accepted April 2002ACM Transactions on Internet Technology, Vol. 2, No. 2, May 2002.</Text>
            <Notes>“A nice overview of RESTful architecture by Roy Fielding himself: http://www.ics.uci.edu/~taylor/documents/2002-REST-TOIT.pdf 

“At ~36 pages, it's much shorter than Fielding's original dissertation on REST while providing plenty of detail on key principles.”
-@sloria

</Notes>
        </Document>
        <Document ID="11">
            <Title>Works Cited Format</Title>
            <Text>The Works Cited page is optional.

Chicago Style citations should be formatted as follows:

Book with one author
Pollan, Michael. The Omnivore’s Dilemma: A Natural History of Four Meals. New York: Penguin, 2006.

Book with two authors
Ward, Geoffrey C., and Ken Burns. The War: An Intimate History, 1941–1945. New York: Knopf, 2007.

Editor, translator, or compiler instead of author
Lattimore, Richmond, trans. The Iliad of Homer. Chicago: University of Chicago Press, 1951.

Book published electronically
Kurland, Philip B., and Ralph Lerner, eds. The Founders’ Constitution. Chicago: University of Chicago Press, 1987. Accessed February 28, 2010. http://press-pubs.uchicago.edu/founders/

Article in a print journal
Weinstein, Joshua I. “The Market in Plato’s Republic.” Classical Philology 104 (2009): 439–58.

Article in an online journal
Kossinets, Gueorgi, and Duncan J. Watts. “Origins of Homophily in an Evolving Social Network.” American Journal of Sociology 115 (2009): 405–50. Accessed February 28, 2010. doi:10.1086/599247.

Thesis or dissertation
Choi, Mihwa. “Contesting Imaginaires in Death Rituals during the Northern Song Dynasty.” PhD diss., University of Chicago, 2008.

Website
Google. “Google Privacy Policy.” Last modified March 11, 2009. http://www.google.com/intl/en/privacypolicy.html.
McDonald’s Corporation. “McDonald’s Happy Meal Toy Safety Facts.” Accessed July 19, 2008. http://www.mcdonalds.com/corp/about/factsheets.html.</Text>
        </Document>
        <Document ID="27">
            <Title>APIs to explore</Title>
            <Text>https://developer.github.com/v3/

http://dev.supportfu.com/api/v1

https://stripe.com/docs/api

When we make backwards-incompatible changes to the API, we release new dated versions. The current version is 2014-08-04. View a changelog and read more about API versions or visit your dashboard to upgrade your API version.


http://www.twilio.com/docs/api/rest/available-phone-numbers
http://www.twilio.com/docs/api

</Text>
        </Document>
        <Document ID="0">
            <Title>Essay</Title>
        </Document>
        <Document ID="28">
            <Title>api-design-ebook-2012-03</Title>
            <Text>￼￼Web API DesignCrafting Interfaces that Developers Love￼￼￼Brian Mulloy￼
￼Table of ContentsWeb API Design - Crafting Interfaces that Developers Love Introduction............................................................................................................................................ 3 Nouns are good; verbs are bad ......................................................................................................... 4 Plural nouns and concrete names ................................................................................................... 8 Simplify associations - sweep complexity under the ‘?’........................................................... 9 Handling errors ...................................................................................................................................10 Tips for versioning..............................................................................................................................13 Pagination and partial response....................................................................................................16 What about responses that don’t involve resources? ............................................................ 19 Supporting multiple formats ..........................................................................................................20 What about attribute names? .........................................................................................................21 Tips for search......................................................................................................................................22 Consolidate API requests in one subdomain.............................................................................23 Tips for handling exceptional behavior......................................................................................25 Authentication...................................................................................................................................... 27 Making requests on your API..........................................................................................................28 Chatty APIs.............................................................................................................................................30 Complement with an SDK.................................................................................................................31 The API Façade Pattern.....................................................................................................................322￼￼￼
￼Web API Design - Crafting Interfaces that Developers LoveIntroductionIf you’re reading this, chances are that you care about designing Web APIs that developers will love and that you’re interested in applying proven design principles and best practices to your Web API.One of the sources for our design thinking is REST. Because REST is an architectural style and not a strict standard, it allows for a lot of flexibly. Because of that flexibility and freedom of structure, there is also a big appetite for design best practices.This e-book is a collection of design practices that we have developed in collaboration with some of the leading API teams around the world, as they craft their API strategy through a design workshop that we provide at Apigee.We call our point of view in API design “pragmatic REST”, because it places the success of the developer over and above any other design principle. The developer is the customer for the Web API. The success of an API design is measured by how quickly developers can get up to speed and start enjoying success using your API.We’d love your feedback – whether you agree, disagree, or have some additional practices and tips to add. The API Craft Google Group is a place where Web API design enthusiasts come together to share and debate design practices – we’d love to see you there.￼Are you a Pragmatist or a RESTafarian?Let’s start with our overall point of view on API design. We advocate pragmatic, not dogmatic REST. What do we mean by dogmatic?You might have seen discussion threads on true REST - some of them can get pretty strict and wonky. Mike Schinkel sums it up well - defining a RESTafarian as follows:“A RESTifarian is a zealous proponent of the REST software architectural style as defined by Roy T. Fielding in Chapter 5 of his PhD. dissertation at UC Irvine. You can find RESTifarians in the wild on the REST-discuss mailing list. But be careful, RESTifarians can be extremely meticulous when discussing the finer points of REST ...”￼￼￼￼￼￼￼Our view: approach API design from the ‘outside-in’ perspective. This means we start by asking - what are we trying to achieve with an API?The API’s job is to make the developer as successful as possible. The orientation for APIs is to think about design choices from the application developer’s point of view.3￼￼
￼Web API Design - Crafting Interfaces that Developers LoveWhy? Look at the value chain below – the application developer is the lynchpin of the entire API strategy. The primary design principle when crafting your API should be to maximize developer productivity and success. This is what we call pragmatic REST.￼Pragmatic REST is a design problemYou have to get the design right, because design communicates how something will be used. The question becomes - what is the design with optimal benefit for the app developer?The developer point of view is the guiding principle for all the specific tips and best practices we’ve compiled.Nouns are good; verbs are badThe number one principle in pragmatic RESTful design is: keep simple things simple.Keep your base URL simple and intuitiveThe base URL is the most important design affordance of your API. A simple and intuitive base URL design makes using your API easy.Affordance is a design property that communicates how something should be used without requiring documentation. A door handle's design should communicate whether you pull or push. Here's an example of a conflict between design affordance and documentation - not an intuitive interface!4￼￼
￼Web API Design - Crafting Interfaces that Developers Love￼A key litmus test we use for Web API design is that there should be only 2 base URLs per resource. Let's model an API around a simple object or resource, a dog, and create a Web API for it.The first URL is for a collection; the second is for a specific element in the collection. /dogs /dogs/1234Boiling it down to this level will also force the verbs out of your base URLs.Keep verbs out of your base URLsMany Web APIs start by using a method-driven approach to URL design. These method- based URLs sometimes contain verbs - sometimes at the beginning, sometimes at the end.For any resource that you model, like our dog, you can never consider one object in isolation. Rather, there are always related and interacting resources to account for - like owners, veterinarians, leashes, food, squirrels, and so on.5￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThink about the method calls required to address all the objects in the dogs’ world. The URLs for our resource might end up looking something like this.￼It's a slippery slope - soon you have a long list of URLs and no consistent pattern making it difficult for developers to learn how to use your API.Use HTTP verbs to operate on the collections and elements.For our dog resources, we have two base URLs that use nouns as labels, and we can operate on them with HTTP verbs. Our HTTP verbs are POST, GET, PUT, and DELETE. (We think of them as mapping to the acronym, CRUD (Create-Read-Update-Delete).)With our two resources (/dogs and /dogs/1234) and the four HTTP verbs, we have a rich set of capability that's intuitive to the developer. Here is a chart that shows what we mean for our dogs.6￼￼
￼Web API Design - Crafting Interfaces that Developers Love￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼ResourcecreatePOSTreadGETList dogsupdatePUTdeleteDELETE￼Delete all dogs￼￼￼￼￼The point is that developers probably don't need the chart to understand how the API behaves. They can experiment with and learn the API without the documentation.In summary:Use two base URLs per resource.Keep verbs out of your base URLs.Use HTTP verbs to operate on the collections and elements.7Create a new dogErrorBulk update dogs￼￼￼￼￼￼￼￼￼￼￼/dogs￼￼￼Show BoIf exists update BoDelete Bo￼￼￼￼￼￼￼￼￼￼￼￼/dogs/1234￼If not error￼￼￼￼
￼Web API Design - Crafting Interfaces that Developers LovePlural nouns and concrete namesLet’s explore how to pick the nouns for your URLs.Should you choose singular or plural nouns for your resource names? You'll see popular APIs use both. Let's look at a few examples:/checkins /deals /ProductFoursquare GroupOn ZapposGiven that the first thing most people probably do with a RESTful API is a GET, we think it reads more easily and is more intuitive to use plural nouns. But above all, avoid a mixed model in which you use singular for some resources, plural for others. Being consistent allows developers to predict and guess the method calls as they learn to work with your API.Achieving pure abstraction is sometimes a goal of API architects. However, that abstraction is not always meaningful for developers.Concrete names are better than abstractTake for example an API that accesses content in various forms - blogs, videos, news articles, and so on.An API that models everything at the highest level of abstraction - as /items or /assets in our example - loses the opportunity to paint a tangible picture for developers to know what they can do with this API. It is more compelling and useful to see the resources listed as blogs, videos, and news articles.The level of abstraction depends on your scenario. You also want to expose a manageable number of resources. Aim for concrete naming and to keep the number of resources between 12 and 24.In summary, an intuitive API uses plural rather than singular nouns, and concrete rather than abstract names.8￼￼
￼Web API Design - Crafting Interfaces that Developers LoveSimplify associations - sweep complexity under the ‘?’In this section, we explore API design considerations when handling associations between resources and parameters like states and attributes.Resources almost always have relationships to other resources. What's a simple way to express these relationships in a Web API?AssociationsLet's look again at the API we modeled in nouns are good, verbs are bad - the API that interacts with our dogs resource. Remember, we had two base URLs: /dogs and dogs/1234.We're using HTTP verbs to operate on the resources and collections. Our dogs belong to owners. To get all the dogs belonging to a specific owner, or to create a new dog for that owner, do a GET or a POST:GET /owners/5678/dogsPOST /owners/5678/dogsNow, the relationships can be complex. Owners have relationships with veterinarians, who have relationships with dogs, who have relationships with food, and so on. It's not uncommon to see people string these together making a URL 5 or 6 levels deep. Remember that once you have the primary key for one level, you usually don't need to include the levels above because you've already got your specific object. In other words, you shouldn't need too many cases where a URL is deeper than what we have above /resource/identifier/resource.Most APIs have intricacies beyond the base level of a resource. Complexities can include many states that can be updated, changed, queried, as well as the attributes associated with a resource.Sweep complexity behind the ‘?’Make it simple for developers to use the base URL by putting optional states and attributes behind the HTTP question mark. To get all red dogs running in the park:GET /dogs?color=red&amp;state=running&amp;location=parkIn summary, keep your API intuitive by simplifying the associations between resources, and sweeping parameters and other complexities under the rug of the HTTP question mark.9￼￼
￼Web API Design - Crafting Interfaces that Developers LoveHandling errorsMany software developers, including myself, don't always like to think about exceptions and error handling but it is a very important piece of the puzzle for any software developer, and especially for API designers.Why is good error design especially important for API designers?From the perspective of the developer consuming your Web API, everything at the other side of that interface is a black box. Errors therefore become a key tool providing context and visibility into how to use an API.First, developers learn to write code through errors. The "test-first" concepts of the extreme programming model and the more recent "test driven development" models represent a body of best practices that have evolved because this is such an important and natural way for developers to work.Secondly, in addition to when they're developing their applications, developers depend on well-designed errors at the critical times when they are troubleshooting and resolving issues after the applications they've built using your API are in the hands of their users.How to think about errors in a pragmatic way with REST?Let's take a look at how three top APIs approach it.FacebookHTTP Status Code: 200{"type" : "OauthException", "message":"(#803) Some of thealiases you requested do not exist: foo.bar"}TwilioHTTP Status Code: 401{"status" : "401", "message":"Authenticate","code": 20003, "moreinfo": "http://www.twilio.com/docs/errors/20003"}SimpleGeoHTTP Status Code: 401{"code" : 401, "message": "Authentication Required"}10￼￼
￼Web API Design - Crafting Interfaces that Developers LoveNo matter what happens on a Facebook request, you get back the 200-status code - everything is OK. Many error messages also push down into the HTTP response. Here they also throw an #803 error but with no information about what #803 is or how to react to it.Twilio does a great job aligning errors with HTTP status codes. Like Facebook, they provide a more granular error message but with a link that takes you to the documentation. Community commenting and discussion on the documentation helps to build a body of information and adds context for developers experiencing these errors.FacebookTwilioSimpleGeo provides error codes but with no additional value in the payload.SimpleGeoA couple of best practices Use HTTP status codesUse HTTP status codes and try to map them cleanly to relevant standard-based codes.There are over 70 HTTP status codes. However, most developers don't have all 70 memorized. So if you choose status codes that are not very common you will force application developers away from building their apps and over to Wikipedia to figure out what you're trying to tell them.Therefore, most API providers use a small subset. For example, the Google GData API usesonly 10 status codes; Netflix uses 9, and Digg, only 8.200 201 304 400 401 403 404 409 410500Google GData200 201 304 400 401 403 404 412 500Netflix200 400 401 403 404 410 500 503DiggHow many status codes should you use for your API?When you boil it down, there are really only 3 outcomes in the interaction between an app and an API:• Everything worked - success• The application did something wrong – client error 11 • The API did something wrong – server error￼￼
￼Web API Design - Crafting Interfaces that Developers LoveStart by using the following 3 codes. If you need more, add them. But you shouldn't need to go beyond 8.• 200-OK• 400 - Bad Request• 500 - Internal Server ErrorIf you're not comfortable reducing all your error conditions to these 3, try picking among these additional 5:• 201 - Created• 304 - Not Modified• 404 – Not Found• 401 - Unauthorized• 403 - Forbidden(Check out this good Wikipedia entry for all HTTP Status codes.)It is important that the code that is returned can be consumed and acted upon by the￼application's business logic - for example, in an if-then-else, or a case statement.Make messages returned in the payload as verbose as possible.Code for code200 – OK401 – UnauthorizedMessage for people{"developerMessage" : "Verbose, plain language description ofthe problem for the app developer with hints about how to fixit.", "userMessage":"Pass this message on to the app user ifneeded.", "errorCode" : 12345, "more info":"http://dev.teachdogrest.com/errors/12345"}In summary, be verbose and use plain language descriptions. Add as many hints as your API team can think of about what's causing an error.We highly recommend you add a link in your description to more information, like Twilio does.12￼￼
￼Web API Design - Crafting Interfaces that Developers LoveTips for versioningVersioning is one of the most important considerations when designing your Web API.Never release an API without a version and make the version mandatory.Let's see how three top API providers handle versioning.Twilio /2010-04-01/Accounts/salesforce.com /services/data/v20.0/sobjects/AccountFacebook ?v=1.0Twilio uses a timestamp in the URL (note the European format).At compilation time, the developer includes the timestamp of the application when the code was compiled. That timestamp goes in all the HTTP requests.When a request arrives, Twilio does a look up. Based on the timestamp they identify the API that was valid when this code was created and route accordingly.It's a very clever and interesting approach, although we think it is a bit complex. For example, it can be confusing to understand whether the timestamp is the compilation time or the timestamp when the API was released.Salesforce.com uses v20.0, placed somewhere in the middle of the URL.We like the use of the v. notation. However, we don't like using the .0 because it implies that the interface might be changing more frequently than it should. The logic behind an interface can change rapidly but the interface itself shouldn't change frequently.Facebook also uses the v. notation but makes the version an optional parameter.This is problematic because as soon as Facebook forced the API up to the next version, all the apps that didn't include the version number broke and had to be pulled back and version number added.13￼￼
￼Web API Design - Crafting Interfaces that Developers LoveHow to think about version numbers in a pragmatic way with REST?Never release an API without a version. Make the version mandatory.Specify the version with a 'v' prefix. Move it all the way to the left in the URL so that it hasthe highest scope (e.g. /v1/dogs).Use a simple ordinal number. Don't use the dot notation like v1.2 because it implies a granularity of versioning that doesn't work well with APIs--it's an interface not an implementation. Stick with v1, v2, and so on.How many versions should you maintain? Maintain at least one version back.For how long should you maintain a version? Give developers at least one cycle to reactbefore obsoleting a version.Sometimes that's 6 months; sometimes it’s 2 years. It depends on your developers' development platform, application type, and application users. For example, mobile apps take longer to rev’ than Web apps.Should version and format be in URLs or headers?There is a strong school of thought about putting format and version in the header.Sometimes people are forced to put the version in the header because they have multiple inter-dependent APIs. That is often a symptom of a bigger problem, namely, they are usually exposing their internal mess instead of creating one, usable API facade on top.That’s not to say that putting the version in the header is a symptom of a problematic API design. It's not!In fact, using headers is more correct for many reasons: it leverages existing HTTP standards, it's intellectually consistent with Fielding's vision, it solves some hard real- world problems related to inter-dependent APIs, and more.￼However, we think the reason most of the popular APIs do not use it is because it's less fun to hack in a browser.Simple rules we follow:If it changes the logic you write to handle the response, put it in the URL so you can see it easily.If it doesn't change the logic for each response, like OAuth information, put it in the header. 14￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThese for example, all represent the same resource:dogs/1Content-Type: application/jsondogs/1Content-Type: application/xmldogs/1Content-Type: application/pngThe code we would write to handle the responses would be very different.There's no question the header is more correct and it is still a very strong API design.15￼￼
￼Web API Design - Crafting Interfaces that Developers LovePagination and partial responsePartial response allows you to give developers just the information they need.Take for example a request for a tweet on the Twitter API. You'll get much more than a typical twitter app often needs - including the name of person, the text of the tweet, a timestamp, how often the message was re-tweeted, and a lot of metadata.Let's look at how several leading APIs handle giving developers just what they need in responses, including Google who pioneered the idea of partial response.LinkedInFacebook/joe.smith/friends?fields=id,name,pictureGoogle/people:(id,first-name,last-name,industry)This request on a person returns the ID, first name, last name, and the industry.LinkedIn does partial selection using this terse :(...) syntax which isn't self-evident. Plus it's difficult for a developer to reverse engineer the meaning using a search engine.?fields=title,media:group(media:thumbnail)Google and Facebook have a similar approach, which works well.They each have an optional parameter called fields after which you put the names of fields you want to be returned.As you see in this example, you can also put sub-objects in responses to pull in other information from additional resources.Add optional fields in a comma-delimited listThe Google approach works extremely well.Here's how to get just the information we need from our dogs API using this approach:/dogs?fields=name,color,locationIt's simple to read; a developer can select just the information an app needs at a given time; it cuts down on bandwidth issues, which is important for mobile apps.16￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThe partial selection syntax can also be used to include associated resources cutting down on the number of requests needed to get the required information.Make it easy for developers to paginate objects in a databaseIt's almost always a bad idea to return every resource in a database.Let's look at how Facebook, Twitter, and LinkedIn handle pagination. Facebook uses offsetand limit. Twitter uses page and rpp (records per page). LinkedIn uses start and count Semantically, Facebook and LinkedIn do the same thing. That is, the LinkedIn start &amp; countis used in the same way as the Facebook offset &amp; limit.To get records 50 through 75 from each system, you would use:• Facebook - offset 50 and limit 25• Twitter - page 3 and rpp 25 (records per page)• LinkedIn - start 50 and count 25Use limit and offsetWe recommend limit and offset. It is more common, well understood in leading databases, and easy for developers./dogs?limit=25&amp;offset=50MetadataWe also suggest including metadata with each response that is paginated that indicated to the developer the total number of records available.What about defaults?My loose rule of thumb for default pagination is limit=10 with offset=0.(limit=10&amp;offset=0)The pagination defaults are of course dependent on your data size. If your resources are large, you probably want to limit it to fewer than 10; if resources are small, it can make sense to choose a larger limit.17￼￼
￼Web API Design - Crafting Interfaces that Developers LoveIn summary:Support partial response by adding optional fields in a comma delimited list.Use limit and offset to make it easy for developers to paginate objects.18￼￼
￼Web API Design - Crafting Interfaces that Developers LoveWhat about responses that don’t involve resources?API calls that send a response that's not a resource per se are not uncommon depending on the domain. We've seen it in financial services, Telco, and the automotive domain to someextent.Actions like the following are your clue that you might not be dealing with a "resource" response.Calculate Translate ConvertFor example, you want to make a simple algorithmic calculation like how much tax someone should pay, or do a natural language translation (one language in request; another in response), or convert one currency to another. None involve resources returned from a database.In these cases:Use verbs not nounsFor example, an API to convert 100 euros to Chinese Yen:/convert?from=EUR&amp;to=CNY&amp;amount=100Make it clear in your API documentation that these “non-resource” scenarios are different.Simply separate out a section of documentation that makes it clear that you use verbs in cases like this – where some action is taken to generate or calculate the response, rather than returning a resource directly.19￼￼
￼Web API Design - Crafting Interfaces that Developers LoveSupporting multiple formatsWe recommend that you support more than one format - that you push things out in one format and accept as many formats as necessary. You can usually automate the mapping from format to format.Here's what the syntax looks like for a few key APIs.Google Data?alt=jsonFoursquare/venue.jsonDigg*Accept: application/json?type=json* The type argument, if present, overrides the Accept header.Digg allows you to specify in two ways: in a pure RESTful way in the Accept header or in the type parameter in the URL. This can be confusing - at the very least you need to document what to do if there are conflicts.We recommend the Foursquare approach.To get the JSON format from a collection or specific element:dogs.json/dogs/1234.jsonDevelopers and even casual users of any file system are familiar to this dot notation. It also requires just one additional character (the period) to get the point across.What about default formats?In my opinion, JSON is winning out as the default format. JSON is the closest thing we have to universal language. Even if the back end is built in Ruby on Rails, PHP, Java, Python etc., most projects probably touch JavaScript for the front-end. It also has the advantage of being terse - less verbose than XML.20￼￼
￼Web API Design - Crafting Interfaces that Developers LoveWhat about attribute names?In the previous section, we talked about formats - supporting multiple formats and working with JSON as the default.This time, let's talk about what happens when a response comes back.You have an object with data attributes on it. How should you name the attributes?Here are API responses from a few leading APIs:Twitter"created_at": "Thu Nov 03 05:19;38 +0000 2011"Bing"DateTime": "2011-10-29T09:35:00Z"Foursquare"createdAt": 1320296464They each use a different code convention. Although the Twitter approach is familiar to me as a Ruby on Rails developer, we think that Foursquare has the best approach.How does the API response get back in the code? You parse the response (JSON parser); what comes back populates the Object. It looks like thisvar myObject = JSON.parse(response);If you chose the Twitter or Bing approach, your code looks like this. Its not JavaScript convention and looks weird - looks like the name of another object or class in the system, which is not correct.timing = myObject.created_at; timing - myObject.DateTime; Recommendations21- Use medial capitalization (aka CamelCase)- Use uppercase or lowercase depending on type of object• Use JSON as default• Follow JavaScript conventions for naming attributes￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThis results in code that looks like the following, allowing the JavaScript developer to write it in a way that makes sense for JavaScript."createdAt": 1320296464timing = myObject.createdAt;Tips for searchWhile a simple search could be modeled as a resourceful API (for example, dogs/?q=red), a more complex search across multiple resources requires a different design.This will sound familiar if you've read the topic about using verbs not nouns when results don't return a resource from the database - rather the result is some action or calculation.If you want to do a global search across resources, we suggest you follow the Google model:Global search/search?q=fluffy+furHere, search is the verb; ?q represents the query.Scoped searchTo add scope to your search, you can prepend with the scope of the search. For example, search in dogs owned by resource ID 5678/owners/5678/dogs?q=fluffy+furNotice that we’ve dropped the explicit search in the URL and rely on the parameter ‘q’ to indicate the scoped query. (Big thanks to the contributors on the API Craft Google group for helping refine this approach.)￼Formatted resultsFor search or for any of the action oriented (non-resource) responses, you can prepend with the format as follows:/search.xml?q=fluffy+fur22￼￼
￼Web API Design - Crafting Interfaces that Developers LoveConsolidate API requests in one subdomainWe’ve talked about things that come after the top-level domain. This time, let's explore stuff on the other side of the URL.Here's how Facebook, Foursquare, and Twitter handle this:Facebook provides two APIs. They started with api.facebook.com, then modified it to orientaround the social graph graph.facebook.com. graph.facebook.com￼￼api.facebook.comFoursquare has one API.api.foursquare.comTwitter has three APIs; two of them focused on search and streaming.stream.twitter.comapi.twitter.comsearch.twitter.comIt's easy to understand how Facebook and Twitter ended up with more than one API. It has a lot to do with timing and acquisition, and it's easy to reconfigure a CName entry in your DNS to point requests to different clusters.But if we're making design decisions about what's in the best interest of app developer, we recommend following Foursquare's lead:Consolidate all API requests under one API subdomain.It's cleaner, easier and more intuitive for developers who you want to build cool apps using your API.Facebook, Foursquare, and Twitter also all have dedicated developer portals.developers.facebook.comdevelopers.foursquare.comdev.twitter.comHow to organize all of this?Your API gateway should be the top-level domain. For example, api.teachdogrest.com23￼￼
￼Web API Design - Crafting Interfaces that Developers LoveIn keeping with the spirit of REST, your developer portal should follow this pattern: developers.yourtopleveldomain. For example,developers.teachdogrest.comDo Web redirectsThen optionally, if you can sense from requests coming in from the browser where the developer really needs to go, you can redirect.Say a developer types api.teachdogrest.com in the browser but there's no other information for the GET request, you can probably safely redirect to your developer portal and help get the developer where they really need to be.api developers (if from browser) dev  developersdeveloper  developers24￼￼
￼Web API Design - Crafting Interfaces that Developers LoveTips for handling exceptional behaviorSo far, we've dealt with baseline, standard behaviors.Here we’ll explore some of the exceptions that can happen - when clients of Web APIs can't handle all the things we've discussed. For example, sometimes clients intercept HTTP error codes, or support limited HTTP methods.What are ways to handle these situations and work within the limitations of a specific client?When a client intercepts HTTP error codesOne common thing in some versions of Adobe Flash - if you send an HTTP response that is anything other than HTTP 200 OK, the Flash container intercepts that response and puts the error code in front of the end user of the app.Therefore, the app developer doesn't have an opportunity to intercept the error code. App developers need the API to support this in some way.Twitter does an excellent job of handling this.They have an optional parameter suppress_response_codes. Ifsuppress_response_codes is set to true, the HTTP response is always 200./public_timelines.json?suppress_response_codes=trueHTTP status code: 200 {"error":"Could not authenticate you."}Notice that this parameter is a big verbose response code. (They could have used something like src, but they opted to be verbose.)This is important because when you look at the URL, you need to see that the response codes are being suppressed as it has huge implications about how apps are going to respond to it.Overall recommendations:1 - Use suppress_response_codes = true2 - The HTTP code is no longer just for the codeThe rules from our previous Handling Errors section change. In this context, the HTTP code is no longer just for the code - the program - it's now to be ignored. Client apps are never going to be checking the HTTP status code, as it is always the same.25￼￼
￼Web API Design - Crafting Interfaces that Developers Love3 - Push any response code that we would have put in the HTTP response down into the response messageIn my example below, the response code is 401. You can see it in the response message. Also include additional error codes and verbose information in that message.Always return OK/dogs?suppress_response_codes = trueCode for ignoring200 - OKMessage for people &amp; code{response_code" : 401, "message" : "Verbose, plain languagedescription of the problem with hints about how to fix it.""more_info" : "http://dev.tecachdogrest.com/errors/12345","code" : 12345}When a client supports limited HTTP methodsIt is common to see support for GET and POST and not PUT and DELETE.To maintain the integrity of the four HTTP methods, we suggest you use the following methodology commonly used by Ruby on Rails developers:Make the method an optional parameter in the URL.Then the HTTP verb is always a GET but the developer can express rich HTTP verbs and still maintain a RESTful clean API.Create/dogs?method=postRead/dogsUpdate/dogs/1234?method=put&amp;location=parkDelete/dogs/1234?method=deleteWARNING: It can be dangerous to provide post or delete capabilities using a GET method because if the URL is in a Web page then a Web crawler like the Googlebot can create or destroy lots of content inadvertently. Be sure you understand the implications of supporting this approach for your applications' context.26￼￼
￼Web API Design - Crafting Interfaces that Developers LoveAuthenticationThere are many schools of thought. My colleagues at Apigee and I don't always agree on how to handle authentication - but overall here's my take.Let's look at these three top services. See how each of these services handles things differently:Permissions Service APIPayPalOAuth 2.0FacebookOAuth 1.0aTwitterNote that PayPal's proprietary three-legged permissions API was in place long before OAuth was conceived.What should you do?Use the latest and greatest OAuth - OAuth 2.0 (as of this writing). It means that Web or mobile apps that expose APIs don’t have to share passwords. It allows the API provider to revoke tokens for an individual user, for an entire app, without requiring the user to change their original password. This is critical if a mobile device is compromised or if a rogue app is discovered.Above all, OAuth 2.0 will mean improved security and better end-user and consumer experiences with Web and mobile apps.Don't do something *like* OAuth, but different. It will be frustrating for app developers if they can't use an OAuth library in their language because of your variation.27￼￼
￼Web API Design - Crafting Interfaces that Developers LoveMaking requests on your APILets take a look at what some API requests and responses look like for our dogs API.Create a brown dog named AlPOST /dogsname=Al&amp;furColor=brownResponse200 OK{"dog":{"id": "1234", "name": "Al", "furColor": "brown"}Rename Al to Rover - UpdatePUT /dogs/1234name=RoverResponse200 OK{"dog":{ "id":"1234", "name": "Rover", "furColor": "brown"}28￼￼
￼Web API Design - Crafting Interfaces that Developers LoveTell me about a particular dogGET /dogs/1234Response200 OK{"dog":{ "id":"1234", "name": "Rover", "furColor": "brown"}Tell me about all the dogsGET /dogsResponse200 OK{"dogs":[{"dog":{ "id":"1233","name": "Fido", "furColor": "white"}}, {"dog":{"id":"1234","name": "Rover","furColor": "brown"}}]"_metadata":[{"totalCount":327,"limit":25,"offset":100}]}Delete Rover :-(DELETE /dogs/1234Response200 OK29￼￼
￼Web API Design - Crafting Interfaces that Developers LoveChatty APIsLet’s think about how app developers use that API you're designing and dealing with chatty APIs.When designing your API and resources, try to imagine how developers will use it to say construct a user interface, an iPhone app, or many other apps.Imagine how developers will use your APISome API designs become very chatty - meaning just to build a simple UI or app, you have dozens or hundreds of API calls back to the server.The API team can sometimes decide not to deal with creating a nice, resource-oriented RESTful API, and just fall back to a mode where they create the 3 or 4 Java-style getter and setter methods they know they need to power a particular user interface.We don't recommend this. You can design a RESTful API and still mitigate the chattiness.Be complete and RESTful and provide shortcutsFirst design your API and its resources according to pragmatic RESTful design principles and then provide shortcuts.￼What kind of shortcut? Say you know that 80% of all your apps are going to need some sort of composite response, then build the kind of request that gives them what they need.Just don't do the latter instead of the former. First design using good pragmatic RESTful principles!Take advantage of the partial response syntaxThe partial response syntax discussed in a previous section can help.To avoid creating one-off base URLs, you can use the partial response syntax to drill down to dependent and associated resources.In the case of our dogs API, the dogs have association with owners, who in turn have associations with veterinarians, and so on. Keep nesting the partial response syntax using dot notation to get back just the information you need./owners/5678?fields=name,dogs.name30￼￼
￼Web API Design - Crafting Interfaces that Developers LoveComplement with an SDKIt’s a common question for API providers - do you need to complement your API with code libraries and software development kits (SDKs)?If your API follows good design practices, is self consistent, standards-based, and well documented, developers may be able to get rolling without a client SDK. Well-documented code samples are also a critical resource.￼On the other hand, what about the scenario in which building a UI requires a lot of domain knowledge? This can be a challenging problem for developers even when building UI and apps on top of APIs with pretty simple domains – think about the Twitter API with it’s primary object of 140 characters of text.You shouldn't change your API to try to overcome the domain knowledge hurdle. Instead, you can complement your API with code libraries and a software development kit (SDK).In this way, you don't overburden your API design. Often, a lot of what's needed is on the client side and you can push that burden to an SDK.The SDK can provide the platform-specific code, which developers use in their apps to invoke API operations - meaning you keep your API clean.Other reasons you might consider complementing your API with an SDK include the following:Speed adoption on a specific platform. (For example Objective C SDK for iPhone.) Many experienced developers are just starting off with objective C+ so an SDK might be helpful.Simplify integration effort required to work with your API - If key use cases are complex or need to be complemented by standard on-client processing.An SDK can help reduce bad or inefficient code that might slow down service for everyone.As a developer resource - good SDKs are a forcing function to create good source code examples and documentation. Yahoo! and Paypal are good examples:Yahoo! http://developer.yahoo.com/social/sdk/Paypal https://cms.paypal.com/us/cgi-bin/?cmd=_render-31￼content&amp;content_ID=developer/library_download_sdks￼To market your API to a specific community - you upload the SDK to a samples or plug- in page on a platform’s existing developer community.￼￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThe API Façade PatternAt this point, you may be asking -What should we be thinking from an architectural perspective?How do we follow all these best practice guidelines, expose my internal services and systems in a way that’s useful to app developers, and still iterate and maintain my API?Back-end systems of record are often too complex to expose directly to application developers. They are stable (have been hardened over time) and dependable (they are running key aspects of you business), but they are often based on legacy technologies and not always easy to expose to Web standards like HTTP. These systems can also have complex interdependencies and they change slowly meaning that they can’t move as quickly as the needs of mobile app developers and keep up with changing formats.In fact, the problem is not creating an API for just one big system but creating an API for an array of complementary systems that all need to be used to make an API valuable to adeveloper.DB Content SOAP JDBC RSSBig systemMgmt￼￼￼￼￼￼￼It’s useful to talk through a few anti-patterns that we’ve seen. Let’s look at why we believe they don’t work well.The Build Up ApproachIn the build-up approach, a developer exposes the core objects of a big system and puts an XML parsing layer on top.XML￼￼￼￼Expose Objects Big System￼￼￼￼￼￼￼￼￼32￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThis approach has merit in that it can get you to market with version 1 quickly. Also, your API team members (your internal developers) already understand the details of the system.Unfortunately, those details of an internal system at the object level are fine grained and can be confusing to external developers. You’re also exposing details of internal architecture, which is rarely a good idea. This approach can be inflexible because you have 1:1 mapping to how a system works and how it is exposed to API. In short, building up from the systems of record to the API can be overly complicated.The Standards Committee ApproachOften the internal systems are owned and managed by different people and departments with different views about how things should work. Designing an API by a standards committee often involves creating a standards document, which defines the schema and URLs and such. All the stakeholders build toward that common goal.Standards Doc￼￼￼￼￼￼￼￼XML XML Expose ExposeXML Expose RSS￼￼￼￼￼￼￼DBContent MgmtBig System￼￼￼￼￼The benefits of this approach include getting to version 1 quickly. You can also create a sense of unification across an organization and a comprehensive strategy, which can be significant accomplishments when you have a large organization with a number of stakeholders and contributors.A drawback of the standards committee pattern is that it can be slow. Even if you get the document created quickly, getting everybody to implement against it can be slow and can lack adherence. This approach can also lead to a mediocre design as a result of too many compromises.33￼￼
￼Web API Design - Crafting Interfaces that Developers LoveThe Copy Cat ApproachWe sometimes see this pattern when an organization is late to market – for example, when their close competitor has already delivered a solution. Again, this approach can get you to version 1 quickly and you may have a built-in adoption curve if the app developers who will use your API are already familiar with your competitor’s API.Competitor’s API Doc￼￼￼￼￼￼￼￼￼XMLXML XML Expose Expose RSS￼￼￼￼￼￼Expose Big SystemDB Content Mgmt￼￼￼￼￼￼However, you can end up with an undifferentiated product that is considered an inferior offering in the market of APIs. You might have missed exposing your own key value and differentiation by just copying someone else’s API design.Solution – The API façade patternThe best solution starts with thinking about the fundamentals of product management. Your product (your API) needs to be credible, relevant, and differentiated. Your product manager is a key member of your API teamOnce your product manager has decided what the big picture is like, it’s up to the architects.We recommend you implement an API façade pattern. This pattern gives you a buffer or virtual layer between the interface on top and the API implementation on the bottom. You essentially create a façade – a comprehensive view of what the API should be and importantly from the perspective of the app developer and end user of the apps they create.34￼￼
￼Web API Design - Crafting Interfaces that Developers Love￼￼Big SystemAPI Facade￼￼￼￼DB Content SOAP JDB RSS Mgmt CThe developer and the app that consume the API are on top. The API façade isolates the developer and the application and the API. Making a clean design in the facade allows you to decompose one really hard problem into a few simpler problems.￼￼￼￼￼￼￼“Use the façade pattern when you want to provide a simple interface to a complex subsystem. Subsystems often get more complex as they evolve.”Design Patterns – Elements of Reusable Object-Oriented Software(Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides)Implementing an API façade pattern involves three basic steps.1 - Design the ideal API – design the URLs, request parameters and responses, payloads, headers, query parameters, and so on. The API design should be self-consistent.2 - Implement the design with data stubs. This allows application developers to use your API and give you feedback even before your API is connected to internal systems.3 - Mediate or integrate between the façade and the systems. 35￼￼
￼Web API Design - Crafting Interfaces that Developers Love￼Ideal Design￼￼API Facade Mediate￼￼￼￼￼Big System￼DB￼Content Mgmt￼SOAP￼JDBC￼RSS￼￼￼￼￼Using the three-step approach you’ve decomposed one big problem to three smaller problems. If you try to solve the one big problem, you’ll be starting in code, and trying to build up from your business logic (systems of record) to a clean API interface. You would be exposing objects or tables or RSS feeds from each silo, mapping each to XML in the right format before exposing to the app. It is a machine–to-machine orientation focused around an app and is difficult to get this right.Taking the façade pattern approach helps shift the thinking from a silo approach in a number of important ways. First, you can get buy in around each of the three separate steps and have people more clearly understand how you’re taking a pragmatic approach to the design. Secondly, the orientation shifts from the app to the app developer. The goal becomes to ensure that the app developer can use your API because the design is self- consistent and intuitive.Because of where it is in the architecture, the façade becomes an interesting gateway. You can now have the façade implement the handling of common patterns (for pagination, queries, ordering, sorting, etc.), authentication, authorization, versioning, and so on, uniformly across the API. (This is a big topic and a full discussion is beyond the scope of this article.)Other benefits for the API team include being more easily able to adapt to different use cases regardless of whether they are internal developer, partner, or open scenarios. The API team will be able to keep pace with the changing needs of developers, including the ever-changing protocols and languages. It is also easier to extend an API by building out more capability from your enterprise or plugging in additional existing systems.36￼￼
￼Web API Design - Crafting Interfaces that Developers LoveResourcesRepresentational State Transfer (REST), Roy Thomas Fielding, 2000 RESTful API Design Webinar, 2nd edition, Brian Mulloy, 2011Apigee API Tech &amp; Best Practices BlogAPI Craft Google Group￼￼￼￼￼About Brian Mulloy Brian Mulloy, ApigeeBrian has 15 years of experience ranging from enterprise software to founding a Web startup. He co-founded and was CEO of Swivel, a Website for social data analysis. He was President and General Manager of Grand Central, a cloud-based offering for application infrastructure (before we called it the cloud). And was Director of Product Marketing at BEA Systems. Brian holds a degree in Physics from the University of Michigan.Brian is a frequent contributor on the Apigee API Tech &amp; best practices blog, the Apigee YouTube channel, the API Craft Google Group, and Webinars.￼￼￼￼￼￼￼
￼￼Apigee is the leading provider of API products and technology for enterprises and developers. Hundreds of enterprises like Comcast, GameSpy, TransUnion Interactive, Guardian Life and Constant Contact and thousands of developers use Apigee's technology. Enterprises use Apigee for visibility, control and scale of their API strategies. Developers use Apigee to learn, explore and develop API-based applications. Learn more at http://apigee.com.About Apigee￼Accelerate your API Strategy Scale Control and Secure your Enterprise￼Developers – Consoles for the APIs you￼￼￼￼￼</Text>
            <Notes>http://pages.apigee.com/rs/apigee/images/api-design-ebook-2012-03.pdf

Our view: approach API design from the ‘outside-in’ perspective. This means we start by asking - what are we trying to achieve with an API?

But above all, avoid a mixed model in which you use singular for some resources, plural for others.

An API that models everything at the highest level of abstraction - as /items or /assets in our example - loses the opportunity to paint a tangible picture for developers to know what they can do with this API.
Aim for concrete naming and to keep the number of resources between 12 and 24.

To get all the dogs belonging to a specific owner, or to create a new dog for that owner, do a GET or a POST:GET /owners/5678/dogsPOST /owners/5678/dogs

Sweep complexity behind the ‘?’
To get all red dogs running in the park:GET /dogs?color=red&amp;state=running&amp;location=park

TwilioHTTP Status Code: 401{"status" : "401", "message":"Authenticate","code": 20003, "moreinfo": "http://www.twilio.com/docs/errors/20003"}

When you boil it down, there are really only 3 outcomes in the interaction between an app and an API:• Everything worked - success• The application did something wrong – client error 11 • The API did something wrong – server error

In summary, be verbose and use plain language descriptions. Add as many hints as your API team can think of about what's causing an error.We highly recommend you add a link in your description to more information, like Twilio does.
Never release an API without a version. Make the version mandatory.

Add optional fields in a comma-delimited list/dogs?fields=name,color,location

pagination
We recommend limit and offset. It is more common, well understood in leading databases, and easy for developers./dogs?limit=25&amp;offset=50</Notes>
        </Document>
        <Document ID="20">
            <Title>Thoughts on RESTful API Design</Title>
            <Text>Thoughts on RESTful API Design¶
Lessons learnt from designing the Red Hat Enterprise Virtualization API¶
Author: Geert Jansen &lt;gjansen@redhat.com&gt;
Date: November 15th, 2012
This work is licensed under a Creative Commons Attribution 3.0 Unported License.
The source of this document can be found on github.
	▪	Introduction
	▪	The Job of the API Designer
	▪	The Application
	▪	The API Code
	▪	The Client
	▪	Resources
	▪	Resource Data
	▪	Representations
	▪	Content-Types
	▪	URLs
	▪	Entry Point
	▪	URL Structure
	▪	Relative vs Absolute
	▪	URL Templates
	▪	Variants
	▪	Methods
	▪	Standard Methods
	▪	Actions
	▪	PATCH vs PUT
	▪	Link Headers
	▪	Asynchronous Requests
	▪	Ranges / Pagination
	▪	Notifications
	▪	Relationships
	▪	Standard Structural Relationships
	▪	Modeling Semantic Relationships
	▪	Forms
	▪	Type Definition
	▪	Service Definition
	▪	Using Forms to Guide Input
	▪	Linking to Forms
	▪	Miscellaneous Topics
	▪	Offline Help


Introduction¶
This essay is an attempt to put down my thoughts on how to design a real-world yet beautiful RESTful API. It draws from the experience I have gained being involved in the design of the RESTful API for Red Hat’s Enterprise Virtualization product, twice. During the design phase of the API we had to solve many of the real-world problems described above, but we weren’t willing to add non-RESTful or “RPC-like” interfaces to our API too easily.
In my definition, a real-world RESTful API is an API that provides answers to questions that you won’t find in introductory texts, but that inevitably surface in the real world, such as whether or not resources should be described formally, how to create useful and automatic command-line interfaces, how to do polling, asynchronous and other non-standard types of requests, and how to deal with operations that have no good RESTful mapping.
A beautiful RESTful API on the other hand is one that does not deviate from the principles of RESTful architecture style too easily. One important design element for example that is not always addressed is the possibility for complete auto-discovery by the API user, so that the API can be used by a human with a web browser, without any reference to external documentation. I will discuss this issue in detail in Forms.
This essay does not attempt to explain the basics about REST, as there are many other texts about that. For a good description of REST, I would recommend reading at least the Atom Publishing Protocol and this blog post by Roy Fielding.
While the views exposed in this essay are my own, they are heavily based on the public discussion on the rhevm-api mailing list. My thanks goes to all people who have contributed and are still contributing to it, including Mark McLoughlin, Michael Pasternak, Ori Liel, Sander Hoentjen, Ewoud Kohl van Wijngaarden, Tomas V.V. Cox and everybody else I forgot.

The Job of the API Designer¶
Before we dive fully into RESTful API design, it makes sense to look in a little more detail what the job of a RESTful API designer is.
APIs don’t exist in isolation. Instead, APIs expose functionality of an application or service that exists independently of the API. In my view, the job of the RESTful API designer is two-fold.
	1.	Understanding enough of the important details of the application for which an API is to be created, so that an informed decision can be made as to what functionality needs to be exposed, how it needs to be exposed, and what functionality can instead be left out.
	2.	Modeling this functionality in an API that addresses all use cases that come up in the real world, following the RESTful principles as closely as possible.
There are three distinct components involved in RESTful API design: the application, the API code, and the client. The image above illustrates how these three components interact.
#
The Application¶
The application for which an API is to be provided exists independently of that API. Maybe the application has a GUI, and you have a requirement to add a programmatic interface to it. Or maybe the application was designed under the assumption that it would be accessed only via the API you are designing.
Like any application, the application for which the API is to be created contains state. That state is dynamic, and will change due to various operations that are executed on it. This state, and the operations on it, need to be modeled and exposed, and will form the API you are designing.
The easiest way to think about the application state, is to assume that it is is described by an application data model, which can be described by an Entity-Relationship diagram. The ER-Diagram lists the details of the entities (I will call them objects) in the application state, and the relationships between them.
In some cases, it is very easy to create the ER diagram. For example, in case of a web application that stores all its state in a database, it can be trivially created from the schema of the database. In other cases where there is not such a precise definition, the job of the API designer becomes a little more difficult. In such a case it would actually make sense to create an ER diagram for the application in question. That is a useful exercise in its own right, as it will make you better understand the application you’re designing the API for. But more importantly, it will also help you in designing a good RESTful API. We will talk more about that in a minute. Going forward, I will assume that an ER diagram is available.
In addition to understanding the application data model, and the operations on it, you of course need an entry point into the application that allows you to access and change the application state. This “way in” is fully application dependent and could take many forms. We will call this “way in” the application interface. Formally, this application interface could be considered an API as well. The difference, though, is that this interface is usually not intended for external consumption or even fully documented. In order not to introduce any confusion, we will not refer to this interface as an API: that term will be reserved for the RESTful API that is being designed.
The API Code¶
The job of the API code is to access the application state, as well as the operations on that state, via the application interface, and expose it as a RESTful API. In between the application interface and the RESTful API, there is a transformation step that adapts the application data model and makes it comply with the RESTful architecture style.
The result of this transformation would be RESTful resources, operations on those resources, and relationships between the resources. All of these are described by what we call the RESTful resource model.
Resources are the foundation behind any RESTful API, and we will go into a lot of detail on them in Resources. For now, just think of resources as being very similar to the entities from the ER diagram (which is why I encouraged you to create an ER diagram for your application in case it didn’t exist).
Relationships between resources are expressed as hyperlinks in the representation of the resource. This is one of the fundamental principles of RESTful API design. Resources also respond to a very limited set of operations (usually just 4), which is a second principle of the RESTful architectural style.
When transforming objects from the application data model to RESTful resources, you may find it useful to define two utility functions:
to_resource()¶
This function is assumed to take an object from the application data model, and convert it into a resource.
from_resource()¶
This function is assumed to take a resource, and translate it into an object in the application data model.
We don’t discuss these methods further, other than mentioning that these can be rather simple functions if the application data model is similar to the resource model you’re exposing, and can be quite complicated if they are very different.
The Client¶
The client consumes the RESTful API via the standard HTTP protocol. In theory, the service could be provided on top of other protocols as well. Since HTTP is so ubiquitous, however, it is not certain how valuable such a mapping to another protocol would be in the real world. Therefore, this essay limits itself to describing our RESTful protocol in terms of HTTP.
Clients would typically use an HTTP library to access the RESTful API. HTTP has become a moderately complex protocol, and very good libraries exist for many target platforms / languages. It therefore makes a lot of sense to use one of those libraries.
In some cases, it may make sense to use a generic REST library on top of an HTTP library. However, since there are so many different conventions in RESTful APIs, this library may actually be specific to the API that you are consuming.

Resources¶
The fundamental concept in any RESTful API is the resource. A resource is an object with a type, associated data, relationships to other resources, and a set of methods that operate on it. It is similar to an object instance in an object-oriented programming language, with the important difference that only a few standard methods are defined for the resource (corresponding to the standard HTTP GET, POST, PUT and DELETE methods), while an object instance typically has many methods.
Resources can be grouped into collections. Each collection is homogeneous so that it contains only one type of resource, and unordered. Resources can also exist outside any collection. In this case, we refer to these resources as singleton resources. Collections are themselves resources as well.
Collections can exist globally, at the top level of an API, but can also be contained inside a single resource. In the latter case, we refer to these collections as sub-collections. Sub-collections are usually used to express some kind of “contained in” relationship. We go into more detail on this in Relationships.
The diagram below illustrates the key concepts in a RESTful API.
￼
We call information that describes available resources types, their behavior, and their relationships the resource model of an API. The resource model can be viewed as the RESTful mapping of the application data model.
Resource Data¶
Resources have data associated with them. The richness of data that can be associated with a resource is part of the resource model for an API. It defines for example the available data types and their behavior.
Based on my experience, I have developed a strong conviction that the JSON data model has just the right “richness” so that it is an ideal data model for RESTful resources. I would recommend that everybody use it.
In JSON, just three types of data exist:
	▪	scalar (number, string, boolean, null).
	▪	array
	▪	object
Scalar types have just a single value. Arrays contain an ordered list of values of arbitrary type. Objects consist of a unordered set of key:value pairs (also called attributes, not to be confused with XML attributes), where the key is a string and the value can have an arbitrary type. For more detailed information on JSON, see the JSON web site.
Why the strong preference for JSON? In my view, JSON has the right balance between expressiveness, and broad availability. The three types of data (scalars, arrays and objects) are powerful enough to describe in a natural way virtually all the data that you might want to expose as resource, and at the same time these types are minimal enough so that almost any modern language has built-in support for them.
XML would be the other obvious contender. Actually, in the final incarnation of the RHEV-M API, XML is used to describe resources, via an XMLSchema definition. With hindsight, I believe that the XML data model is a bad choice for a RESTful API. On one side, it is too rich, and on the other side, it lacks features. XML, as an SGML off-shoot, is in my view great for representing structured documents, but not for representing structured data.
Features of XML that are too rich include:
	1.	Attributes vs elements. An XML element can have both attributes as well as sub-elements. A data item associated with a resource could be encoded in either one, and it would not be clear beforehand which one a client or a server should use.
	2.	Relevance of order. The order between child-elements is relevant in XML. It is not natural in my view for object attributes to have ordering.
The limitations of the XML data model are:
	1.	Lack of types. Elements in XML documents do not have types, and in order to use types, one would have to use e.g. XMLSchema. XMLSchema unfortunately is a strong contender for the most convoluted specification ever written.
	2.	Lack of lists. XML cannot natively express lists. This can lead to issues whereby it is not clear whether a certain element is supposed to be a list or an object, and where that element ends up being both.
Application Data¶
We define the data that can be associated with a resource in terms of the JSON data model, using the following mapping rules:
	1.	Resources are modeled as a JSON object. The type of the resource is stored under the special key:value pair “_type”.
	2.	Data associated with a resource is modeled as key:value pairs on the JSON object. To prevent naming conflicts with internal key:value pairs, keys must not start with “_”.
	3.	The values of key:value pairs use any of the native JSON data types of string, number, true, false, null, or arrays thereof. Values can also be objects, in which case they are modeling nested resources.
	4.	Collections are modeled as an array of objects.
We will also refer to key:value pairs as attributes of the JSON object, and we will be sloppy and use that same term for data items associated with resources, too. This use of attributes is not to be confused with XML attributes.
REST Metadata¶
In addition to exposing application data, resources also include other information that is specific to the RESTful API. Such information includes URLs and relationships.
The following table lists generic attributes that are defined and have a specific meaning on all resources. They should not be used for mapping application model attributes.
Attribute
Type
Meaning
id
String
Identifies the unique ID of a resource.
href
String
Identifies the URL of the current resource.
link
Object
Identifies a relationship for a resource. This attribute is itself an object and has “rel” “href” attributes.
Other Data¶
Apart from application data, and REST metadata, sometimes other data is required as well. This is usually “RPC like” data where a setting is needed for an operation, and where that setting will not end up being part of the resource itself.
One example that I can give here is where a resource creation needs a reference to another resource that is used during the creation, but where that other resource will not become part of the resource itself.
It is the responsibility of the API code to merge the application data together with the REST metadata and the other data into a single resource, resolving possible naming conflicts that may arise.
Representations¶
We have defined resources, and defined the data associated with them in terms of the JSON data model. However, these resources are still abstract entities. Before they can be communicated to a client over an HTTP connection, they need to be serialized to a textual representation. This representation can then be included as an entity in an HTTP message body.
The following representations are common for resources. The table also lists the appropriate content-type to use:
Type
Content-Type
JSON
application/x-resource+json
application/x-collection+json
YAML
application/x-resource+yaml
application/x-collection+yaml
XML
application/x-resource+xml
application/x-collection+xml
HTML
text/html
Note: all these content types use the “x-” experimental prefix that is allowed by RFC2046.
JSON Format¶
Formatting a resource to JSON is trivial because the data model of a resource is defined in terms of the JSON model. Below we give an example of a JSON serialization of a virtual machine:
{
  "_type": "vm",
  "name": "A virtual machine",
  "memory": 1024,
  "cpu": {
    "cores": 4,
    "speed": 3600
  },
  "boot": {
    "devices": ["cdrom", "harddisk"]
  }
}
YAML Format¶
Formatting to YAML is only slightly different than representing a resource in JSON. The resource type that is stored under the “_type” key/value pair is serialized as a YAML ”!type” annotation instead. The same virtual machine as above, now in YAML format:
!vm
name: A virtual machine
memory: 1024
cpu:
  cores: 4
  speed: 3600
boot:
  devices:
  - cdrom
  - harddisk
XML Format¶
XML is the most complex representation format due to both its complexity as well as its limitations. I recommend the following rules:
	▪	Resources are mapped to XML elements with a tag name equal to the resource type.
	▪	Attributes of resources are mapped to XML child elements with the tag name equal to the attribute name.
	▪	Scalar values are stored as text nodes. A special “type” attribute on the containing element should be used to refer to an XML Schema Part 2 type definition.
	▪	Lists should be stored as a single container element with child elements for each list item. The tag of the container element should be the English plural of the attribute name. The item tag should be the English singular of the attribute name. Lists should have the “xd:list” type annotation.
The same VM again, now in XML:
&lt;vm xmlns:xs="http://www.w3.org/2001/XMLSchema"&gt;
  &lt;name type="xs:string"&gt;My VM&lt;/name&gt;
  &lt;memory type="xs:int"&gt;1024&lt;/memory&gt;
  &lt;cpu&gt;
    &lt;cores type="xs:int"&gt;4&lt;/cores&gt;
    &lt;speed type="xs:int"&gt;3600&lt;/speed&gt;
  &lt;/cpu&gt;
  &lt;boot&gt;
    &lt;devices type="xs:list"&gt;
      &lt;device type="xs:string"&gt;cdrom&lt;/device&gt;
      &lt;device type="xs:string"&gt;harddisk&lt;/device&gt;
    &lt;/devices&gt;
  &lt;/boot&gt;
&lt;/vm&gt;
HTML Format¶
The exact format of a HTML response can be API dependent. HTML is for human consumption, and the only requirement is therefore that it be easy to understand. A simple implementation may choose the following representation:
	▪	For a collection, a &lt;table&gt; with a column per attribute where each object is listed in a separate row.
	▪	For a resource, a &lt;table&gt; with two columns, one with all the attribute names, one with the corresponding attribute value.
Content-Types¶
As can be seen above, I am advocating the use of a generic content types “application/x-resource+format” and “application/x-collection+format”. In my view this represents the right middle ground between two extremes that are commonly found in RESTful APIs:
Some RESTful APIs only use the “bare” XML, JSON or YAML content types. An example of such as API is the Red Hat Enterprise Virtualization API. In this case, the content type expresses nothing but the fact that an entity is in XML, JSON or YAML format. In my view, this is not sufficient. Resources and collections have some specific semantics around for example the use of “href” attributes, “link” attributes, and types. Therefore, these are a specialization of XML, JSON and YAML and should be defined as such.
Other RESTful APIs define a content-type for every resource type that exists in the resource model. Examples of such APIs include for example VMware’s vSphere Director API. In my view, this is not proper either. Specifying detailed content types invites both the API implementer, as well as a client implementer to think about these types as having specific interfaces. In my view though, all resources should share the same basic interface, which is defined by the RESTful design principles and expressed by the “application/x-resource” content type.
One reason that is sometimes given in favor of defining detailed content types is that this way, the content type can be associated with a specific definition in some type definition language (such as XMLSchema). This, supposedly, facilitates client auto-discovery because a client can know available attributes for a certain type. I go into a lot of detail on this topic in Forms but the summary is that I do not agree with this argument.
Selecting a Representation Format¶
Clients can express their preference for a certain representation format using the HTTP “Accept” header. The HTTP RFC defines an elaborate set of rules in which multiple formats can be requested, each with its own priority. In the following example, the client tells the API that it accepts only YAML input:
GET /api/collection
Accept: application/x-collection+yaml

URLs¶
Entry Point¶
A RESTful API needs to have one and exactly one entry point. The URL of the entry point needs to be communicated to API clients so that they can find the API.
Technically speaking, the entry point can be seen as a singleton resource that exists outside any collection. It is common for the entry point to contain some or all of the following information:
	▪	Information on API version, supported features, etc.
	▪	A list of top-level collections.
	▪	A list of singleton resources.
	▪	Any other information that the API designer deemed useful, for example a small summary of operating status, statistics, etc.
URL Structure¶
Each collection and resource in the API has its own URL. URLs should never be constructed by an API client. Instead, the client should only follow links that are generated by the API itself.
The recommended convention for URLs is to use alternate collection / resource path segments, relative to the API entry point. This is best described by example. The table below uses the ”:name” URL variable style from Rail’s “Routes” implementation.
URL
Description
/api
The API entry point
/api/:coll
A top-level collection named “coll”
/api/:coll/:id
The resource “id” inside collection “coll”
/api/:coll/:id/:subcoll
Sub-collection “subcoll” under resource “id”
/api/:coll/:id/:subcoll/:subid
The resource “subid” inside “subcoll”
Even though sub-collections may be arbitrarily nested, in my experience, you want to keep the depth limited to 2, if possible. Longer URLs are more difficult to work with when using simple command-line tools like curl.
Relative vs Absolute¶
It is strongly recommended that the URLs generated by the API should be absolute URLs.
The reason is mostly for ease of use by the client, so that it never has to find out the correct base URI for a resource, which is needed to interpret a relative URL. The URL RFC specifies the algorithm for determining a base URL, which is rather complex. One of the options for finding the base URL is to use the URL of the resource that was requested. Since a resource may appear under multiple URLs (for example, as part of a collection, or stand-alone), it would be a significant overhead to a client to remember where he retrieved a representation from. By using absolute URLs, this problem doesn’t present itself.
URL Templates¶
A draft standard for URL templates exists. URL templates can be useful in link attributes where the target URL takes query arguments. It is recommended though to make only conservative use of these. So far the only good use case I have come across is when searching in a collection. In that case, it seems fair that the search criteria can be specified as GET style query arguments appended to the collection URL.
I would recommend to only use the “literal expansion” part of the URL templates spec, because the “expression expansion” part adds too much complexity to a client in my view, for very little benefit.
Variants¶
Sometimes you need to work on a variant of a resource. In our RHEV-M API for example, some attributes on a virtual machine can be updated while a virtual machine is running. This amounts to a hot plug/unplug of the resource, which is a different operation altogether from changing the saved representation. A nice way to implement this is using ”;variant” identifier in a URL. For example:
URL
Description
/api/:coll/:id;saved
Identifies the saved variant of a resource.
/api/:coll/:id;current
Identifies the current variant of a resource.
The use of the semicolon to provide options that are specific to a path segment is explicitly alowed in RFC3986 The advantage over using a ”?variant” query argument is that this format is specific to a path segment only, and would allow you to e.g. work on the saved variant of a sub-resource in a sub-collection of the current variant of another resource.

Methods¶
Standard Methods¶
We already discussed that resources are the fundamental concept in a RESTful API, and that each resource has its own unique URL. Methods can be executed on resources via their URL.
The table below lists the standard methods that have a well-defined meaning for all resources and collections.
Method
Scope
Semantics
GET
collection
Retrieve all resources in a collection
GET
resource
Retrieve a single resource
HEAD
collection
Retrieve all resources in a collection (header only)
HEAD
resource
Retrieve a single resource (header only)
POST
collection
Create a new resource in a collection
PUT
resource
Update a resource
PATCH
resource
Update a resource
DELETE
resource
Delete a resource
OPTIONS
any
Return available HTTP methods and other options
Normally, not all resources and collections implement all methods. There are two ways to find out which methods are accepted by a resource or collection.
	1.	Use the OPTIONS method on the URL, and look at the “Allow” header that is returned. This header contains a comma-separated list of methods are are supported for the resource or collection.
	2.	Just issue the method you want to issue, but be prepared for a “405 Method Not Allowed” response that indicates the method is not accepted for this resource or collection.
Actions¶
Sometimes, it is required to expose an operation in the API that inherently is non RESTful. One example of such an operation is where you want to introduce a state change for a resource, but there are multiple ways in which the same final state can be achieved, and those ways actually differ in a significant but non-observable side-effect. Some may say such transitions are bad API design, but not having to model all state can greatly simplify an API. A great example of this is the difference between a “power off” and a “shutdown” of a virtual machine. Both will lead to a vm resource in the “DOWN” state. However, these operations are quite different.
As a solution to such non-RESTful operations, an “actions” sub-collection can be used on a resource. Actions are basically RPC-like messages to a resource to perform a certain operation. The “actions” sub-collection can be seen as a command queue to which new action can be POSTed, that are then executed by the API. Each action resource that is POSTed, should have a “type” attribute that indicates the type of action to be performed, and can have arbitrary other attributes that parameterize the operation.
It should be noted that actions should only be used as an exception, when there’s a good reason that an operation cannot be mapped to one of the standard RESTful methods. If an API has too many actions, then that’s an indication that either it was designed with an RPC viewpoint rather than using RESTful principles, or that the API in question is naturally a better fit for an RPC type model.
PATCH vs PUT¶
The HTTP RFC specifies that PUT must take a full new resource representation as the request entity. This means that if for example only certain attributes are provided, those should be remove (i.e. set to null).
An additional method called PATCH has been proposed recently. The semantics of this call are like PUT inthat it updates a resource, but unlike PUT, it applies a delta rather than replacing the entire resource. At the time of writing, PATCH was still a proposed standard waiting final approval.
For simple resource representations, the difference is often not important, and many APIs simply implement PUT as a synonym for PATCH. This usually doesn’t give any problems because it is not very common that you need to set an attribute to null, and if you need to, you can always explicitly include it.
However for more complex representations, especially including lists, it becomes very important to be able to express accurately the changes you want to make. Therefore, it is my recommendation now to both provide PATCH and PUT, and make PATCH do an relative update and have PUT replace the entire resource.
It is important to realize that the request entity to PATCH is of a different content-type that the entity that it is modifying. Instead of being a full resource, it is a resource that describes modifications to be made to a resource. For a JSON data model, which is what this essay is advocating, I believe that there are two sensible ways to define the patch format.
	1.	An informal approach where you accept a dict with a partial representation of the object. Only attributes that are present are updated. Attributes that are not present are left alone. This approach is simple, but it has the drawback that if the resource has a complex internal structure e.g. containing a big list of dicts, then that entire list of dicts need to be given in the entity. Effectively PATCH becomes similar to PUT again.
	2.	A more formal approach would be to accept a list of modifications. Each modification can be a dict specifying the JSON path of the node to modify, the modification (‘add’, ‘remove’, ‘change’) and the new value.
Link Headers¶
An Internet draft exists defining the “Link:” header type. This header takes the “link” attributes of the response entity, and formats them as an HTTP header. It is argued that the Link header is useful because it allows a client to quickly get links from a response without having to parse the response entity, or even without retrieving the response entity at all using the HEAD HTTP method.
In my view, the usefulness of this feature is dubious. First of all, it can increase response sizes quite significantly. Second, it can only be used when a resource is being returned; it does not make sense to be used with collections. Because I have not yet seen any good use of this header in the context of a RESTful API, I recommend not to implement Link headers.
Asynchronous Requests¶
Sometimes an action takes too long to be completed in the context of a single HTTP request. In that case, a “202 Accepted” status can be returned to the client. Such a response should only be returned for POST, PUT, PATCH or DELETE.
The response entity of a 202 Accepted response should be a regular resource with only the information filled in that was available at the time the request was accepted. The resource should contain a “link” attribute that points to a status monitor that can be polled to get updated status information.
When polling the status monitor, it should return a “response” object with information on the current status of the asynchronous request. If the request is still in progress, such a response could look like this (in YAML):
!response
status: 202 Accepted
progress: 50%
If the call has finished, the response should include the same headers and response body had the request been fulfilled synchronously:
!response
status: 201 Created
headers:
 - name: content-type
   value: applicaton/x-resource+yaml
response: !!str
  Response goes here
After the response has been retrieved once with a status that is not equal to “202 Accepted”, the API code may garbage collect it and therefore clients should not assume it will continue to be available.
A client may request the server to modify its asynchronous behavior with the following “Expect” headers:
	▪	“Expect: 200-ok/201-created/204-no-content” disables all asynchronous functionality. The server may return a “417 Expectation Failed” if it is not willing to wait for an operation to complete.
	▪	“Expect: 202-accepted” explicitly request an asynchronous response. The server may return a “417 Expectation Failed” if it is not willing to perform the request asynchronously.
If no expectation is provided, client must be prepared to accept a 202 Accepted status for any request other than GET.
Ranges / Pagination¶
When collections contain many resources, it is quite a common requirement for a client to retrieve only a subset of the available resources. This can be implemented using the Range header with a “resource” range unit:
GET /api/collection
Range: resources=100-199
The above example would return resources 100 through 199 (inclusive).
Note that it is the responsibility of the API implementer to ensure a proper and preferably meaningful ordering can be guaranteed for the resources.
Servers should provide an “Accept-Ranges: resource” header to indicate to a client that they support resource-based range queries. This header should be provided in an OPTIONS response:
OPTIONS /api/collection HTTP/1.1

HTTP/1.1 200 OK
Accept-Ranges: resources
Notifications¶
Another common requirement is where a client wants to be notified immediately when some kind of event happens.
Ideally, such a notification would be implemented using a call-out from the server to the client. However, there is no good portable standard to do this over HTTP, and it also breaks with network address translation and HTTP proxies. A second approach called busy-loop polling is horribly inefficient.
In my view, the best approach is what is is called “long polling”. In long polling, the client will retrieve a URL but the server will not generate a response yet. The client will wait for a configurable amount of time, until it will close the connection and reconnect. If the server becomes aware of an event that requires notification of clients, it can provide that event immediately to clients that are currently waiting.
Long polling should be disabled by default, and can be enabled by a client using an Expect header. For example, a client could long poll for new resources in a collection using a combination of long-polling and a resource-based range query:
GET /api/collection
Range: 100-
Expect: nonempty-response
In this case, resource “100” would be the last resource that was read, and the call is requesting the API to return at least one resource with an ID &gt; 100.
Server implementers need to decide whether they want to implement long polling using one thread per waiting client, or one thread that uses multiplexed IO to wait for all clients. This is a trade-off to be made between ease of implementation and scalability (that said, threads are pretty cheap on modern operating systems).

Relationships¶
As we have seen in Resources, the resource is the fundamental unit in RESTful API design. Resources model objects from the application data model.
Resources do not exist in isolation, but have relationships to other other resources. Sometimes these relationships exist between the mapped objects in the application data model as well, sometimes they are specific to the RESTful resources.
One of the principles of the RESTful architecture style is that these relationships are expressed by hyperlinks to the representation of a resource.
In our resource model, we interpret any object with an “href” attribute as a hyperlink. The value of the href attribute contains an absolute URL that can be retrieved with GET. Using GET on a such a URL is guaranteed to be side-effect free.
Two types of hyperlinks are normally used:
	1.	Using “link” objects. This is a special object of type “link” with “href” and a “rel” attributes. The “rel” attribute value identifies the semantics of the relationship. The link object is borrowed from HTML.
	2.	Using any object with a “href” attribute. The object type defines the semantics of the relationship. I’ll call these “object links,” not to be confused with the “link objects” above.
Below is an example of a virtual machine representation in YAML that illustrates the two different ways in which a relationship can be expressed:
!vm
id: 123
href: /api/vms/123
link:
  rel: collection/nics
  href: /api/vms/123/nics
cluster:
  href: /api/clusters/456
The “link” object is used with rel=”collection/nics” to refer to a sub-collection to the VM holding the virtual network interfaces. The cluster link instead points to the cluster containing this VM by means of a “cluster” object.
Note that the VM itself has a href attribute as well. This is called a “self” link, and it is a convention for a resource to provide such a link. That way, the object can later be easily re-retrieved or updated without keeping external state.
There are no absolute rules when to use link objects, and when not. That said, we have been using the rules below in the rhevm-api project with good success. They seem to be a good trade-off between compactness (favoring object links) and the ability to understand links without understanding the specific resource model (favoring link objects).
	▪	Link objects are used to express structural relationships in the API. So for example, the top-level collections, singleton resources and sub-collections (including actions) are all referenced using link objects.
	▪	Object links are used to express semantic relationships from the application data model. In the example above, the vm to cluster link comes directly from the application data model and is therefore modeled as a link.
Note however that sub-collections can express both a semantic relationship, as well as a structural relationship. See for example the “collection/nics” sub-collection to a VM in the example above. In such a case, our convention has been to use link objects.
Standard Structural Relationships¶
It makes sense to define a few standard structural relationship names that can be relevant to many different APIs. In this way, client authors know what to expect and can write to some extent portable clients that work with multiple APIs. The table below contains a few of these relationship names.
Relationship
Semantics
collection/{name}
Link to a related collection {name}.
resource/{name}
Link to a related resource {name}.
form/{name}
Link to a related form {name}.
Here, {name} is replaced with a term that has meaning for the specific API. For example, a collection of virtual machines could be linked from the entry point of a virtualization API using rel=”collection/vms”.
Modeling Semantic Relationships¶
Semantic relationships can be modeled either by an object link, or by a sub-collection. I believe that choosing the right way to represent a sub-collection is important to get a consistent API experience for the client. I would advocate using the following rules:
	1.	In case of a 1:N relationship, where the target object is existentially dependent on the source object, I’d recommend to use a sub-collection. By “existentially dependent” I mean that a target object cannot exist without its source. In database parlance, this would be a FOREIGN KEY relationship with an ON DELETE CASCADE. An example of such a relationship are the NICs that are associated with a VM.
	2.	In case of a 1:N relationship, where there is data that is associated with the link, I’d recommend to use a sub-collection. Note that we are talking about data here that is neither part of the source object, nor the target object. The resources in the sub-collection can hold the extra data. In this case, the data in the sub-resource would therefore be a merge of the data from the mapped object from the application data model, and link data.
	3.	In case of any other 1:N relationship, I’d recommend to use an object link.
	4.	In case of a N:M relationship, I’d recommend to use a sub-collection. The sub-collection can be defined initially to support the lookup direction that you most often need to follow. If you need to query both sides of the relationship often, then two sub-collections can be defined, one on each linked object.


Forms¶
In this section, a solution is proposed to a usability issue that is present in a lot of RESTful APIs today. This issue is that, without referring to external documentation, an API user does not know what data to provide to operations that take input. For example, the POST call is used to create a new resource in a collection, and an API user can find out with the OPTIONS call that a certain collection supports POST. However, he does not know what data is required to create the new resource. So far, most RESTful APIs require the user to refer to the API documentation to get that information. This violates an important RESTful principle that APIs should be self-descriptive.
As far as I can see, this usability issue impacts two important use cases:
	1.	A person is browsing the API, trying to use and learn it as he goes. The person can either be using a web browser, or a command-line tool like “curl”.
	2.	The development of a command-line or graphical interface to a RESTful API. Without a proper description of required input types, knowledge about these has to be encoded explicitly in the client. This has the disadvantage that new features are not automatically exposed, and that there are strict requirements around not making backwards incompatible changes.
When we look at the web, this issue doesn’t exist. Millions of users interact with all kinds of IT systems over the web at any given moment, and none of them has to resort to API documentation to understand how to do that. Interactions with IT systems on the web are all self-explanatory using hypertext, and input is guided by forms.
In fact, forms are what I see as the solution here. But before I go into that, let’s look into two classes of solutions that have been proposed to address this issue, and why I think they actually do not solve it. The first is using a type definition language like XMLSchema to describe the input types, the second is using some kind of service description language like WADL.
Type Definition¶
As alluded to in Resources, some RESTful APIs use a type definition language to provide information about how to construct request entities. In my view this is a bad approach, and has the following issues:
	1.	It creates a tight coupling between servers and clients.
	2.	It still does not allow a plain web browser as an HTTP client.
	3.	It does not help in the automatic construction of CLIs.
Often, XMLSchema is used as the type definition language. This brings in a few more issues:
	1.	XMLSchema is a horribly complicated specification on one side, but on the other side it can’t properly solve the very common requirement of an unordered set of key:value pairs (Note that &lt;xs:all&gt; does not solve this. Its limitations make it almost useless.).
	2.	Type definitions in XMLSchema are context-free. This means that you cannot just define one type, you need to define one type for PUT, one for POST, etc. And since one XML element can only have one type, this means you’re PUTing a different element that you’d POST or GET. This breaks that “homogeneous collection” principle of REST.
Service Definition¶
WADL is an approach to define a service description language for RESTful APIs. It tries to solve the problem by meticulously defining entry points and parameters to those. The problem I have with WADL, is that it feels very non-RESTful. I can’t see a lot of difference between a method on a WADL resource, and an RPC entry point.
It would be possible to construct a more RESTful service description language. It would focus on mapping the RESTful concepts of resource, collection, relationships, and links. It would probably need a type definition language as well to define the constraints on types for each method (again, PUT may have different constraints than POST). There have been discussions in the RHEV-M project on this.
In the end though, this approach also feels wrong. What I think is needed is something that works like the web, where everything is self descriptive, and guided only by the actual URL flow the user is going through, not by reference to some external description.
Using Forms to Guide Input¶
The right solution to guide client input in my view is to use forms. Note that I’m using the word “forms” here in a generic sense, as an entity that guides user input, and not necessarily equal to an HTML form.
In my view, forms need to specify three pieces of information:
	1.	How to contact the target and format the input.
	2.	A list of all available input fields.
	3.	A list of constraints which which the input fields must comply.
HTML forms provide for #1 and #2, but not for #3 above. #3 is typically achieved using client-side Javascript. Because Javascript is not normally available to clients that use the API, we need to define another way to do validation. The approach I propose here is to define a form definition language that captures the information #1 through #3. A form expressed in this language can be sent over to the client as-is, in case the client understands the form language. It can also be translated into an HTML form with Javascript in case the client is a web browser.
Form Definition Language¶
Our forms are defined as a special RESTful resource with type “form”. This means they use the JSON data model, can be represented in JSON, YAML and XML according to the rules in Resources.
Forms have their own content type. This is listed in the table below:
Type
Content-Type
Form
application/x-form+json
application/x-form+yaml
application/x-form+xml
Rather than providing a formal definition, I will introduce the form definition language by using an example. Below is an example of a form that guides the the creation of a virtual machine. In YAML format:
!form
method: POST
action: {url}
type: vm
fields:
- name: name
  type: string
  regex: [a-zA-Z0-9]{5,32}
- name: description
  type: string
  maxlen: 128
- name: memory
  type: number
  min: 512
  max: 8192
- name: restart
  type: boolean
- name: priority
  type: number
  min: 0
  max: 100
constraints:
- sense: mandatory
  field: name
- sense: optional
  field: description
- sense: optional
  field: cpu.cores
- sense: optional
  field: cpu.sockets
- sense: optional
  exclusive: true
  constraints:
  - sense: mandatory
    field: highlyavailable
  - sense: optional
    field: priority
As can be seen the form consists of 3 parts: form metadata, field definitions, and constraints.
Form Metadata¶
The form metadata is very simple. The following attributes are defined:
Attribute
Description
method
The HTTP method to use. Can be GET, POST, PUT or DELETE.
url
The URL to submit this form to.
type
The type of the resource to submit.
Fields¶
The list of available fields are specified using the “fields” attribute. This should be a list of field definitions. Each field definition has the following attributes:
Attribute
Description
name
The field name. Should be in dotted-name notation.
type
One of “string”, “number” or “boolean”
min
Field value must be greater than or equal to this (numbers)
max
Field value must be less than or equal to this (numbers)
minlen
Minimum field length (strings)
maxlen
Maximum field length (strings)
regex
Field value needs to match this regular expression (strings)
multiple
Boolean that indicates if multiple values are accepted (array).
Constraints¶
First we need to answer the question of what kind of constraints we want to express in our form definition language. I will start by mentioning that in my view, it is impossible to express each and every constraint on the client side. Some constraints for example require access to other data (e.g. when creating relationships), are computationally intensive, or even unknown to the API designer because they are undocumented for the application the API is written for. So in my view we need to find a good subset that is useful, without making it too complicated and without worrying about the fact that some constraints possibly cannot not be expressed.
This leads me to define the following two kinds of constraints, which in my view are both useful, as well as sufficient for our purposes:
	1.	Constraints on individual data values.
	2.	Presence constraints on fields, i.e. whether a field is allowed or not allowed, and if allowed, whether it is mandatory.
The constraints on data values are useful because CLIs and GUIs could use this information to help a user input data. For example, depending on the type, a GUI could render a certain field a checkbox, a text box, or a dropdown list. For brevity, constraints on individual data values are defined as part of the field definition, and were discussed in the previous section.
Presence constraints are also useful, as they allow an API user to generate a concise synopsis on how to call a certain operation to be used e.g. in CLIs. Presence constraints specify which (combination of) input fields can and cannot exist. Each presence constraint has the following attributes:
Attribute
Description
sense
One of “mandatory” or “optional”
field
This constraint refers to a field.
constraints
This constraint is a group with nested contrainst.
exclusive
This is an exclusive group (groups only).
Either “field” or “constraints” has to be specified, but not both. A constraint that has the field attribute set is called a simple constraint. A constraint that has the constraints attribute set, is called a group.
Checking Constraints¶
Value constraints should be checked first, and should be checked only on non-null values.
After value constraints, the presence constraints should to be checked. This is a bit more complicated because the constraints are not only used for making sure that all mandatory fields exist, but also that no non-optional fields are present. The following algorithm should be used:
	1.	Start with an empty list called “referenced” that will collect all referenced fields.
	2.	Walk over all constraints in order. If a constraint is a group, you need to recurse into it, depth first, passing it the “referenced” list.
	3.	For every simple constraint that validates, add the field name to “referenced”.
	4.	In exclusive groups, matching stops at the first matching sub-constraint, in which case the group matches. In non-exclusive groups, matching stops at the first non-matching sub-constraint, in which case the group does not match.
	5.	When matching a group, you need to backtrack to the previous value of “referenced” in case the group does not match.
	6.	A constraint only fails if it is mandatory and it is a top-level constraint. If a constraint fails, processing may stop.
	7.	When you’ve walked through all constraints, it is an error if there are fields that have a non-null value but are not in the referenced list.
Building the Request Entity¶
After all constraints have been satisfied, a client should build a request entity that it will pass in the body of the POST, PUT or DELETE method. In case the form was requested in JSON, YAML or XML format, it is assumed that the client is not a web browser, and the following applies:
First, a new resource is created of the type specified in the form metadata. Dotted field names should be interpreted as follows. Each dot creates a new object, and stores it under the name immediately left of the dot in its parent object. This means that the parent must be an object as well, which means it cannot correspond to a field definition with “multiple” set to “true” (which would make it a list). This resource is then represented in a format that the server supports, using the rules described in Resources.
If a client requested a “text/html” representation of the form, it is assumed that the client is a web browser, and we assume the form will be processed as a regular HTML form. In this case:
	▪	An HTML &lt;form&gt; should be generated, with an appropriate &lt;input&gt; element for each field.
	▪	The HTML form’s “method” attribute should be set to POST, unconditionally. In case the RESTful form’s method is not POST, the server should include a hidden input element with the name “_method” to indicate to the server the original method. (HTML does not support PUT or DELETE in a form).
	▪	The form’s “enctype” should be set to “multipart/form-data” or “application/x-www-form-urlencoded”, as appropriate for the input elements.
	▪	A hidden field called “_type” is generated that contains the value of the “type” attribute in the form metadata.
	▪	The server may generate Javascript and include that in the HTML to check the value and presence constraints.
Linking to Forms¶
Forms are associated with resources and collections by link objects. The name of the link object defines the meaning of the form. The following standard forms names are defined:
Name
Scope
Description
form/search
collection
Form to search for resources
form/create
collection
Form to create a new resource
form/update
resource
Form to update a resource
form/delete
resource
Form to delete a resource


Miscellaneous Topics¶
Offline Help¶
A requirement that sometimes comes up is that to have an offline description of the API that can be used to generate offline help for a command-line interface (CLI) for that API. When connected to an API, all this information is of course available because of the self-descriptive nature of a RESTful API. But the request is to have that information offline as well, so that CLI users can get help without have to connect first.
In my view it is a bad idea to offer this offline help. It introduces a tight coupling between a client and a server, which will break the RESTful model. A server cannot be independently upgraded anymore from a client, and clients become tied to a specific server version. Also I doubt the usefulness of this feature as having to connect first to get help does not seem like a big issue to me.
That said, it is relatively straightforward to define a way that such an offline description can be facilitated.
One server-side change is required for this. For each collection, the API should implement a placeholder resource with a special ID (let’s say “_”, but it doesn’t matter as long as it cannot be a valid ID), and some fake data. When a special HTTP Expect header is set, only this placeholder resource is returned when querying a collection.
Having the placeholder resources in place for every collection, the following procedure can be used by the client to retrieve all relevant metadata:
	1.	Retrieve the entry point of the API and store it in memory as a resource.
	2.	For every hyperlink in the entry point, fetch the target, and store it under the “target” property under the link object, recursively. When collections are retrieved, the special Expect header must be set.
	3.	For every hyperlink in the entry point, execute an OPTIONS call on the target, and store the resulting headers under the “options” property of the link object, recursively.
	4.	Serialize the resulting object to a file.
The result of this is basically a recursive dump of the “GET”-able part of an API with one placeholder resource in every collection. It will contain all metadata including forms, options, available resource types and collections, in a format that is very similar to the real on-line data. It can be used as an off-line store to generate any help that you would be able to generate online as well.
An API can store a version number on its entry point, so that clients can see when their offline cached copy expired and can therefore generate a new one.
</Text>
            <Notes>https://restful-api-design.readthedocs.org/en/latest/</Notes>
        </Document>
        <Document ID="3">
            <Title>Title Page</Title>
            <Text>









&lt;$projecttitle&gt;











&lt;$fullname&gt;











Course Title
Instructor’s Name
&lt;$longdate&gt;
</Text>
            <Notes>The &lt;$projecttitle&gt; and &lt;$fullname&gt; tags get replaced with the information set in Project &gt; Meta-Data Settings… &gt; Project Properties. You can edit those settings or just replace this text altogether.</Notes>
        </Document>
        <Document ID="21">
            <Title>Stop Designing Fragile Web APIs</Title>
            <Synopsis>	◦	Design your API with a specific intent in mind.
	◦	Be vague in the details.
	◦	Provide multiple APIs differentiated by their intent.
	◦	Reduce code duplication by sharing common implementations, not by providing a generic service.
</Synopsis>
            <Text>Stop Designing Fragile Web APIs
My customers will be angry if I break my API.
When you release your Web API, it’s carved into stone. It’s a scary commitment to never make an incompatible change. If you fail, you’ll have irate customers yelling in your inbox, followed by your boss, and then your boss’s boss. You have to support this API. Forever. Unless you version it, right?
After publishing The Web API Checklist, I received comments (#1, #2) regarding API versioning. Before you struggle with how to version your API, I want you to know how to design your API to avoid future incompatibilities.
If my API is well designed, it won’t be fragile.
It is possible to design your API in a manner that reduces its fragility and increases its resilience to change. The key is to design your API around its intent. In the SOA world, this is also referred to as business-orientation. It’s a difficult design concept to understand, best explained with a fictitious example:

What’s the difference between these URLs?
http://api.fbi.gov/wanted?
  order_by=notoriety,desc&amp;
  limit=10&amp;
  page=1&amp;
  fields=name,aka,known_associates,
  reward,description,last_seen
This is a URL for the FBI most wanted list. The API contains many features: you can order by arbitrary fields, ascending or descending; you can specify the result count; you can query page-by-page; and you can specify details to retrieve.
Versus…
http://api.fbi.gov/wanted/most
This is a URL for the FBI most wanted list.
These URLs have the same goal, but they vary in how they accomplish it. The first is a programmer’s design. Programmers know what they can provide, and they give you every feature they can. The design does not describe the intent of the user, but instead defines details about the request. The second URL is very specific about its intent to deliver “The FBI Most Wanted List”, and vague about the details; this is an intent-driven design.
Designing your API with “intent” in mind will reduce fragility.
The intent-driven design has advantages over the programmer’s design:
#1. Easier to Use — No knobs to turn, no details to learn.
#2. Flexible — If the list ordering changes from by notoriety to by public_outcry, the intent-driven API can change server-side.
#3. Consistent — A 24-hour news network can’t sort the list by race then notoriety.
#4. Loosely Coupled — After complaints that “notoriety” is a made-up number, the FBI can hide the field; the intent-driven design is unchanged. (Edit: Whoops, this is a backwards incompatible change; there’s a blunder! If it was important to remove “notoriety” immediately due to a PR kerfuffle, you’re not going to want to provide it in a versioned API either. You could just start returning a fixed value all the time though, which would be backwards-compatible and work well in the intent-driven API.)
#5. Optimizable — Can be calculated when the database is updated, instead of built on-demand. Optimizing the programmer’s design for every combination is much more difficult.
#6. Cacheable — Easily made cacheable. The complexity of the programmer’s design makes caching difficult, (eg. normalizing the query parameters), and ineffective (eg. &amp;limit=5 / 10 / 15 are cache misses).
#7. Easier to Develop — Higher complexity makes it difficult and time-consuming to develop and test the programmer’s API design.
#8. Efficient Validation Model Caching — Quickly check If-None-Match/If-Modified-Since HTTP headers to provide “304 Not Modified” responses.
But, I need a more generic API design…
I hope you’re thinking of a specific reason why you need that generic design. That reason is the intent that allows you to design a better API. For example, the flexible API is great for developing a user interface: it allows sorting by any field, customizable pagination size, and filtering or searching. With that UI intent in mind, you can make design decisions that will provide an easy-to-use, easy-to-develop, flexible, consistent, loosely coupled, optimizable, cacheable, and efficient API. For example:
	◦	Asked to sort by a field that’s been removed? Meh, ignore it. Interactive users will compensate by selecting a new ordering.
	◦	API request has a ridiculous pagination size, and a request is like a DoS attack? Max out at a reasonable page size, like 100.
	◦	Ordering records by an expensive aggregate calculation? Denormalize that field and have it calculated in an overnight job, and add visibility into the staleness for the UI.
Create multiple, specific API endpoints for specific intents, and use that intent to influence the design.
What about DRY (don’t repeat yourself)?…
Don’t repeat yourself in your implementation, but don’t worry about repeating yourself in your API design. If you provide multiple API endpoints to retrieve similar objects with different intent, start with common code paths. Specialize when needed.
More specific services are easier for clients to use, and for you to maintain. Those are the same advantages that DRY gives you.
Does this really work in a real-world API?
Yes, this applies to the real world. An excellent example of this is GitHub’s commit status API, which is used by continuous integration services to mark repository versions as good to merge.
	◦	Defines specific functionality: assign a state to a revision of a repository.
	◦	GitHub automatically associates it with pull-requests, and displays the status prettily.
	◦	No HTML, or customizable states; the API has the minimal amount of data required.
	◦	Future-proof: it satisfies a defined problem in a minimalist manner.
	◦	Statuses are associated with revisions; adding more commits to a pull request works like magic.
	◦	GitHub has flexibility to make changes without breaking compatibility:
	◦	GitHub could rewrite the entire pull request feature without touching this API.
	◦	Statuses could be applied to other UIs, like the repository history.
	◦	The same API could be applied if GitHub became MercurialHub or SubversionHub or PerforceHub or CvsHub or RcsHub.
	◦	GitHub has flexibility in displaying the commit status, because of the simple information collected:
	◦	Statuses can be displayed in a mobile application.
	◦	Localization of the “Good to merge” text is easy.
	◦	Did you know that colors have different associations in different cultures? Green, yellow, and red can be localized too.
Another real-world example is Twilio’s AvailablePhoneNumbers API. The intent is to search for a phone number to assign to your account. It looks like a typical collection API, but details that aren’t relevant to the intent are missing, like specifying how many numbers to return or ordering them.
Summary
	◦	Design your API with a specific intent in mind.
	◦	Be vague in the details.
	◦	Provide multiple APIs differentiated by their intent.
	◦	Reduce code duplication by sharing common implementations, not by providing a generic service.

Please join in the discussion on this article over on Hacker News and reddit.

Did you find this article interesting? I plan to write more on APIs in the future, if you’d like to catch whatever I have to say next, I’m starting a mailing list where I’ll be distributing more API-related content.
</Text>
            <Notes>https://mathieu.fenniak.net/stop-designing-fragile-web-apis/

http://api.fbi.gov/wanted?
  order_by=notoriety,desc&amp;
  limit=10&amp;
  page=1&amp;
  fields=name,aka,known_associates,
  reward,description,last_seen
Versus…
http://api.fbi.gov/wanted/most

Create multiple, specific API endpoints for specific intents, and use that intent to influence the design.
An excellent example of this is GitHub’s commit status API,</Notes>
        </Document>
        <Document ID="14">
            <Title>Ideas</Title>
        </Document>
        <Document ID="22">
            <Title>Best Practices for Designing a Pragmatic RESTful API</Title>
            <Text>Best Practices for Designing a Pragmatic RESTful API
Your data model has started to stabilize and you're in a position to create a public API for your web app. You realize it's hard to make significant changes to your API once it's released and want to get as much right as possible up front. Now, the internet has no shortage on opinions on API design. But, since there's no one widely adopted standard that works in all cases, you're left with a bunch of choices: What formats should you accept? How should you authenticate? Should your API be versioned?
In designing an API for SupportFu (a Zendesk Alternative), I've tried to come up with pragmatic answers to these questions. My goal is for the SupportFu API to be easy to use, easy to adopt and flexible enough to dogfood for our own user interfaces.
TL;DR
An API is a user interface for a developer - so put some effort into making it pleasant Use RESTful URLs and actions Use SSL everywhere, no exceptions An API is only as good as its documentation - so have great documentation Version via the URL, not via headers Use query parameters for advanced filtering, sorting &amp; searching Provide a way to limit which fields are returned from the API Return something useful from POST, PATCH &amp; PUT requests HATEOAS isn't practical just yet Use JSON where possible, XML only if you have to You should use camelCase with JSON, but snake_case is 20% easier to read Pretty print by default &amp; ensure gzip is supported Don't use response envelopes by default Consider using JSON for POST, PUT and PATCH request bodies Paginate using Link headers Provide a way to autoload related resource representations Provide a way to override the HTTP method Provide useful response headers for rate limiting Use token based authentication, transported over OAuth2 where delegation is needed Include response headers that facilitate caching Define a consumable error payload Effectively use HTTP Status codes  ... or just skip to the bottom and signup for updates
Key requirements for the API
Many of the API design opinions found on the web are academic discussions revolving around subjective interpretations of fuzzy standards as opposed to what makes sense in the real world. My goal with this post is to describe best practices for a pragmatic API designed for today's web applications. I make no attempt to satisfy a standard if it doesn't feel right. To help guide the decision making process, I've written down some requirements that the API must strive for:
	•	It should use web standards where they make sense
	•	It should be friendly to the developer and be explorable via a browser address bar
	•	It should be simple, intuitive and consistent to make adoption not only easy but pleasant
	•	It should provide enough flexibility to power majority of the SupportFu UI
	•	It should be efficient, while maintaining balance with the other requirements
An API is a developer's UI - just like any UI, it's important to ensure the user's experience is thought out carefully!
Use RESTful URLs and actions
If there's one thing that has gained wide adoption, it's RESTful principles. These were first introduced by Roy Felding in Chapter 5 of his dissertation on network based software architectures.
The key principles of REST involve separating your API into logical resources. These resources are manipulated using HTTP requests where the method (GET, POST, PUT, PATCH, DELETE) has specific meaning.
But what can I make a resource? Well, these should be nouns (not verbs!) that make sense from the perspective of the API consumer. Although your internal models may map neatly to resources, it isn't necessarily a one-to-one mapping. The key here is to not leak irrelevant implementation details out to your API! Some of SupportFu's nouns would be ticket, user and group.
Once you have your resources defined, you need to identify what actions apply to them and how those would map to your API. RESTful principles provide strategies to handle CRUD actions using HTTP methods mapped as follows:
	•	GET /tickets - Retrieves a list of tickets
	•	GET /tickets/12 - Retrieves a specific ticket
	•	POST /tickets - Creates a new ticket
	•	PUT /tickets/12 - Updates ticket #12
	•	PATCH /tickets/12 - Partially updates ticket #12
	•	DELETE /tickets/12 - Deletes ticket #12
The great thing about REST is that you're leveraging existing HTTP methods to implement significant functionality on just a single /tickets endpoint. There are no method naming conventions to follow and the URL structure is clean &amp; clear. REST FTW!
Should the endpoint name be singular or plural? The keep-it-simple rule applies here. Although your inner-grammatician will tell you it's wrong to describe a single instance of a resource using a plural, the pragmatic answer is to keep the URL format consistent and always use a plural. Not having to deal with odd pluralization (person/people, goose/geese) makes the life of the API consumer better and is easier for the API provider to implement (as most modern frameworks will natively handle /tickets and /tickets/12 under a common controller).
But how do you deal with relations? If a relation can only exist within another resource, RESTful principles provide useful guidance. Let's look at this with an example. A ticket in SupportFu consists of a number of messages. These messages can be logically mapped to the /tickets endpoint as follows:
	•	GET /tickets/12/messages - Retrieves list of messages for ticket #12
	•	GET /tickets/12/messages/5 - Retrieves message #5 for ticket #12
	•	POST /tickets/12/messages - Creates a new message in ticket #12
	•	PUT /tickets/12/messages/5 - Updates message #5 for ticket #12
	•	PATCH /tickets/12/messages/5 - Partially updates message #5 for ticket #12
	•	DELETE /tickets/12/messages/5 - Deletes message #5 for ticket #12
Alternatively, if a relation can exist independently of the resource, it makes sense to just include an identifier for it within the output representation of the resource. The API consumer would then have to hit the relation's endpoint. However, if the relation is commonly requested alongside the resource, the API could offer functionality to automatically embed the relation's representation and avoid the second hit to the API.
What about actions that don't fit into the world of CRUD operations?
This is where things can get fuzzy. There are a number of approaches:
	1.	Restructure the action to appear like a field of a resource. This works if the action doesn't take parameters. For example an activate action could be mapped to a boolean activated field and updated via a PATCH to the resource.
	2.	Treat it like a sub-resource with RESTful principles. For example, GitHub's API lets you star a gist with PUT /gists/:id/star and unstar with DELETE /gists/:id/star.
	3.	Sometimes you really have no way to map the action to a sensible RESTful structure. For example, a multi-resource search doesn't really make sense to be applied to a specific resource's endpoint. In this case, /search would make the most sense even though it isn't a resource. This is OK - just do what's right from the perspective of the API consumer and make sure it's documented clearly to avoid confusion.
SSL everywhere - all the time
Always use SSL. No exceptions. Today, your web APIs can get accessed from anywhere there is internet (like libraries, coffee shops, airports among others). Not all of these are secure. Many don't encrypt communications at all, allowing for easy eavesdropping or impersonation if authentication credentials are hijacked.
Another advantage of always using SSL is that guaranteed encrypted communications simplifies authentication efforts - you can get away with simple access tokens instead of having to sign each API request.
One thing to watch out for is non-SSL access to API URLs. Do not redirect these to their SSL counterparts. Throw a hard error instead! The last thing you want is for poorly configured clients to send requests to an unencrypted endpoint, just to be silently redirected to the actual encrypted endpoint.
Documentation
An API is only as good as its documentation. The docs should be easy to find and publically accessible. Most developers will check out the docs before attempting any integration effort. When the docs are hidden inside a PDF file or require signing in, they're not only difficult to find but also not easy to search.

The docs should show examples of complete request/response cycles. Preferably, the requests should be pastable examples - either links that can be pasted into a browser or curl examples that can be pasted into a terminal. GitHub and Stripe do a great job with this.
Once you release a public API, you've committed to not breaking things without notice. The documentation must include any deprecation schedules and details surrounding externally visible API updates. Updates should be delivered via a blog (i.e. a changelog) or a mailing list (preferably both!).
Versioning
Always version your API. Versioning helps you iterate faster and prevents invalid requests from hitting updated endpoints. It also helps smooth over any major API version transitions as you can continue to offer old API versions for a period of time.
There are mixed opinions around whether an API version should be included in the URL or in a header. Academically speaking, it should probably be in a header. However, the version needs to be in the URL to ensure browser explorability of the resources across versions (remember the API requirements specified at the top of this post?).
I'm a big fan of the approach that Stripe has taken to API versioning - the URL has a major version number (v1), but the API has date based sub-versions which can be chosen using a custom HTTP request header. In this case, the major version provides structural stability of the API as a whole while the sub-versions accounts for smaller changes (field deprecations, endpoint changes, etc).
An API is never going to be completely stable. Change is inevitable. What's important is how that change is managed. Well documented and announced multi-month deprecation schedules can be an acceptable practice for many APIs. It comes down to what is reasonable given the industry and possible consumers of the API.
Result filtering, sorting &amp; searching
It's best to keep the base resource URLs as lean as possible. Complex result filters, sorting requirements and advanced searching (when restricted to a single type of resource) can all be easily implemented as query parameters on top of the base URL. Let's look at these in more detail:
Filtering: Use a unique query parameter for each field that implements filtering. For example, when requesting a list of tickets from the /tickets endpoint, you may want to limit these to only those in the open state. This could be accomplished with a request like GET /tickets?state=open. Here, state is a query parameter that implements a filter.
Sorting: Similar to filtering, a generic parameter sort can be used to describe sorting rules. Accommodate complex sorting requirements by letting the sort parameter take in list of comma separated fields, each with a possible unary negative to imply descending sort order. Let's look at some examples:
	•	GET /tickets?sort=-priority - Retrieves a list of tickets in descending order of priority
	•	GET /tickets?sort=-priority,created_at - Retrieves a list of tickets in descending order of priority. Within a specific priority, older tickets are ordered first
Searching: Sometimes basic filters aren't enough and you need the power of full text search. Perhaps you're already using ElasticSearch or another Lucene based search technology. When full text search is used as a mechanism of retrieving resource instances for a specific type of resource, it can be exposed on the API as a query parameter on the resource's endpoint. Let's say q. Search queries should be passed straight to the search engine and API output should be in the same format as a normal list result.
Combining these together, we can build queries like:
	•	GET /tickets?sort=-updated_at - Retrieve recently updated tickets
	•	GET /tickets?state=closed&amp;sort=-updated_at - Retrieve recently closed tickets
	•	GET /tickets?q=return&amp;state=open&amp;sort=-priority,created_at - Retrieve the highest priority open tickets mentioning the word 'return'
Aliases for common queries
To make the API experience more pleasant for the average consumer, consider packaging up sets of conditions into easily accessible RESTful paths. For example, the recently closed tickets query above could be packaged up as GET /tickets/recently_closed
Limiting which fields are returned by the API
The API consumer doesn't always need the full representation of a resource. The ability select and chose returned fields goes a long way in letting the API consumer minimize network traffic and speed up their own usage of the API.
Use a fields query parameter that takes a comma separated list of fields to include. For example, the following request would retrieve just enough information to display a sorted listing of open tickets:
GET /tickets?fields=id,subject,customer_name,updated_at&amp;state=open&amp;sort=-updated_at
Updates &amp; creation should return a resource representation
A PUT, POST or PATCH call may make modifications to fields of the underlying resource that weren't part of the provided parameters (for example: created_at or updated_at timestamps). To prevent an API consumer from having to hit the API again for an updated representation, have the API return the updated (or created) representation as part of the response.
In case of a POST that resulted in a creation, use a HTTP 201 status code and include a Location header that points to the URL of the new resource.
Should you HATEOAS?
There are a lot of mixed opinions as to whether the API consumer should create links or whether links should be provided to the API. RESTful design principles specify HATEOAS which roughly states that interaction with an endpoint should be defined within metadata that comes with the output representation and not based on out-of-band information.
Although the web generally works on HATEOAS type principles (where we go to a website's front page and follow links based on what we see on the page), I don't think we're ready for HATEOAS on APIs just yet. When browsing a website, decisions on what links will be clicked are made at run time. However, with an API, decisions as to what requests will be sent are made when the API integration code is written, not at run time. Could the decisions be deferred to run time? Sure, however, there isn't much to gain going down that route as code would still not be able to handle significant API changes without breaking. That said, I think HATEOAS is promising but not ready for prime time just yet. Some more effort has to be put in to define standards and tooling around these principles for its potential to be fully realized.
For now, it's best to assume the user has access to the documentation &amp; include resource identifiers in the output representation which the API consumer will use when crafting links. There are a couple of advantages of sticking to identifiers - data flowing over the network is minimized and the data stored by API consumers is also minimized (as they are storing small identifiers as opposed to URLs that contain identifiers).
Also, given this post advocates version numbers in the URL, it makes more sense in the long term for the API consumer to store resource identifiers as opposed to URLs. After all, the identifier is stable across versions but the URL representing it is not!
JSON only responses
It's time to leave XML behind in APIs. It's verbose, it's hard to parse, it's hard to read, its data model isn't compatible with how most programming languages model data and its extendibility advantages are irrelevant when your output representation's primary needs are serialization from an internal representation.
I'm not going to put much effort into explaining the above breathful as it looks like others (YouTube, Twitter &amp; Box) have already started the XML exodus.
I'll just leave you the following Google Trends chart (XML API vs JSON API) as food for thought:
#
However, if your customer base consists of a large number of enterprise customers, you may find yourself having to support XML anyway. If you must do this, you'll find yourself with a new question:

Should the media type change based on Accept headers or based on the URL? To ensure browser explorability, it should be in the URL. The most sensible option here would be to append a .json or .xml extension to the endpoint URL.
snake_case vs camelCase for field names
If you're using JSON (JavaScript Object Notation) as your primary representation format, the "right" thing to do is to follow JavaScript naming conventions - and that means camelCase for field names! If you then go the route of building client libraries in various languages, it's best to use idiomatic naming conventions in them - camelCase for C# &amp; Java, snake_case for python &amp; ruby.
Food for thought: I've always felt that snake_case is easier to read than JavaScript's convention of camelCase. I just didn't have any evidence to back up my gut feelings, until now. Based on an eye tracking study on camelCase and snake_case (PDF) from 2010, snake_case is 20% easier to read than camelCase! That impact on readability would affect API explorability and examples in documentation.
Many popular JSON APIs use snake_case. I suspect this is due to serialization libraries following naming conventions of the underlying language they are using. Perhaps we need to have JSON serialization libraries handle naming convention transformations.
Pretty print by default &amp; ensure gzip is supported
An API that provides white-space compressed output isn't very fun to look at from a browser. Although some sort of query parameter (like ?pretty=true) could be provided to enable pretty printing, an API that pretty prints by default is much more approachable. The cost of the extra data transfer is negligible, especially when you compare to the cost of not implementing gzip.
Consider some use cases: What if an API consumer is debugging and has their code print out data it received from the API - It will be readable by default. Or if the consumer grabbed the URL their code was generating and hit it directly from the browser - it will be readable by default. These are small things. Small things that make an API pleasant to use!
But what about all the extra data transfer?
Let's look at this with a real world example. I've pulled some data from GitHub's API, which uses pretty print by default. I'll also be doing some gzip comparisons:
$ curl https://api.github.com/users/veesahni &gt; with-whitespace.txt
$ ruby -r json -e 'puts JSON JSON.parse(STDIN.read)' &lt; with-whitespace.txt &gt; without-whitespace.txt
$ gzip -c with-whitespace.txt &gt; with-whitespace.txt.gz
$ gzip -c without-whitespace.txt &gt; without-whitespace.txt.gz
The output files have the following sizes:
	•	without-whitespace.txt - 1252 bytes
	•	with-whitespace.txt - 1369 bytes
	•	without-whitespace.txt.gz - 496 bytes
	•	with-whitespace.txt.gz - 509 bytes
In this example, the whitespace increased the output size by 8.5% when gzip is not in play and 2.6% when gzip is in play. On the other hand, the act of gzipping in itself provided over 60% in bandwidth savings. Since the cost of pretty printing is relatively small, it's best to pretty print by default and ensure gzip compression is supported!
To further hammer in this point, Twitter found that there was an 80% savings (in some cases) when enabling gzip compression on their Streaming API. Stack Exchange went as far as to never return a response that's not compressed!
Don't use an envelope by default, but make it possible when needed
Many APIs wrap their responses in envelopes like this:
{
  "data" : {
    "id" : 123,
    "name" : "John"
  }
}
There are a couple of justifications for doing this - it makes it easy to include additional metadata or pagination information, some REST clients don't allow easy access to HTTP headers &amp; JSONP requests have no access to HTTP headers. However, with standards that are being rapidly adopted like CORS and the Link header from RFC 5988, enveloping is starting to become unnecessary.
We can future proof the API by staying envelope free by default and enveloping only in exceptional cases.
How should an envelope be used in the exceptional cases?
There are 2 situations where an envelope is really needed - if the API needs to support cross domain requests over JSONP or if the client is incapable of working with HTTP headers.
JSONP requests come with an additional query parameter (usually named callback or jsonp) representing the name of the callback function. If this parameter is present, the API should switch to a full envelope mode where it always responds with a 200 HTTP status code and passes the real status code in the JSON payload. Any additional HTTP headers that would have been passed alongside the response should be mapped to JSON fields, like so:
callback_function({
  status_code: 200,
  next_page: "https://..",
  response: {
    ... actual JSON response body ... 
  }
})
Similarly, to support limited HTTP clients, allow for a special query parameter ?envelope=true that would trigger full enveloping (without the JSONP callback function).
JSON encoded POST, PUT &amp; PATCH bodies
If you're following the approach in this post, then you've embraced JSON for all API output. Let's consider JSON for API input.
Many APIs use URL encoding in their API request bodies. URL encoding is exactly what it sounds like - request bodies where key value pairs are encoded using the same conventions as one would use to encode data in URL query parameters. This is simple, widely supported and gets the job done.
However, URL encoding has a few issues that make it problematic. It has no concept of data types. This forces the API to parse integers and booleans out of strings. Furthermore, it has no real concept of hierarchical structure. Although there are some conventions that can build some structure out of key value pairs (like appending [ ] to a key to represent an array), this is no comparison to the native hierarchical structure of JSON.
If the API is simple, URL encoding may suffice. However, complex APIs should stick to JSON for their API input. Either way, pick one and be consistent throughout the API.
An API that accepts JSON encoded POST, PUT &amp; PATCH requests should also require the Content-Type header be set to application/json or throw a 415 Unsupported Media Type HTTP status code.
Pagination
Envelope loving APIs typically include pagination data in the envelope itself. And I don't blame them - until recently, there weren't many better options. The right way to include pagination details today is using the Link header introduced by RFC 5988.
An API that uses the Link header can return a set of ready-made links so the API consumer doesn't have to construct links themselves. This is especially important when pagination is cursor based. Here is an example of a Link header used properly, grabbed from GitHub's documentation:
Link: &lt;https://api.github.com/user/repos?page=3&amp;per_page=100&gt;; rel="next", &lt;https://api.github.com/user/repos?page=50&amp;per_page=100&gt;; rel="last"
But this isn't a complete solution as many APIs do like to return the additional pagination information, like a count of the total number of available results. An API that requires sending a count can use a custom HTTP header like X-Total-Count.
Auto loading related resource representations
There are many cases where an API consumer needs to load data related to (or referenced) from the resource being requested. Rather than requiring the consumer to hit the API repeatedly for this information, there would be a significant efficiency gain from allowing related data to be returned and loaded alongside the original resource on demand.
However, as this does go against some RESTful principles, we can minimize our deviation by only doing so based on an embed (or expand) query parameter.
In this case, embed would be a comma separated list of fields to be embedded. Dot-notation could be used to refer to sub-fields. For example:
GET /tickets/12?embed=customer.name,assigned_user
This would return a ticket with additional details embedded, like:
{
  "id" : 12,
  "subject" : "I have a question!",
  "summary" : "Hi, ....",
  "customer" : {
    "name" : "Bob"
  },
  assigned_user: {
   "id" : 42,
   "name" : "Jim",
  }
}
Of course, ability to implement something like this really depends on internal complexity. This kind of embedding can easily result in an N+1 select issue.
Overriding the HTTP method
Some HTTP clients can only work with simple GET and POST requests. To increase accessibility to these limited clients, the API needs a way to override the HTTP method. Although there aren't any hard standards here, the popular convention is to accept a request header X-HTTP-Method-Override with a string value containing one of PUT, PATCH or DELETE.
Note that the override header should only be accepted on POST requests. GET requests should never change data on the server!
Rate limiting
To prevent abuse, it is standard practice to add some sort of rate limiting to an API. RFC 6585 introduced a HTTP status code 429 Too Many Requests to accommodate this.
However, it can be very useful to notify the consumer of their limits before they actually hit it. This is an area that currently lacks standards but has a number of popular conventions using HTTP response headers.
At a minimum, include the following headers (using Twitter's naming conventions as headers typically don't have mid-word capitalization):
	•	X-Rate-Limit-Limit - The number of allowed requests in the current period
	•	X-Rate-Limit-Remaining - The number of remaining requests in the current period
	•	X-Rate-Limit-Reset - The number of seconds left in the current period
Why is number of seconds left being used instead of a time stamp for X-Rate-Limit-Reset?
A timestamp contains all sorts of useful but unnecessary information like the date and possibly the time-zone. An API consumer really just wants to when they can send the request again &amp; the number of seconds answers this question with minimal additional processing on their end. It also avoids issues related to clock skew.
Some APIs use a UNIX timestamp (seconds since epoch) for X-Rate-Limit-Reset. Don't do this!
Why is it bad practice to use a UNIX timestamp for X-Rate-Limit-Reset?
The HTTP spec already specifies using RFC 1123 date formats (currently being used in Date, If-Modified-Since &amp; Last-Modified HTTP headers). If we were to specify a new HTTP header that takes a timestamp of some sort, it should follow RFC 1123 conventions instead of using UNIX timestamps.
Authentication
A RESTful API should be stateless. This means that request authentication should not depend on cookies or sessions. Instead, each request should come with some sort authentication credentials.
By always using SSL, the authentication credentials can be simplified to a randomly generated access token that is delivered in the user name field of HTTP Basic Auth. The great thing about this is that it's completely browser explorable - the browser will just popup a prompt asking for credentials if it receives a 401 Unauthorized status code from the server.
However, this token-over-basic-auth method of authentication is only acceptable in cases where it's practical to have the user copy a token from an administration interface to the API consumer environment. In cases where this isn't possible, OAuth 2 should be used to provide secure token transfer to a third party. OAuth 2 uses Bearer tokens &amp; also depends on SSL for its underlying transport encryption.
An API that needs to support JSONP will need a third method of authentication, as JSONP requests cannot send HTTP Basic Auth credentials or Bearer tokens. In this case, a special query parameter access_token can be used. Note: there is an inherent security issue in using a query parameter for the token as most web servers store query parameters in server logs.
For what it's worth, all three methods above are just ways to transport the token across the API boundary. The actual underlying token itself could be identical.
Caching
HTTP provides a built-in caching framework! All you have to do is include some additional outbound response headers and do a little validation when you receive some inbound request headers.
There are 2 approaches: ETag and Last-Modified
ETag: When generating a request, include a HTTP header ETag containing a hash or checksum of the representation. This value should change whenever the output representation changes. Now, if an inbound HTTP requests contains a If-None-Match header with a matching ETag value, the API should return a 304 Not Modified status code instead of the output representation of the resource.
Last-Modified: This basically works like to ETag, except that it uses timestamps. The response header Last-Modified contains a timestamp in RFC 1123 format which is validated against If-Modified-Since. Note that the HTTP spec has had 3 different acceptable date formats and the server should be prepared to accept any one of them.
Errors
Just like an HTML error page shows a useful error message to a visitor, an API should provide a useful error message in a known consumable format. The representation of an error should be no different than the representation of any resource, just with its own set of fields.
The API should always return sensible HTTP status codes. API errors typically break down into 2 types: 400 series status codes for client issues &amp; 500 series status codes for server issues. At a minimum, the API should standardize that all 400 series errors come with consumable JSON error representation. If possible (i.e. if load balancers &amp; reverse proxies can create custom error bodies), this should extend to 500 series status codes.
A JSON error body should provide a few things for the developer - a useful error message, a unique error code (that can be looked up for more details in the docs) and possibly a detailed description. JSON output representation for something like this would look like:
{
  "code" : 1234,
  "message" : "Something bad happened :(",
  "description" : "More details about the error here"
}
Validation errors for PUT, PATCH and POST requests will need a field breakdown. This is best modeled by using a fixed top-level error code for validation failures and providing the detailed errors in an additional errors field, like so:
{
  "code" : 1024,
  "message" : "Validation Failed",
  "errors" : [
    {
      "code" : 5432,
      "field" : "first_name",
      "message" : "First name cannot have fancy characters"
    },
    {
       "code" : 5622,
       "field" : "password",
       "message" : "Password cannot be blank"
    }
  ]
}
HTTP status codes
HTTP defines a bunch of meaningful status codes that can be returned from your API. These can be leveraged to help the API consumers route their responses accordingly. I've curated a short list of the ones that you definitely should be using:
	•	200 OK - Response to a successful GET, PUT, PATCH or DELETE. Can also be used for a POST that doesn't result in a creation.
	•	201 Created - Response to a POST that results in a creation. Should be combined with a Location header pointing to the location of the new resource
	•	204 No Content - Response to a successful request that won't be returning a body (like a DELETE request)
	•	304 Not Modified - Used when HTTP caching headers are in play
	•	400 Bad Request - The request is malformed, such as if the body does not parse
	•	401 Unauthorized - When no or invalid authentication details are provided. Also useful to trigger an auth popup if the API is used from a browser
	•	403 Forbidden - When authentication succeeded but authenticated user doesn't have access to the resource
	•	404 Not Found - When a non-existent resource is requested
	•	405 Method Not Allowed - When an HTTP method is being requested that isn't allowed for the authenticated user
	•	410 Gone - Indicates that the resource at this end point is no longer available. Useful as a blanket response for old API versions
	•	415 Unsupported Media Type - If incorrect content type was provided as part of the request
	•	422 Unprocessable Entity - Used for validation errors
	•	429 Too Many Requests - When a request is rejected due to rate limiting
In Summary
An API is a user interface for developers. Put the effort in to ensure it's not just functional but pleasant to use.</Text>
            <Notes>http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api

keep the URL format consistent and always use a plural. 

	1.	Restructure the action to appear like a field of a resource. This works if the action doesn't take parameters. For example an activate action could be mapped to a boolean activated field and updated via a PATCH to the resource.
	2.	Treat it like a sub-resource with RESTful principles. For example, GitHub's API lets you star a gist with PUT /gists/:id/star and unstar with DELETE /gists/:id/star.
	3.	Sometimes you really have no way to map the action to a sensible RESTful structure. For example, a multi-resource search doesn't really make sense to be applied to a specific resource's endpoint. In this case, /search would make the most sense even though it isn't a resource. This is OK - just do what's right from the perspective of the API consumer and make sure it's documented clearly to avoid confusion.

One thing to watch out for is non-SSL access to API URLs. Do not redirect these to their SSL counterparts. Throw a hard error instead! 

docs should show examples of complete request/response cycles
Preferably, the requests should be pastable examples

Always version your API.
the URL has a major version number (v1), but the API has date based sub-versions which can be chosen using a custom HTTP request header
An API is never going to be completely stable.

It's best to keep the base resource URLs as lean as possible.
consider packaging up sets of conditions into easily accessible RESTful paths

To prevent an API consumer from having to hit the API again for an updated representation, have the API return the updated (or created) representation as part of the response.

I don't think we're ready for HATEOAS on APIs just yet.

It's time to leave XML behind in APIs.

snake_case is 20% easier to read than camelCase

an API that pretty prints by default is much more approachable

the whitespace increased the output size by 8.5% when gzip is not in play and 2.6% when gzip is in play. On the other hand, the act of gzipping in itself provided over 60% in bandwidth savings.

If the API is simple, URL encoding may suffice. However, complex APIs should stick to JSON for their API input. Either way, pick one and be consistent throughout the API.

An API that uses the Link header can return a set of ready-made links so the API consumer doesn't have to construct links themselves. This is especially important when pagination is cursor based. Here is an example of a Link header used properly, grabbed from GitHub's documentation:

Link: &lt;https://api.github.com/user/repos?page=3&amp;per_page=100&gt;; rel="next", &lt;https://api.github.com/user/repos?page=50&amp;per_page=100&gt;; rel="last"

Some HTTP clients can only work with simple GET and POST requests. […] the popular convention is to accept a request header X-HTTP-Method-Override with a string value containing one of PUT, PATCH or DELETE

However, this token-over-basic-auth method of authentication is only acceptable in cases where it's practical to have the user copy a token from an administration interface to the API consumer environment. In cases where this isn't possible, OAuth 2 should be used to provide secure token transfer to a third party. OAuth 2 uses Bearer tokens &amp; also depends on SSL for its underlying transport encryption.

all three methods above are just ways to transport the token across the API boundary. The actual underlying token itself could be identical.

At a minimum, the API should standardize that all 400 series errors come with consumable JSON error representation. If possible (i.e. if load balancers &amp; reverse proxies can create custom error bodies), this should extend to 500 series status codes.</Notes>
        </Document>
        <Document ID="15">
            <Title>Notes</Title>
        </Document>
        <Document ID="5">
            <Title>Essay Body</Title>
        </Document>
        <Document ID="23">
            <Title>Your API Consumers Aren’t Who You Think They Are</Title>
            <Text>Your API Consumers Aren’t Who You Think They Are
This post is adapted from a talk I gave at APIdays SF, view slides here.
At Zapier, we sit at a unique crossroads between APIs and lots of users. Because of our position, we’re noticing a shift in API consumption. While many of our users have no idea that everything they do on Zapier is powered by an API, many of them are becoming more aware of that fact. Consequently, they’re asking us for API advice, they’re pinging their vendors about API related limitations, and they’re starting to get comfortable with the idea that an API can be a powerful tool for their toolbox.
Traditionally, you’d expect most inquiries about APIs to be very technical: How can I upload bulk attachments with multipart POST bodies? Increasingly, questions are becoming more like amateur Stack Overflow questions: How do I hook the Microsoft Access into the REST? The users behind such questions aren’t tech-illiterate, they simply have no idea how to code and aren’t familiar with the lingo. They don’t really know where to start, but they want to know about your API.
This is awesome.
So who exactly are these users and what are their skill-sets? They defy universal categorization, but here are a handful of examples:
	•	The lone IT worker who has only ever replaced PC hardware and installed software before. Now they are tasked with setting up and migrating to their first cloud app.
	•	The tech-savvy blogger who knows how to install WordPress on shared hosting. They can, after much trial and error, tweak the HTML and CSS of their blog to suit their style.
	•	The small business owner who has cobbled together a powerful Excel spreadsheet to organize their business, but realizes it can’t scale past one or two employees.
The reason why non-coders are becoming interested in your API is actually quite simple: APIs are the only means by which they could possibly implement some vital but missing feature. It is worth noting that these missing features are also very long-tail, which means that you, as the vendor, probably won’t add that functionality in the native app. While you’ve got the 90% use case nailed down, they require that last 10%.
All of these users are tech-savvy, but don’t code. All of these users are DIY’ers that invested a lot of time into getting your SaaS set up and working almost perfectly. But they hit a wall, they started researching and without fail, they’ll find your API. Then what? The first thing they do is look at your docs just like any developer would, but then they suddenly feel one of two things: empowered or overwhelmed.
Before we can start addressing the issue of making APIs more accessible to those who cannot code, we need one more piece of information: what are the goals of these non-coders? What features are they missing? Unfortunately, because they are inherently long-tail features, there isn’t a single answer. But we can categorize them. Consider these common requests:
	•	Can we copy records to another project?
	•	Can we import/export/backup our records via insert format?
	•	Can we move or modify a record after we apply a special tag?
	•	Can we get an overview email every night/week/month?
	•	etc…
In the absence of these features, the only other option is to perform this work by hand. That means wasted hours on work that could easily be automated. The features aren’t usually that complicated either, a slight remixing of basic CRUD operations usually does the trick.
We’ll call these sorts of features CRUD automation.
So now that we’ve identified these non-coders and figured out what sorts of things they want do do, we’re left with two quintessential questions:
	1.	What can we do to help non-coders consume APIs?
	2.	What can we do to improve APIs for CRUD automation?
Just to set your mind in the direction: The answer to the first question addresses the empowerment issue and revolves mostly around your API experience. The answer to the second question mostly revolves around reducing complexity and common API “gotchas”.
Also, these ideas can make your API more joyful to user for even the most seasoned of neck-beards.
Let’s get started.
What can we do to help non-coders consume APIs?
First up is easier documentation. Note I didn’t say better documentation, because that can be highly subjective and varies with the audience. The best documentation for an extremely talented developer might just give them the facts as fast as possible: here’s how to auth, here are the endpoints. We’re not suggesting that here.
Instead, focus on the introduction that your landing page gives to your API. This is the point where you need to be very encouraging and talk about use cases, individual success stories and other humanizing bits. Skip the jargon. Again, you want to empower the user and open their eyes to the possibilities that lay before them.
Next, provide a wedge by spoon feeding some great “hello world!” examples to the user as soon as possible. These sorts of DIYers are the kind that take chances on tech: jailbreaking their smart phone, editing their WordPress theme’s HTML &amp; CSS, and creating crazy spreadsheet formulas. If you can get them to upload your simple PHP example to their shared hosting, they can iterate from there. With enough wind at their back, they’ll even brute force a solution. So, give them a good wedge and they’ll surprise you every time.
Finally, provide live data right in the docs. Their experience with your app so far is via the standard web UI which doesn’t always map cleanly to your API resources or endpoints. As soon as you inline some examples of real data they’ll recognize, they can make that connection much faster.
Second up is multiple auth mechanisms. This will almost always be the first thing a user has to grok and implement before they can even get started. So it might be the most vital of technical details. If they can get past this, things are looking good!
The best thing you can do is skip OAuth and provide API key or basic authentication. Don’t get me wrong, OAuth is wonderful for platforms, and if you are doing a platform where users can publish apps you need to do OAuth. But for your average lone wolf, access tokens and refresh tokens are rather annoying and confusing, especially when they expire! At the very least generate access tokens and don’t expire them, but you’d be better served to just do API keys or basic authentication too.
Additionally, make your example code snippets live code. That means you hand them a loaded gun in the form of code that will do something on their account when ran. Insert the real tokens right into the the code.
Third up is useful error messages, which is not the same as descriptive error messages. This just means you shouldn’t just tell the user what went wrong, but what they should do to fix it.
Consider a 401: Not Authenticated error, that alone doesn’t really help a really green user move forward. Instead, 401: Could not authenticate this request. Try adding an API key to the URL like this: https://:current-url:?api_key=123abc gives a much better next step for the users. Of course, if they just paste in the example (and invalid) key, tell them where to get the key next!
Also, always serialize your errors the same way. This may seem like common sense, but I’ve seen these three errors more than I’d like to admit:
	1.	Sometimes a list of errors, sometimes a single string message.
	2.	No matching URL routes giving HTML 404s instead of JSON/XML.
	3.	500 errors giving HTML error pages instead of JSON/XML.
The final piece of advice to help non-coders with APIs is a little out there: don’t do what I say, do what I mean. Basically, if you can write a sufficiently intelligent and prescriptive error message, you could have probably just done what the user had meant to do in the first place. It can get tricky, but as you write the really nice error messages, just think: could I just do this for them? You can only do this in some special cases; you can’t bend the rules for something like authentication.
What can we do to improve APIs for CRUD automation?
A common theme here will be complexity reduction. Any non-coder is going to have a hard time managing complexity, especially around data manipulation and traversing. Remember, even syntax is going to trip them up quite a bit, so the more your API can abstract away, the more they can skip coding and continue playing.
First up are flexible endpoints which are a must for enabling efficient automation. The reasoning is fairly straight forward: filtering, paging, and merging data from an API in code is a cumbersome and error prone process even in the best of situations.
Also, the long-tail nature of these automation opportunities mean they won’t be easily predicted. So if you can provide a slew of query-string arguments that help a user only get at the data they want, you’ll make their lives much easier. Besides, an extra WHERE (for filter) or JOIN (for inlining) is much more efficient than paging through every page of results or merging extra resources from extra API calls. Your servers will thank you.
As a final point around flexibility: a majority of users won’t know a lot about big O notation. That means they are a few nested for-loops away from attempting to make more API calls than there are atoms in the known universe. Just inline the related resources.
Second up are real time endpoints. At first glance, a real time enabled API seems to add complexity, but that is only because you are looking at it through your own lens. Real time (if done correctly) is vastly simpler for API consumers. The complexity is all on your side.
The reason for all the complexity is because naive read operations involve polling if you want to identify new data. Polling means infrastructure (via crons or queues) and deduplication logic of some sort. Worse, it is basically hidden complexity; a user will get started and be suddenly blindsided when they realize they need to filter out data they’ve already seen.
No matter how you slice it, polling sucks. It wastes resources, adds complexity, and is slow. Let’s just agree to kill polling once and for all.
The solution has been with us for a long time: webhooks. Not only should you be able to create them in the GUI (and as many as you want, not just one), you should be able to create them via the API too. Plus webhooks are dead simple: point them at some PHP script and capture the POST body. They make for very nice “Hello world!” examples.
Just skip long-polling and websockets for now, they just add complexity and infrastructure on both sides. And until node.js or event based programming is de-facto on shared hosting it is unlikely to change.
Third up and final point for making APIs easier to automate, provide PATCH and upsert capabilities.
PATCH simply removes the null value gotcha if you forget to include an original value during a PUT update. The particularly nasty bit about a PUT nulling values is that the discovery is often delayed and the rogue PUT is hard to identify as the source. Lots of APIs already do this with PUT, but it is nice to be explicit about it no matter what you decide on.
Upsert removes the complexity around managing uniqueness in an API. It sort of wraps the whole get or create pattern that is fairly common in ORMs: it will either insert or update a record based on some included unique field (often email).
Again, we want to handle the complexity of insert or updating records so the user doesn’t have to.
Who is at the end of your API requests?
Before we wrap up, its worth considering that even if a brilliant coder did the grunt work of building a feature based on an API, it is possible a less skilled coder will attempt to expand on or will inherit that body of work.
Also, it is unlikely that the end-user of said API enabled feature is a coder either, so when things break, the above tips can be the difference between “it doesn’t work” and “ah, I think I can fix that”.
As a parting thought, there are a lot more non-coding but tech-savvy users out there than there are coders. This deviation towards the mean is something to celebrate and embrace.
Remember, your API consumers aren’t always who you think they are.</Text>
            <Notes>http://bryanhelmig.com/your-api-consumers-arent-who-you-think-they-are/
The first thing they do is look at your docs just like any developer would, but then they suddenly feel one of two things: empowered or overwhelmed.
what are the goals of these non-coders? What features are they missing?
easier documentation
focus on the introduction
provide a wedge by spoon feeding some great “hello world!” examples to the user as soon as possible
provide live data right in the docs.
multiple auth mechanisms.
example code snippets live code
useful error messages - what they should do to fix it. 401: Could not authenticate this request. Try adding an API key to the URL like this: https://:current-url:?api_key=123abc
always serialize your errors the same way.
I’ve seen these three errors more than I’d like to admit:
	1.	Sometimes a list of errors, sometimes a single string message.
	2.	No matching URL routes giving HTML 404s instead of JSON/XML.
	3.	500 errors giving HTML error pages instead of JSON/XML.

don’t do what I say, do what I mean. Basically, if you can write a sufficiently intelligent and prescriptive error message, you could have probably just done what the user had meant to do in the first place. You can only do this in some special cases
the more your API can abstract away, the more they can skip coding and continue playing.
provide a slew of query-string arguments that help a user only get at the data they want
they are a few nested for-loops away from attempting to make more API calls than there are atoms in the known universe
real time endpoints webhooks
skip long-polling and websockets for now, they just add complexity and infrastructure on both sides
provide PATCH and upsert
Upsert removes the complexity around managing uniqueness in an API. It sort of wraps the whole get or create pattern that is fairly common in ORMs: it will either insert or update a record based on some included unique field (often email).


</Notes>
        </Document>
        <Document ID="16">
            <Title>First Page Header</Title>
            <Text>Your Name
Instructor name
Course title
&lt;$longdate&gt;
API V2</Text>
        </Document>
        <Document ID="7">
            <Title>Works Cited</Title>
            <Text>Works Cited

Works cited should go here (this page is optional). See the Works Cited Format document for information on formatting citations.</Text>
        </Document>
        <Document ID="8">
            <Title>Main Content</Title>
        </Document>
        <Document ID="24">
            <Title>REST APIs must be hypertext-driven</Title>
            <Synopsis>API designers, please note the following rules before calling your creation a REST API:</Synopsis>
            <Text>REST APIs must be hypertext-driven
Posted by Roy T. Fielding under software architecture, web architecture  [51] Comments
I am getting frustrated by the number of people calling any HTTP-based interface a REST API. Today’s example is the SocialSite REST API. That is RPC. It screams RPC. There is so much coupling on display that it should be given an X rating.
What needs to be done to make the REST architectural style clear on the notion that hypertext is a constraint? In other words, if the engine of application state (and hence the API) is not being driven by hypertext, then it cannot be RESTful and cannot be a REST API. Period. Is there some broken manual somewhere that needs to be fixed?
API designers, please note the following rules before calling your creation a REST API:
	•	A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification. [Failure here implies that identification is not separated from interaction.]
	•	A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP’s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP’s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.]
	•	A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.]
	•	A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC's functional coupling].
	•	A REST API should never have “typed” resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation’s media type and standardized relation names. [ditto]
	•	A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.]
There are probably other rules that I am forgetting, but the above are the rules related to the hypertext constraint that are most often violated within so-called REST APIs. Please try to adhere to them or choose some other buzzword for your API.

</Text>
            <Notes>http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven</Notes>
        </Document>
        <Document ID="17">
            <Title>Miscellaneous</Title>
        </Document>
        <Document ID="9">
            <Title>Notes</Title>
            <Text>Notes
Any footnotes you created in your paper will get inserted here upon Compile (this purple bubble will be removed automatically). If you don’t have any footnotes, you can deselect “Include in Compile” for this document in the Inspector or Compile settings, or just delete it completely.</Text>
        </Document>
    </Documents>
</SearchIndexes>